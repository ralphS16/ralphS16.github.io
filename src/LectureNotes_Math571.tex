\documentclass{tufte-handout} % A4 paper and 11pt font size
\usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,spacing=true,factor=1100,stretch=10,shrink=10]{microtype}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{mathpazo} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm, amssymb} % Math packages
\usepackage{pgf,tikz}
\usetikzlibrary{positioning,matrix,arrows}
\usepackage{float}
\usepackage{tikz-cd}
\usepackage{caption}
\usepackage{stmaryrd}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\normalfont \bfseries} % Make all sections centered, the default font and small caps
\usepackage{enumerate}
\usepackage{pythonhighlight}
\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header
\allowdisplaybreaks
%--------Theorem Environments--------
%theoremstyle{plain} --- default
\newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{facts}[thm]{Facts}
\newtheorem{fact}[thm]{Fact}
\newtheorem{clm}[thm]{Claim}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}
\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}
\newtheorem{exer}[thm]{Exercise}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}

\newcommand{\bra}[1]{\left(#1\right)}
\newcommand{\sbra}[1]{\left[#1\right]}
\newcommand{\Mod}[1]{\ (\text{mod}\ #1)}
\newcommand{\op}[1]{#1^{\text{op}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\mZ}{\mathcal{Z}}
\renewcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\bA}{\mathbb{A}}
\newcommand{\bH}{\mathbb{H}}
\newcommand{\one}{\mathbb{1}}
\newcommand{\mC}{\mathcal{C}}
\newcommand{\mO}{\mathcal{O}}
\newcommand{\mR}{\mathcal{R}}
\newcommand{\mS}{\mathcal{S}}
\newcommand{\lp}{{\mathfrak{p}}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\aut}{Aut}
\DeclareMathOperator{\gal}{Gal}
\DeclareMathOperator{\var}{\textbf{var}}
\DeclareMathOperator{\orb}{Orb}
\DeclareMathOperator{\ff}{Frac}
\DeclareMathOperator{\stab}{Stab}
\DeclareMathOperator{\inn}{Inn}
\DeclareMathOperator{\Ind}{Ind}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\spn}{Span}
\DeclareMathOperator{\out}{Out}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\tors}{Tors}
\DeclareMathOperator{\Mor}{Mor}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Nat}{Nat}
\DeclareMathOperator{\spec}{Spec}
\DeclareMathOperator{\ann}{Ann}
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\conjc}{Conj}
\DeclareMathOperator{\Br}{Br}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Nm}{Nm}
\DeclareMathOperator{\Char}{char}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\inp}[2]{\left\langle #1, #2 \right\rangle}

\def \v {\vspace{0.2cm}}

\geometry{
	left=13mm, % left margin
	textwidth=130mm, % main text block
	marginparsep=8mm, % gutter between main text block and margin notes
	marginparwidth=55mm % width of margin notes
}
\fontsize{10}{20}\selectfont
%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{	
	\normalfont\normalsize 
	{McGill University - Winter 2019} \\ [0pt] % Your university, school and/or department name(s)
	\huge Lecture Notes - MATH 571% The assignment title
}\author{Ralph Sarkis, David Marcil} % Your name
\date{\vspace{-5pt}\normalsize\today} % Today's date or a custom date

\begin{document}
\justifying 
\maketitle

\tableofcontents

\section{Prerequisites}
%TODO: divide prerequisites in Ring theory, module theory, field theory and others.
We recall some basic notions for completeness and to introduce the notation.
\begin{defns}[Basic definitions]
	Let $R$ be a commutative ring.
	\begin{itemize}
		\item An element $x \in R$ is a \textbf{unit} if it has a multiplicative inverse.
		\item An element $x \in R$ is \textbf{irreducible} if it is not a unit and it cannot be written as the product of two non-units.
		\item An element $x \in R$ is \textbf{prime} if for any $a,b \in R$, $x \mid ab$ implies $x \mid a$ or $x \mid b$.
		\item A subset $I \subseteq R$ is an \textbf{ideal} if $0 \in I$ and for any $a,b \in I$, $r \in R$, $a+rb \in I$. We denote $I \lhd R$.
		\item An ideal $I \lhd R$ is \textbf{prime} if $I\neq R$ and for any $a,b \in R$, $ab \in I$ implies $a \in I$ or $b \in I$.
		\item An ideal $I \lhd R$ is \textbf{maximal} if $I\neq R$ and for any ideal $J\lhd R$ with $I \subseteq J \subseteq R$, either $I = J$ or $J = R$.
		\item We say that $R$ is an \textbf{integral domain} if for any $a,b \in R$, $ab = 0$ implies $a = 0$ or $b = 0$ (equivalently, the ideal $0$ is prime).
		\item $R$ is a \textbf{principal ideal domain} (PID) if any ideal is generated by one element.
		\item $R$ is a \textbf{unique factorization domain} (UFD) if any element can be written as a product of irreducible elements uniquely up to units and ordering.
		\item $R$ \textbf{Noetherian} if all of its ideals are finitely generated.
		\item $R$ \textbf{local} if it has a unique maximal ideal, or equivalently if the set $R - R^\times$ is an ideal of $R$.
		\item \footnote{In general, the natural homomorphism $R \to S^{-1}R$ is not injective. It will be if and only if $S$ does not contain zero divisors of $R$. In class, we only used in on integral domains, so this condition is always satisfied.} Given a multiplicative set $S \subset R - \{0\}$, we can consider the \textbf{localization} $$R[S^{-1}] = S^{-1}R = \{\frac{r}{s} : r \in R, s \in S\}/\sim \ ,$$ where $\frac{r_1}{s_1} \sim \frac{r_2}{s_2}$ if $\exists\ s \in S$ such that $s(r_1s_2 - r_2s_2) = 0$. One can verify that $S^{-1}R$ is a commutative ring.
		\item If $R$ is an integral domain, we denote $\ff(R)$ to be the \textbf{fraction field} of $R$ (i.e.: the localization $R$ at the prime ideal 0).
		\item Let $K$ be a field, a \textbf{field extension} is a field $L$ containing $K$. The \textbf{degree} of the extension, denoted $[L:K]$ is the dimension of $L$ as a $K$-vector space.
		\item A field extension $L/K$ is \textbf{algebraic} if every element of $L$ is a root of some polynomial in $K[x]$.
		\item A field extension $L/K$ is \textbf{separable} if for every $\alpha \in L$, the minimal polynomial of $\alpha$ over $K$ is separable in $L$ (it splits in linear factors).
		\item Let $R$ be a ring and $M$ be a (left) $R$-module, the annihilator of an element $m \in M$ is $\ann_{M}(m) = \{r \in R \mid rm = 0\}$. It is an ideal of $R$.
	\end{itemize}
	%TODO: Talk about Galois stuff. Talk about finite generation stuff. Talk about topology.
\end{defns}
\begin{facts}[Basic properties]
	\begin{prop}
	    Given a commutative ring $R$, the following conditions are equivalent :
	    \begin{enumerate}
	        \item[a)] $R$ is Noetherian
	        \item[b)] Every ascending chain of ideals
	        \[
	            \mathfrak{a}_1 \subset \mathfrak{a}_2 \subset \cdots \subset \mathfrak{a}_n \subset \cdots \ ,
	        \]
	        there exists some $N \geq 1$ such that for all $m \geq N$, $\mathfrak{a}_m = \mathfrak{a}_N$.
	        \item[c)] Every nonempty set $S$ of ideals in $R$ has a maximal element.
	    \end{enumerate}
	\end{prop}
	
	\begin{defns}
	    A module $M$ is said to be \textbf{Noetherian} if every submodule is finitely generated.\footnote{This definition and the following proposition are crucial in our proof of the algebraic statement of the Nullstellensatz.}
	\end{defns}
	
	\begin{prop}
	    Let $R$ be a Noetherian ring. Every finitely generated $R$-module is Noetherian.
	\end{prop}
\end{facts}

In the first part of the course, we will only work with commutative rings (and mostly with integral domains), so all the rings will be commutative until we mention otherwise.
\section{Rings of Dimension One}
\marginnote{The main reference for this section is J.S. Milne's \textit{Algebraic Number Theory}, chapter 1-3.}
\subsection{Krull dimension}
\begin{defn}[Krull dimension]
	Let $R$ be a ring, its \textbf{Krull dimension} is the maximal length\footnote{The length of a chain $I_0 \subset \cdots \subset I_n$ is $n$, i.e.: the number of inclusions (not the number of ideals).} of a strict chain of inclusions of prime ideals in $R$.
\end{defn}
\begin{exmps}\label{exmpkrulldim}
	\begin{enumerate}
		\item[]
		\item Let $R$ be an integral domain. If its Krull dimension is zero, then $R$ is a field. Indeed, since $0$ is a prime ideal in an integral domain and every prime ideal is maximal when the Krull dimension is zero, we get $R/0 \cong R$ is a field.
		\item The Krull dimension of $\Z$ is one. Since $\Z$ is a PID, we can write a chain of length two as $(a_1) \subset (a_2) \subset (a_3)$, where the $a_i$'s are distinct prime and $a_2, a_3 \neq 0$ because no prime ideal is properly contained $(0)$. Thus, we have two non-zero primes $a_2$ and $a_3$ with $a_3 \mid a_2$ which is absurd. An example of a chain of length one in $\Z$ is $0 \subset (2)$.
		\item Let $k$ be a field, then the Krull dimension of $k[x_1, \dots, x_n]$ is $n$.
		\item The ring $\Z[i]$\footnote{This is the ring $\Z$ adjoined with an element $i$ that satisfies $i^2 = -1$. Equivalently, $\Z[i] \cong \Z[x]/(x^2+1)$.} has Krull dimension one. We still have the chain $0 \subset (2)$ of length one, thus it is enough to show that if $\lp$ is a non-zero prime idea, then it is maximal. Pick some non-zero $a+bi \in \lp$ and let $n = (a+bi)(a-bi) \in \lp$, it is also non-zero, so $\Z[i]/(n) \cong \Z/n\Z[i]$ is finite. Moreover, since $(n) \subseteq \lp$, we infer that $\Z[i]/(n)$ maps surjectively onto $\Z[i]/\lp$ and then conclude the latter is a finite integral domain, hence a field.\footnote{Pick any $a \neq 0$ in a finite integral domain $R$, then the multiplication map $x \mapsto ax$ must be injective, and by cardinality, it must also be surjective, yielding $ax = 1$ for some $x$. Note that this argument also holds when $R$ is a finite dimensional vector space over a field because the multiplication map is linear and injectivity still implies surjectivity.}
		\item All the previous examples are UFDs but it is not true that a Krull dimension equal to one implies unique factorization. We can easily see that $\Z[\sqrt{-5}]$ is not a UFD as $6 = 2\cdot 3 = (1+\sqrt{-5})(1-\sqrt{-5})$ are two decompositions in irreducibles. However, the same argument as for $\Z[i]$ works to show this ring has Krull dimension one.
	\end{enumerate}
\end{exmps}

\begin{prop}\label{krulldimoneufdispid}
	If $R$ has Krull dimension one and is a UFD, then it is a PID.
\end{prop}
\begin{proof}
	Let $I$ be a prime ideal of $R$, $r \in I$ and $r = p_1\cdots p_t$ be the unique decomposition into primes. Without loss of generality, since $I$ is prime, $p_1 \in I$ and we get a prime ideal $(p_1) \subseteq I$. Because $R$ has Krull dimension one, $(p_1)$ is maximal and we conclude $I = (p_1)$.
	
	The proposition follows after a simple argument showing that if every prime ideal of $R$ is principal, then $R$ is a PID. Assume towards a contradiction that $R$ is not a PID, then it has some non-principal ideals. Every chain of such ideals has an upper bound,\footnote{Let $\{I_{\alpha}\}_{\alpha \in A}$ be a chain of non-principal ideals and let $I = \cup_{\alpha \in A} I_{\alpha}$, it is an upper-bound for the chain because if $I$ were principal, then $I = (x)$ would imply $x \in I_i$ and furthermore $I =  (x) \subseteq I_i \subseteq I$, contradicting the non-principality of $I$.} therefore, by Zorn's lemma, there exists a maximal non-principal ideal $I$.
	
	We claim that $I$ is prime. Suppose that $ab \in I$ and $a,b \notin I$, then $(I,a)$ is strictly larger than $I$, hence principal, yielding $(I,a) = (c)$ for $c \in R$. Also, the set $I:a := \{r \in R \mid ra \in I\}$ is an ideal containing $I$ and $b$, thus it is principal and we have $I:a = (d)$ for $d \in R$. Now, pick any $i \in I \subseteq (I,a) = (c)$, it can be written as $uc$, hence $u\cdot (c) \subseteq I$. In particular, we have $ua \in I$, or equivalently $u \in I:a = (d)$. We obtain $u = vd$ and $i = vcd$ which means $I \subseteq (cd)$.
	
	However, we also have $(cd) \subseteq I$ because $da \in I$ implies $d(I,a) = d(c) \subseteq I$. We conclude that $I = (cd)$, but this contradicts the definition of $I$.
\end{proof}
\subsection{Integrality}
For the following, let $R$ be an integral domain, $K = \ff(R)$ and $L$ be a field containing $K$.
\begin{defn}
	An element $\alpha \in L$ is said to be \textbf{integral} over $R$ if $\alpha$ is the root of a monic polynomial with coefficients in $R$.
\end{defn}
\marginnote{
	\begin{exmp}
		Say $R = \Z$, $K = \Q$ and $L = \Q(i)$. The element $i$ is integral over $\Z$ since it solves $x^2+1 = 0$. The element $\frac{i}{2}$ is not integral because the minimal polynomial is $x^2 - \frac{1}{4} = 0$ and theorem \ref{minpolyint}.
	\end{exmp}}

\begin{thm}\label{intsubring}
	The set of elements of $L$ which are integral over $R$ is a subring of $L$.
\end{thm}
We will give two different proofs of this theorem based on two different lemmas.
\begin{lem}[Newton]
	Let $\{e_0, \dots, e_n\}$ be the elementary symmetric polynomials in $n$ variables, namely,
	\[e_i(x_1, \dots, x_n) = \sum_{1 \leq a_1 < \cdots < a_i \leq n} x_{a_1}\cdots x_{a_i}.\]
	All the symmetric polynomials\footnote{A polynomial $p$ in several variables is said to be symmetric if $p \circ \sigma = p$ for any permutation $\sigma$ of the variables.} in $R[x_1, \dots, x_n]$ are $R$-generated by the elementary symmetric polynomials.
\end{lem}
\begin{proof}[First proof of theorem \ref{intsubring}]
	Let $\alpha$ and $\beta$ be integral over $R$. Let $f \in R[x]$ be a monic polynomial that $\alpha$ satisfies. We can write \[f(x) = x^n+a_{n-1}x^{n-1}+\cdots + a_1x+a_0 =  (x-\alpha_1)\cdots (x-\alpha_n),\] where $\forall 1\leq i\leq n$, $a_i \in R$, the $\alpha_i$'s are in the splitting field of $f$ and $\alpha = \alpha_1$. If we expand the R.H.S., we obtain
	\[f(x) = \sum_{i=0}^n e_{n-i}(\alpha_1, \dots, \alpha_n)x^i,\] so we have\footnote{In words, applying elementary symmetric polynomials to $\alpha_1, \dots, \alpha_n$ yields elements of $R$.} 
	\[\forall 0 \leq i \leq n, e_{n-i}(\alpha_1, \dots, \alpha_n) = a_i \in R.\]
	By Newton's lemma, we infer that for any symmetric polynomial $p \in R[x_1, \dots, x_n]$, $p(\alpha_1, \dots, \alpha_n) \in R$.
	
	Let $g$ be the monic polynomial of $R[x]$ that $\beta$ satisfies and write $g = (x-\beta_1)\cdots(x-\beta_m)$. We can conclude in the same way that for any symmetric polynomial $p \in R[x_1, \dots, x_m]$, $p(\beta_1, \dots, \beta_m) \in R$.
	
	Let \[h(x) = \prod_{i=1}^n\prod_{j=1}^m (x-(\alpha_i + \beta_j)).\]
	Note that the coefficients of $h$ are symmetric polynomials in $\alpha_1,\dots,\alpha_n$ and $\beta_1,\dots, \beta_m$, and, by appropriately regrouping the terms, we can see them as symmetric polynomials in $\alpha_1, \dots, \alpha_n$ with coefficients being symmetric polynomials in $\beta_1, \dots, \beta_m$. %TODO write explicity the polynomials in a footnote.
	By the previous observation, we conclude $h(x) \in R[x]$. Since $h$ is monic and has $\alpha + \beta$ as a root, we have shown that $\alpha+\beta$ is integral. A similar construction works to show $\alpha\beta$ is integral.
\end{proof}
\marginnote{\begin{cor}\label{polyint}
	More generally, for any polynomial $g \in R[x_1, \dots, x_n]$ and integral elements $\alpha_1, \dots, \alpha_n \in L$, $g(\alpha_1, \dots, \alpha_n)$ is integral.
\end{cor}}
\begin{lem}\label{lemintchar}
	An element $\alpha \in L$ is integral over $R$ if and only if there exists a finitely generated $R$-submodule $M$ of $L$ such that $\alpha M \subseteq M$.
\end{lem}
\begin{proof}
	($\Rightarrow$) Let $f \in R[x]$ be a monic polynomial, say of degree $n$, that $\alpha$ satisfies. Observe that $M = R[\alpha]$ is finitely generated as a module because it is generated by $1, \alpha, \dots, \alpha^{n-1}$.\footnote{If we isolate $\alpha^n$ in $f(\alpha)$, we see that all higher powers of $\alpha$ are $R$-generated by the powers less than $n$.} It is clear that $\alpha M \subseteq M$.
	
	($\Leftarrow$) Suppose that $M = Re_1 + \cdots + Re_m$ is a finitely generated $R$-module and $\alpha M \subseteq M$. Since for any $1 \leq i \leq n$, $\alpha e_i$ can be written as an $R$-linear combination of the $e_j$'s, we have a matrix $A \in M_n(R)$ such that $A[e_1, \dots, e_n]^t= \alpha[e_1, \dots, e_n]^t$. Let $f_A= \det(xI_n-A)$ be the characteristic polynomial of $A$, it is a monic polynomial in $R[x]$ by definition. Moreover, we know that $f_A(\alpha) = 0$ because $\alpha$ is an eigenvalue of $A$, so we conclude that $\alpha$ is integral. We may also conclude this from Cayley-Hamilton which tells us that $f_A(A)$ is identically the zero map, then evaluating $f_A(A)$ at any non-zero $v \in M$ yields $f_A(\alpha) = 0$.
\end{proof}
\begin{proof}[Second proof of theorem \ref{intsubring}]
	Let $\alpha$ and $\beta$ be integral over $R$ and let $M = Re_1 + \cdots + Re_n$ and $N = Rf_1 + \cdots + Rf_m$ be the modules that satisfy the condition of the lemma for $\alpha$ and $\beta$ respectively.
	
	It is clear that $MN = \{xy \mid x \in M, y \in N\}$ is stable under multiplication by both $\alpha$ and $\beta$ and $MN$ is clearly generated by $\{e_if_j \mid i \in [n], j \in [m]\}$. Therefore, $\alpha+\beta$ and $\alpha\beta$ are integral over $R$.
\end{proof}

\begin{defn}
    The ring of elements of $L$ which are integral over $R$ is called the \textbf{integral closure} of $R$ in $L$.
\end{defn}
\begin{defn}
	An integral domain $R$ is said to be \textbf{integrally closed} if the integral closure of $R$ in $\ff(R)$ is $R$ itself.
\end{defn}


\begin{prop}\label{minpolyint}
	Assume that $R$ is integrally closed. An element $\alpha \in L$ is integral over $R$ if and only if its minimal polynomial is contained in $R[x]$.
\end{prop}
\begin{proof}
	($\Leftarrow$) Follows from the definition of integrality and the fact that the minimal polynomial is monic.
	
	($\Rightarrow$) Let $f$ be the minimal polynomial of $\alpha$ over $K$. For any root $\beta$ of $f$, we know\footnote{This is an important fact usually proved in an introduction to Galois theory course.} that there exists an isomorphism $\phi: K[\alpha] \rightarrow K[\beta]$ such that $\phi(\alpha) = \beta$ and $\phi|_K = \text{id}_K$. Let $p \in R[x]$ be the monic polynomial that $\alpha$ satisfies, we see from applying $\phi$ to $p$ that $\beta$ also satisfies it and hence any root of $f$ is integral over $R$. Finally, writing $f = (x-\alpha_1)\cdots (x-\alpha_n)$ and expanding, we see that the coefficients of $f$, being polynomials of integral elements, are integral as well (by corollary \ref{polyint}). As $R$ is assumed to be integrally closed and the coefficients lie in $K$, it follows that they are contained in $R$, hence we conclude that $f \in R[x]$ as desired.
\end{proof}


\begin{prop}\label{l/stors}
	Assume $L/K$ is an algebraic extension. If $S$ is the integral closure of $R$ in $L$, then $L/S$ is a torsion $R$-module.
\end{prop}
\begin{proof}
	We will show that if $\alpha \in L$, then there exists $d \in R$, such that $d\alpha \in S$. If $\alpha \in L$, then there is a monic polynomial with coefficients in $K$ satisfied by $\alpha$ (since $L$ is algebraic), $f(x) = x^n + a_{n-1}x^{n-1}+ \cdots +a_1x+a_0$, $a_i \in K$. We can find a non-zero $d \in R$ such that $da_i \in R$ for all $1\leq i\leq n$.\footnote{For instance, we can let $d$ be the product of the denominators of the $a_i$'s.} We see that $d\alpha$ satisfies $$y^n+da_{n-1}y^{n-1} + \cdots + d^{n-1}a_1y + d^na_0$$ which is monic and has coefficients in $R$. The proposition follows.
\end{proof}
\begin{cor}
	The field of fractions of $S$ is $L$.\footnote{Any element $\alpha \in L$ can be identified with $\frac{d\alpha}{d} \in \ff(S)$ as seen above. The corollary then follows because $\ff(S)$ is the smallest field containing $S$.
	}
\end{cor}
\begin{prop}\label{ufdint}
	If $R$ is a UFD, then $R$ is integrally closed.
\end{prop}
\begin{proof}
	Let $\alpha = \frac{a}{b} \in K$ be an integral element, since $R$ is a UFD, we can assume that $a$ and $b$ have no common irreducible factors. If $b \in R^{\times}$, then $\alpha \in R$.
	
	Otherwise, there exists an irreducible element $\pi \in R$ such that $\pi \mid b$. By integrality of $\alpha$, we have
	\[\bra{\frac{a}{b}}^n + c_{n-1}\bra{\frac{a}{b}}^{n-1}+ \cdots + c_1\frac{a}{b} + c_0= 0\]
	If we multiply this by $b^n$, we see that $\pi$ divides the R.H.S. and all terms other than the first in the L.H.S., and this leads to a contradiction.\footnote{Observe that $\pi \mid 0$ always holds and all terms but the first in the L.H.S. are multiples of $b$, hence multiples of $\pi$. However, the first term is $a^n$ and $\pi$ cannot divided, or $a$ and $b$ would have common factors.} We conclude that $\alpha$ being integral implies $\alpha \in R$.
\end{proof}
\begin{exmps}\label{exmpintclosed}
	\begin{enumerate}
		\item[]
		\item By the last result, $\Z$ and $\Z[i]$ are integrally closed because they are UFDs.
		\item By the contrapositive of the last proposition, any ring which is not integrally closed is not a UFD. For instance, $\Z[2i]$ is not integrally closed (because $i\notin \Z[2i]$ is integral over $\Z$) one can indeed verify that it is not a UFD.
		\footnote{Recall that if $p$ is prime and $p \equiv 1 \Mod{4}$, then there exists $a \in 2\Z$ and $b \notin 2\Z$ such that $p = a^2+b^2$. In other words, we have $a+bi \notin \Z[2i]$ and $|a+bi| = p$.
			
		Let $p_1, \dots, p_n$ be such primes with decompositions $p_j = a_j^2 + b_j^2$. Then, $$\alpha_{j,k} = (a_j+ib_j)(a_k+ib_k) \in \Z[2i]$$ is irreducible  because it factorizes in irreducible elements in the bigger ring $\Z[i]$. It is clear that $$\alpha_{1,2}\alpha_{3,4} = \alpha_{1,3}\alpha_{2,4},$$ so $\Z[2i]$ does not have a unique factorization.}
		
		\item Let us find for which $n$,  $R = \Z[\sqrt{n}]$ is integrally closed. The field of fraction is $K = \Q(\sqrt{n})$ and every element is of the form $a+b\sqrt{n}$ with $a,b \in \Q$. 
		
		If $n$ is a square, then $R= \Z$ nad $K = \Q$, so $R$ is integrally closed.
		
		If $n= d^2m$ with $d > 1$, then $\sqrt{m} = \frac{\sqrt{d^2}m}{d} \in \Q(\sqrt{n})$ is a root of $x^2-m \in \Z[x]$, but $\sqrt{m} \notin \Z[\sqrt{n}]$. Hence, $\Z[\sqrt{n}]$ is not integrally closed.
		
		Assume $n$ is square-free. %TODO: complete this.
	\end{enumerate}
\end{exmps}
\begin{defn}
	A ring extension $S/R$ is \textbf{integral} if $\forall \alpha \in S$, $\alpha$ is integral over $R$.
\end{defn}
\begin{prop}
	If $S/R$ is integral and finitely generated as an $R$-algebra, then $S$ is finitely generated as an $R$-module.
\end{prop}
\begin{proof}
	We proceed by induction on $n$, the number of generators of $S$ as an $R$-algebra. If $n=1$, then $S= R[\alpha]$, but $\alpha$ is integral, so $S$ is finitely generated as a module (it is generated by $1,\alpha, \dots, \alpha^{n-1}$).
	
	Suppose it is true for $n$ and write $S = R[\alpha_1, \dots, \alpha_n][\alpha_{n+1}]$. By induction hypothesis, we have $R[\alpha_1, \dots, \alpha_n] = R\beta_1 + \cdots + R\beta_N$ as an $R$-module. Moreover, since $\alpha_{n+1}$ is integral, $S$ is finitely generated as a $R[\alpha_1, \dots, \alpha_n]$-module. It follows that $S$ is finitely generated $R$-module.
\end{proof}
\marginnote{\begin{rem}
	If $S_2$ is a finitely generated integral extension of $S_1$ and $S_1$ is a finitely generated integral extension of $R$, then $S_2$ is a finitely generated integral extension of $R$.
\end{rem}}

\begin{exmp}%TODO: Understand this example
	We want to illustrate the connection between integrality and smoothness.
	
	Let $R = k[x]$, where $k$ is algebraically closed of characteristic $0$. Formally, we have $\spec(R) = \mathbb{A}_{k}^1 = \{(x - a) : a \in k\} \cup \{(0)\}$, but one can simply see $\mathbb{A}^1_k = k \cup \{*\}$ by identifying $(x-a)$ with $a$ and $(0)$ with $*$.\footnote{The spectrum of a ring $R$, denoted $\spec(R)$, is the set of prime ideals of $R$, it will be further studied in a later section. We will learn to see $\spec(R)$ as a geometric object associated to $R$. In general, an inclusion $R \subset S$ corresponds to a map $\spec(S) \rightarrow \spec(R)$. We want to give evidence here that the integral closure of $R$ ensures the smoothness of $\spec(R)$ and vice-versa.} Thus, $\spec(R) = k \cup \{*\}$ can geometrically be seen as a 1-dimensional $k$-line, with an extra point "at infinity". Formally, the point at infinity certainly has its importance, but for this example, think nothing of it. We see that $R$ is integrally closed and that $\spec(R)$ is smooth. \v
	
	In contrast, let $S = k[x, y]/(p(x,y))$ for some polynomial $p$, of degree $d$ in $y$. The structure of $\spec(S)$ is slightly more complicated now (it is a variety $V_p$ in $\mathbb{A}^2_k$), but we can think of it geometrically as the zero locus of $p(x,y) = 0$ in $k^2$. As $R \subset S$, we have that $V_p$ is related to $\spec(R)$ from the projection $\spec(S) = V_p \rightarrow \mathbb{A}_R^1 = \spec(R)$ that maps $(a,b) \mapsto a$. Since $k$ is algebraically closed, we know that given any point $a \in k$, there exists $d$ solutions (counting multiplicities) of $p(a, y) = 0$, so we can think of $V_p$ as a $d$-sheeted cover of $\mathbb{A}^1_k$ (i.e. there are $d$ points of $V_p$ above any given point of $\mathbb{A}^1_k$). \v
	
	If we pick the concrete example $S = k[x,y]/(y^2-x^2(x+1))$, we want to show that it is not integrally closed. This is not hard as $\frac{y}{x}$ satisfies $t^2 - (x+1)$. Moreover, the solution set $V_p$ of $y^2 = x^2(x+1)$ (i.e. $\spec(S)$) is not a smooth curve since the point $(0,0)$ is a singularity\footnote{Namely, $\frac{\partial F}{\partial x}(0,0) = \frac{\partial F}{\partial y}(0,0) = 0$ for $F = y^2 - x^2(x+1)$}. \v
	
	As an exercise, one can try to show that the integral closure of $R = k[x]$ in Frac($S$) is $\tilde{S} = k[x][\sqrt{x+1}] = k[x, y]/(y^2 - (x+1)) \cong k[y]$. This would again provide evidence of an integrally closed ring $\tilde{S}$ such that $\spec(\tilde{S}) = \mathbb{A}^1_k = k$ is smooth. Moreover, one can check that the natural inclusion $S \subset \tilde{S}$ now provides the map $\spec(\tilde{S}) = \mathbb{A}^1_k \to \spec(S) = V_p$ as $a \mapsto (a^2 - 1, a(a^2-1))$. \footnote{This example is trying to motivate the notion of integrality, but one does not really need to understand it to move on in the notes.} In algebraic geometry, we say that this map provides is the \emph{resolution of singularity} of $\spec(S)$. One can take $k = \C$ and plot the "$\R$ part" of $a \mapsto (a^2 - a, a(a^2 - 1))$ to see that this essentially takes the real line and "loops it" to cover the "nodal curve" $y^2 = x^2(x+1)$ (plot in Desmos to understand this name). This cover is one-to-one, except over the singularity $(0,0)$ where both $0, 1 \in \R$ map to it, so this map can be seen as pulling apart both branches of the "nodal curve", creating a smooth line. This motivates the name for "resolution of singularity".
\end{exmp}

Our next big goal is motivated by the following construction. Let $K$ be a finite extension of $\Q$ (resp. of $k(x)$, with $k$ a finite field). The integral closure of $\Z$ (resp. of $k[x]$) in $K$ is called the ring of integers of $K$ and denoted $\mO_K$. More generally, if $L$ is a finite extension of $K$, then $\mO_K \subset \mO_L$. We know that $L \cong K^n$ as a $K$-module, where $n = [L:K]$.

\begin{quest}
	Is it true that $\mO_K \cong \Z^n, n = [K:\Q]$ or $\mO_L \cong \mO_K^m, m = [L:K]$?
\end{quest}
We will see that the answers are yes and no respectively.
\begin{defn}
	Let $R$ be an integral domain. An $R$-module $M$ is said to be \textbf{free} if $M$ has an $R$-basis.\footnote{i.e.: there exists a set $\{e_{\alpha}\}_{\alpha \in I}$ that is linearly independent and generates $M$. If, in addition, $M$ is finitely generated, then $M \cong R^n$ where $n = |I|$.}
\end{defn}
\begin{rem}
	Let $S$ be an $R$-algebra which is free of finite rank over $R$. For an element $\alpha \in S$, we can define $m_{\alpha}: S\rightarrow S$ to be the endomorphism sending $x$ to $\alpha x$. If $e_1, \dots, e_n$ forms an $R$-basis for $S$, then we can see $m_{\alpha}$ as an $n\times n$ matrix with entries in $R$.
\end{rem}
\begin{defn}
	In the setting of the above remark, we define the \textbf{trace} of $\alpha$ as the sum of the diagonal elements of the matrix $m_{\alpha}$ and denote it $\Tr_{S/R}(\alpha)$. The \textbf{norm} of $\alpha$ is the determinant of this matrix and we denote it $\Nm_{S/R}(\alpha)$. Note that the trace and norm are invariant on the choice of basis.
\end{defn}
\begin{exmp}\label{normandtracequaternion}
	Consider the Hamilton quaternion $\mathbb{H} = \R + \R i + \R j + \R k$. Given an element $\alpha = a + bi + cj + dk \in \mathbb{H}$, one can define $\Tr(\alpha) = 2a$ and $\Nm(\alpha) = a^2 + b^2 + c^2 + d^2$. On the other hand, recall that there is an embedding $\mathbb{H} \subset M_2(\C)$ given from
	\[
	    1 \mapsto \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \ \ ; \ \ i \mapsto \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix} \ \ ; \ \ j \mapsto \begin{pmatrix} 0 & i \\ i & 0 \end{pmatrix} \ \ ; \ \ k \mapsto \begin{pmatrix} i & 0 \\ 0 & -i \end{pmatrix}
	\]
	then one readily verifies that the trace of an element corresponds to its trace as a matrix, and its norm is its determinant. This can serve as motivation for our definitions above.
\end{exmp}
\marginnote{\begin{prop}\label{proptrace}[Some properties of the trace and norm]
	For any $\alpha_1, \alpha_2 \in S$ and $a \in R$, we have
	\begin{align*}
		\Tr_{S/R}(\alpha_1 + \alpha_2) &= \Tr_{S/R}(\alpha_1) + \Tr_{S/R}(\alpha_2)\\
		\Tr_{S/R}(a\alpha_1) &= a\Tr_{S/R}(\alpha_1)\\
		\Nm_{S/R}(\alpha_1\alpha_2) &= \Nm_{S/R}(\alpha_1)\Nm_{S/R}(\alpha_2)\\
		\Nm_{S/R}(a\alpha_1) &= a^n\Nm_{S/R}(\alpha_1).
	\end{align*}
\end{prop}}
\begin{defn}
	Let $K < L$ be an extension of fields, we define the \textbf{trace form} on $L/K$ as $\inp{a}{b} = \Tr_{L/K}(ab)$ for any $a,b \in L$. Using the properties in proposition \ref{proptrace}, one can show that $\inp{\cdot}{\cdot}$ is $K$-bilinear.
\end{defn}
\begin{prop}\label{non-deg}
	Suppose that $L/K$ is separable, then $\inp{\cdot}{\cdot}$ is non-degenerate, i.e.:
	\[\{a \in L \mid \inp{a}{x} = 0, \forall x \in L\} = \{a \in L \mid \inp{x}{a} = 0, \forall x \in L\} = \{0\}.\]
\end{prop}
We need the following lemma that we state without proof.
\begin{lem}
	A field extension $L < K$ is separable if and only if $\Tr_{L/K} \not\equiv 0$.
\end{lem} %TODO Proof of lemma
\begin{proof}[Proof of proposition \ref{non-deg}]
	Assume towards a contradiction that $0\neq a \in L$ is such that $\inp{a}{x} = 0$ for all $x \in L$, then $aL \subseteq \{\ell \in L \mid \Tr_{L/K}(\ell) = 0\}$. On the other hand, since the multiplication map $a: L \rightarrow L$ is an isomorphism of $K$-vector spaces (its inverse being multiplication by $a^{-1}$), we have $aL = L$. Thus, all elements have trace zero, but this contradicts the lemma.
\end{proof}
As a consequence of this result, we get a $K$-vector space isomorphism $L \rightarrow \Hom(L,K)$ that sends $a$ to $\inp{a}{\cdot}$.\footnote{The fact that the trace form is bilinear and non-degenerate yields injectivity and surjectivity follows because both sides have the same dimension.
} If $\{\gamma_1, \dots, \gamma_d\}$ is a basis for $L$ as a $K$-vector space, then we write $\gamma_1^*, \dots, \gamma_d^*$ for the dual basis of $L^*$ where $\gamma_j^*(\gamma_k) = \delta_{j,k}$ (Kronecker delta). The isomorphism lets us think of these basis elements as living in $L$ and write \[\inp{\gamma_i}{\gamma_j^*} = \delta_{i,j}.\]

\begin{prop}
    Let $L/K$ be an extension of fields of degree $n$, and let $\beta \in L$ with minimal polynomial $f$ over $K$. Say that $\beta_1, \ldots, \beta_m$ are the disctinc roots of $f$ in $L$. Then,
    \[
        \Tr_{L/K}(\beta) = r(\beta_1 + \cdots + \beta_m) \ \ \ \text{and} \ \ \ \Nm_{L/K}(\beta) = (\beta_1 \cdots \beta_m)^r
    \]
    where $r = [L : K[\beta]] = n/m$.
\end{prop}
\begin{cor}\label{intTrNm}
    Let $L/K$ and $\beta$ be as above. If $L/K$ is separable, with $\mathrm{Aut}(L/K) = \{\sigma_1, \ldots, \sigma_n\}$, then
    \[
        \Tr_{L/K}(\beta) = \sigma_1\beta + \cdots + \sigma_n\beta) \ \ \ \text{and} \ \ \ \Nm_{L/K}(\beta) = \sigma_1\beta \cdots \sigma_n\beta
    \]
    and so, if we also have that $R$ is integrally closed and $\beta \in L$ is integral over $R$, then both $\Tr_{L/K}(\beta)$ and $\Nm_{L/K}(\beta)$ are in $R$.
\end{cor}
\begin{prop}\label{fingenint}
	Assume that $R$ is integrally closed in $K = \ff(R)$, let $K < L$ be a separable field extension of degree $d$ and $S$ be the integral closure of $R$ in $L$. Then, there exist free $R$-modules $\Omega$ and $\Omega^*$ of rank $d$ in $L$ such that $\Omega \subseteq S \subseteq \Omega^*$.
\end{prop}
\begin{proof}
	Let $\gamma_1, \dots, \gamma_d$ be a basis for $L/K$. By proposition \ref{l/stors}, there exists some $0\neq a \in R$ such that $a\gamma_i \in S$ for all $i$. Set $\gamma_i = a\gamma_i$, they still form a linearly independent set over $K$, hence $\Omega = R\gamma_1 \oplus \cdots \oplus R\gamma_d$ is a free $R$-submodule of $L$ and clearly $\Omega \subseteq S$.
	
	Let $\Omega^* = R\gamma_1^* \oplus \cdots \oplus R\gamma_d^*$ (with the identification of $\gamma_i^*$ in $L$). We claim that $S \subseteq \Omega^*$. If $\alpha \in S$, then we can write $\alpha = x_1\gamma_1^* + \cdots + x_d\gamma_d^*$ where $x_i \in K$, since $\{\gamma_1^*, \ldots, \gamma_d^*\}$ also provides a $K$-basis of $L$. Since $\inp{\cdot}{\cdot}$ is $K$-bilinear, we have 
	\[
	    \Tr_{L/K}(\alpha \cdot \gamma_j) = \inp{\alpha}{\gamma_j} = \sum_{i=1}^d x_i\inp{\gamma_i^*}{\gamma_j} = x_j.
	\]
	
	As $\alpha$, $\gamma_j \in S$, we know that $x_i = \Tr(\alpha \gamma_j) \in R$ from corollary \ref{intTrNm}. The proposition follows.
\end{proof}
\begin{cor}
	If $R$ is Noetherian, then $S$ is a finitely generated $R$-module. If $R$ is a PID, then $S = R^d$.
\end{cor} %TODO proof of that

\begin{thm}\label{intclosefingen}%TODO: I think corollary is enough here.
	Let $S$ be the integral closure of $R$ in a separable extension $L/K$ of degree $d$. Then, $S$ be a finitely generated $R$-module and if $R$ is PID, then $S$ is free of rank $d$ over $R$.
\end{thm}

\subsection{Dedekind Domains}
\begin{defn}
	An integral domain $R$ is a \textbf{Dedekind domain} if 
	\begin{enumerate}[(i)]
		\item $R$ is Noetherian (finiteness),
		\item $R$ is integrally closed (smoothness), and
		\item Every non-zero prime ideal of $R$ is maximal.\footnote{Note that a Dedekind can have Krull dimension zero or one. In the former, the last property is vacuously satisfied.}
	\end{enumerate}
\end{defn}
\begin{exmp}
    We have already seen that $\Z[i]$ satisfy theses three properties,\footnote{They are Noetherian because they are the quotient of the Noetherian ring $\Z[x]$, they are integrally closed by examples \ref{exmpintclosed} and they have Krull dimension one by examples \ref{exmpkrulldim}.} so they are Dedekind domains.
\end{exmp}
\begin{prop}
	If $R$ is a Dedekind domain and $S \subseteq R\setminus \{0\}$ is a multiplicative set, then $R[S^{-1}]$ is also a Dedekind domain.
\end{prop}
\begin{proof}
	We will prove that if $R$ satisfies any property from the definition above, then $R[S^{-1}]$ also satisfies it.
	\begin{enumerate}[(i)]
		\item Suppose $R$ is Noetherian and let $\mathfrak{a} \lhd R[S^{-1}]$, we have $\mathfrak{a} = (\mathfrak{a} \cap R)[S^{-1}]$. Moreover, since $R$ is Noetherian, $\mathfrak{a} \cap R$ is finitely generated by $x_1, \dots, x_n \in R$, but then $x_1, \dots, x_n$ also generate $\mathfrak{a}$ as an $R[S^{-1}]$ module. We conclude that any ideal of $R[S^{-1}]$ is finitely generated, so $R[S^{-1}]$ is Noetherian.
		\item Suppose $R$ is integrally closed and let $\alpha \in \ff(R) = \ff(R[S^{-1}])$ be integral over $R[S^{-1}]$, then $\alpha$ satisfies the polynomial $x^n+ a_{n-1}x^{n-1}+ \cdots + a_0$ with $a_i \in R[S^{-1}]$. By the construction of $R[S^{-1}]$, there exists $s \in S$ such that $sa_i \in R$ for every $i$. Multiplying the polynomial by $s^n$, we find that $s\alpha$ satisfies a monic polynomial with coefficients in $R[x]$.\footnote{\begin{align*}s^n\alpha^n + s^na_{n-1}\alpha^{n-1} + \cdots + s^na_0 &= 0\\(s\alpha)^n + sa_{n-1}(s\alpha)^{n-1} + \cdots + s^na_0&=0\end{align*}} Since $R$ is integrally closed, $sa \in R$, implying $a \in R[S^{-1}]$. We conclude that $R[S^{-1}]$ is integrally closed.
		\item Suppose $R$ has Krull dimension one. Recall that $\spec(R[S^{-1}]) = \spec(R) \setminus I_S$ where $I_S$ is the set of prime ideals of $R$ that intersect $S$. Moreover, we have the following natural correspondence for $a \in \spec(R[S^{-1}])$: $R[S^{-1}]/a \cong R/(a \cap R)$. However, $a \cap R$ is a non-zero prime ideal, so it must be maximal. This implies the R.H.S. is a field, so the L.H.S. is also a field, yielding that $a$ is a maximal ideal. We conclude that $R[S^{-1}]$ has Krull dimension one. %TODO explain natural correspondnece
	\end{enumerate}
\end{proof}
\begin{cor}
	If $\lp$ is a prime ideal of a Dedekind domain $R$, then $R_{\lp}$\footnote{If $\lp$ is a prime ideal, then $R-\lp$ is a multiplicative set not containing 0 and $R_{\lp}$ denotes the localization of $R$ at $R-\lp$.} is a Dedekind domain with a unique non-zero prime ideal $\lp R_{\lp}$.
\end{cor}
\begin{proof}
	We just saw that $R_{\lp}$ is a Dedekind domain. Assume towards a contradiction that there is a non-zero prime ideal other than $\lp R_{\lp}$, by one-dimensionality, it cannot be contained or contain $\lp R_{\lp}$. Hence, it contains some $0 \neq x \in R_{\lp} \setminus \lp R_{\lp}$. We can write $x = \frac{a}{b}$ where $a,b \in R \setminus \lp$ are non-zero, thus $x$ is a unit with inverse $\frac{b}{a}$, thus this ideal is the whole ring, contradicting its primeness.
\end{proof}
\begin{defn}
	A Dedekind domain which is local (i.e.: has a unique non-zero prime ideal) is called a discrete valuation ring (DVR).
\end{defn}
\begin{prop}
	Assume $R$ is Noetherian.\footnote{This assumption is necessary as there exists non-Noetherian domains with all their localizations being DVR's. Instead of assuming $R$ is Noetherian, one could also assume every non-zero element of $R$ is contained in finitely many maximal ideals.} If all the localizations at non-zero prime ideals of $R$ are DVR's, then $R$ is a Dedekind domain.
\end{prop}
\begin{proof}
	We need to show that $R_{\lp}$ being Dedekind for all $\lp \in \spec(R)$ implies $R$ is Dedekind. Suppose all the localizations at prime ideals are Dedekind domains, then we show $R$ satisfies each property.
	\begin{enumerate}[(i)]
		%\item Let $I \lhd R$ and $I_{\lp}$ be the ideal it generates in localization at a prime ideal $\lp$. Because $R_{\lp}$ is Noetherian, $I_{\lp}$ is generated by $\frac{i_1}{p_1}, \dots, \frac{i_n}{p_n}$. It is then clear that $\frac{i_1}{1}, \dots, \frac{i_n}{1}$ also generates it and hence that $i_1, \dots, i_n$ generates $I_{\lp}\cap R  = I$. **WRONG PROOF**
		\item By assumption.
		\item Let $\alpha \in \ff(R)$ be integral over $R$ and $\mathfrak{a} = \{x \in R \mid x\alpha \in R\} \lhd R$. For all prime ideals $\lp$, $\alpha$ is integral over $R_{\lp}$ (since $R \subseteq R_{\lp}$), so $\alpha \in R_{\lp}$ because $R_{\lp}$ is integrally closed. Then, there exists $s \in R-\lp$ such that $s\alpha \in R$, thus the ideal $\mathfrak{a}$ is not contained in $\lp$. Letting $\lp$ vary over $\spec(R)$, we conclude that $\mathfrak{a}$ is not contained in any maximal ideal implying $\mathfrak{a} = R$. Since $1 \in\mathfrak{a}$, $1\alpha = \alpha \in R$, so $R$ is also integrally closed.
		\item Assume $\lp$ be a non-zero prime ideal that is not maximal, then it is contained in another prime ideal $\mathfrak{q}$. However, we observe that $\lp R_{\mathfrak{q}} \subset \mathfrak{q}R_{\mathfrak{q}}$ are also non-zero prime ideals of $R_{\mathfrak{q}}$ and this contradicts the fact $R_{\mathfrak{q}}$ is a DVR.
	\end{enumerate}
\end{proof}
\begin{thm}
	Let $A$ be a DVR (and not a field) and $\lp \lhd R$ be the unique prime ideal, then $\lp$ is a principal ideal.
\end{thm}
\begin{proof}
	Let $0\neq c \in A \setminus A^{\times}$. Observe that $A/(c)$ is a non-zero $A$-module so we can choose $0\neq b+(c)\in A/(c)$ such that $I = \ann_{A/(c)}(b+(c)) \lhd A$ is maximal among all choices of $b$.\footnote{This choice can be made because $A$ is Noetherian, thus every collection of ideals (in our case $\{\ann_{A/(c)}(m) \mid m \in A/(c) \setminus \{0\}\}$) has a maximal element.} In other words, we choose $b$ such that there is no proper ideal of $A$ which arises as $\ann_{A/(c)}(x+(c))$ for some $x \in A$ and properly contains $I$.
	
	First, we claim that $I$ is a non-zero prime ideal. Suppose $x,y \in A$ and $xy \in I$, namely, $xyb \in (c)$. If $x \notin I$, then $xb \notin (c)$ and $xb+(c)$ is non-zero in $A/(c)$. Let $I' = \ann_{A/(c)}(xb+(c))$, we clearly have $I' \supseteq I$ and $I' \supseteq (y)$.\footnote{If $ab \in (c)$, then $axb \in (c)$, so \[I' = \ann_{A/(c)}(xb+(c)) \supseteq \ann_{A/(c)}(b+(c)) = I,\] and since $yxb + (c) = 0$, we have $(y) \in \ann_{A/(c)}(xb+c)$.} Therefore, $I' = I$ by maximality of $I$ and we obtain $y \in I$. We conclude that $I$ is prime (it is not the whole ring by definition and it is non-zero because $c \in I$).
	
	Second, we must have $\lp = I$, so it is enough to show that $I$ is principal. Since $c$ is not a unit, $\frac{b}{c} \in \ff(A)$ is not in $A$. Furthermore, by definition of $I$, $\frac{b}{c}I \lhd A$. Assume towards a contradiction that $\frac{b}{c}I \subseteq I$, then since $I$ is a finitely generated $A$-module\footnote{Since $A$ is Noetherian.}, we find that $\frac{b}{c}$ is integral (by proposition \ref{lemintchar}) and $\frac{b}{c} \in A$ by integral closure, so we get a contradiction. Thus, we must have $\frac{b}{c}I = A$, then $I = (\frac{c}{b})$. In particular, $b$ divides $c$ and $\pi = \frac{c}{b}$ is a generator for $I = \lp$.
\end{proof}
\begin{prop}
	If $A$ is a DVR, then $A$ is a PID.
\end{prop}
\begin{proof}
	Let $\pi$ be a generator of $\lp$, the unique non-zero prime ideal of $A$ and let $I \lhd A$. We will show that $I$ is generated by one element. Consider the sequence $I \subseteq \pi^{-1}I \subseteq \pi^{-2}I \subseteq \cdots$ in the fraction field $K = \ff(A)$. We claim that $\pi^{-j}I \neq \pi^{-(j+1)}$ for any $j$, otherwise $\pi^{-1}(\pi^{-j}I) \subseteq \pi^{-j}I$ implies that $\pi^{-1}$ is integral\footnote{It follows from proposition \ref{lemintchar} and the fact that $\pi^{-j}I$ is finitely generated because $A$ is Noetherian.} and hence $\pi^{-1} \in A$ contradicting the primeness of $\lp = (\pi) = A$. We conclude that this sequence is strictly increasing. Since $A$ is Noetherian, there must be some $j$ such that $\pi^{-j}I \subseteq A$ and $\pi^{-1}(\pi^{-j}I) \not\subseteq A$, thus $\pi^{-j}I \not\subseteq (\pi)$. However, $(\pi)$ is the only maximal ideal, so we must have $\pi^{-j}I = A$ and we obtain $I = \pi^jA = (\pi^j)$.
\end{proof}
\begin{cor}\label{dvrgenideal}
	Every ideal of a DVR is generated by $\pi^j$ where $j\in \N$ and $\pi$ is the generator of the unique prime ideal.
\end{cor}
\begin{cor}
	If $A$ is a DVR, then every $a \in A$ can be written as $u\pi^n$ where $n \geq 0$ and $u \in A^{\times}$.
\end{cor}
\begin{proof}
	Write $(a) = (\pi^j)$ for some $j$ and conclude that $a$ and $\pi^j$ must be associates.\footnote{i.e.: there exists $u \in A^{\times}$ such that $a = u\pi^n$.}
\end{proof}
\begin{cor}
	Every element of $K = \ff(A)$ can be written as $u\pi^{j}$ where $j \in \Z$ and $u \in A^{\times}$.
\end{cor}
\begin{defn}
	We define $j$ to be the valuation of $a$ in $A$ and write $v(a) = j$. We also write $v(a) = \ord_{\pi}(a)$.
\end{defn}
\begin{prop}
	$v(ab) = v(a)+v(b)$ and $v(a+b) \geq \min\{v(a), v(b)\}$.\footnote{Write $a = u\pi^i$ and $b = u'\pi^j$, without loss of generality, $i\leq j$. Then,
	\[ab = u\pi^iu'\pi^j = u''\pi^{i+j},\]
	so $v(ab) = i+j =  v(a) + v(b)$. Also,
	\[a+b = \pi^i(u+u'\pi^{j-i}) = \pi^i(u''\pi^k) = u''\pi^{i+k},\]
	hence $v(a+b) \geq \min\{v(a), v(b)\}$.}
\end{prop}
\begin{exmp}
	Recall that $A = \Z[\sqrt{-5}]$ is a Dedekind domain but $2\cdot 3 = (1+\sqrt{-5})(1-\sqrt{-5})$ are two decompositions of $6$ into irreducibles, so it is not a UFD. We can also write the prime ideal decomposition $(6) = (2,1+\sqrt{-5})^2(3, 1+\sqrt{5})(3,1-\sqrt{5})$. %TODO explain how to get that
	
	Let $\lp = (3,1+\sqrt{-5})$, it is a prototypical example of a non-principal ideal in $\Z[\sqrt{-5}]$. Consider the localization of $A$ at $\lp$ and the ideal that $\lp$ generates. It is now principal because $1+\sqrt{-5}$ divides both $1+\sqrt{-5}$ and $3$ (as $3 = \frac{1-\sqrt{-5}}{2}\cdot (1+\sqrt{-5})$).
\end{exmp}
\begin{exer}
	Show that $(2,1+\sqrt{-5})$ is principal in $A_{\lp}$ with $\lp = (2,\sqrt{-5})$, but $(2,1+\sqrt{-5}) \neq (2)$.
\end{exer}%TODO: Solve this exercise.

Our next goal is to show that ideals have unique decompositions as prime ideals in Dedekind domains.

\begin{lem}\label{containprod}
	Suppose $A$ is a Noetherian integral domain, then any ideal $I\lhd A$ contains a product of prime ideals.
\end{lem}
\begin{proof}
	Let $\Sigma$ be the set of ideals $I \lhd A$ which do not contain a product of prime ideals. Assume towards a contradiction that $\Sigma$ is not empty, let $J$ be a inclusion-wise maximal in $\Sigma$.\footnote{$A$ being Noetherian guarantees the existence of $J$.} In particular, $J$ is not prime, so there exists $a,b \notin J$, $ab \in J$. Let $J_1 = (a) + J$ and $J_2 = (b) + J$, by maximality, we can find prime ideals $p_1, \dots, p_r$ and $q_1, \dots, q_s$ such that $J_1 = p_1\cdots p_r$ and $J_2 = q_1\cdots q_s$, thus $J \supset J_1J_2 \supseteq p_1\cdots p_rq_1\cdots q_s$, which contradicts $J \in \Sigma$.
\end{proof}
\begin{lem}\label{coprimepowers}
	If $I$ and $J$ are relatively prime ideals\footnote{i.e.: $I+J = A$.}, then so are $I^m$ and $J^n$ for any $m,n \in \N$.
\end{lem}
\begin{proof}
	Since $I + J = A$, there exists $i \in I$ and $j \in J$ such that $1 = i+j$ and we can write
	\[1 = 1^{m+n} = (i+j)^{m+n} = \sum_{k=0}^{m+n}i^{m+n-k}j^k.\]
	Notice that every term of the R.H.S. is either divisible by $i^m$ or by $j^n$, thus $1 \in I^m+J^n$. The lemma follows.
\end{proof}
\begin{lem}\label{moddinginlocal}
	Let $A$ be a Dedekind domain. For any prime ideal $\lp \lhd A$, the inclusion $A \hookrightarrow A_{\lp}$ induces an isomorphism $A/\lp^n \cong A_{\lp}/\lp^nA_{\lp}$ for any $n \in \N$.
\end{lem}
\begin{proof}
	Fix $n \in \N$, let $f: A\rightarrow A_{\lp}/\lp^nA_{\lp}$ be the composition of the inclusion with the projection $A_{\lp} \twoheadrightarrow A_{\lp}/\lp^nA_{\lp}$ and note that \[\ker(f) = A \cap \lp^n[(R-\lp)^{-1}] = \lp^n.\] Moreover, to see that $f$ is surjective, observe that for any $\frac{a}{s} \in A_{\lp}$, we have $\lp^n + (s) = A$,\footnote{Since $s$ is not in the unique maximal ideal $\lp$, $s$ and $\lp$ must generate $A$ otherwise $\lp^n + (s)$ would sit in a different maximal ideal.} so there exists $x \in \lp^n$ and $y \in A$ such that $x+sy = 1$. Consequently, $y$ is sent to $\frac{1}{s} \Mod{\lp^n}$ and $ay$ to $\frac{a}{s} \Mod{\lp^n}$. The lemma follows from the first isomorphism theorem.
\end{proof}
\begin{thm}
	If $A$ is a Dedekind domain, then every non-zero ideal $I$ can be uniquely written (up to permutations) as $I= p_1^{e_1}\cdots p_r^{e_r}$ where $e_j \geq 0$ and $p_j$ are prime ideals.
\end{thm}
\begin{proof}
	Let $I \lhd A$, lemma \ref{containprod} yields $J = \lp_1^{r_1} \cdots \lp_n^{r_n} \subseteq I$. By the Chinese Remainder Theorem (CRT), lemma \ref{coprimepowers} which says that $\lp_i^{r_i}$ is coprime to $\lp_j^{r_j}$ for all $i\neq j$ and lemma \ref{moddinginlocal}, we obtain\footnote{Abusing notation, we write $\lp_i^i$ for both the ideal in $A$ and the ideal generated by $\lp_i^i$ in $A_{\lp_i}$.}:
	\[A/J = A/\lp_1^{r_1} \cdots \lp_n^{r_n} \cong A/\lp_1^{r_1} \times \cdots \times A/\lp_n^{r_n} \cong A_{\lp_1}/\lp_1^{r_1} \times \cdots \times A_{\lp_n}/\lp_n^{r_n}.\]
	The map $A\rightarrow A/J$ induces a bijection between the ideals of $A$ containing $J$ and the ideals of $I/J$. The image of $I$ in $A_{\lp_1}/\lp_1^{r_1}\times \cdots \times A_{\lp_n}/\lp_n^{r_n}$ is of the form \[\pi_1^{e_1}A_{\lp_1}/\lp_1^{r_1}\times \cdots \times \pi_n^{e_n}A_{\lp_n}/\lp_n^{r_n},\] where $e_i \leq r_i$ \footnote{Because each $A_{\lp_i}$ is a DVR where every ideal is generated by some power of $\pi_i$ (the generator of the unique prime ideal), recall corollary \ref{dvrgenideal}. To see that $e_i \leq r_i$, observe that $\pi_i \in \lp_i$, so it vanishes in $A_{\lp_i}/\lp_i^{r_i}$ when raised to the power $r_i$.}. On the other hand, the ideal $\lp_1^{e_1} \times \cdots \times \lp_r^{e_r}$ is another ideal containing $J$ which has the same image in $A/J$, so this is the decomposition of $I$. Furthermore, the exponents depend only on $I$.
\end{proof}
Many properties of ideals in Dedekind domains can be checked "locally".
\begin{cor}
	\begin{align*}
		I = J &\Leftrightarrow IA_{\lp} = JA_{\lp}, \forall \lp \in \spec(A)-0\\
		I \subseteq J &\Leftrightarrow IA_{\lp} \subseteq JA_{\lp}, \forall \lp \in \spec(A) - 0
	\end{align*}
\end{cor}
\begin{proof}
	%TODO:
\end{proof}
\begin{cor}
	If a Dedekind domain $A$ has finitely many prime ideals, then $A$ is a PID.
\end{cor}
\begin{proof}
	Suppose $\spec(A) = \{0, \lp_1, \dots, \lp_r\}$, it is enough to show $\lp_i$ is principal for every $i$.\footnote{Recall the proof of proposition \ref{krulldimoneufdispid}.} Choose $a \in A$ which is congruent to $\pi_i$ modulo $\pi_i^2$ where $\pi_i$ generates $\lp_iA_{\lp_i}$ and congruent to $1$ modulo $\lp_k$ for any $k \neq i$. By the CRT, we can find such an $a$ and it is clear that $(a) = \lp_i$.%TODO: explain further
\end{proof}
\begin{lem}\label{twoideals}
	If $I \supseteq J$ are two ideals of $A$, then there exists $a \in I$ such that $I = (a) + J$.
\end{lem}
\begin{proof}
	Consider the decompositions
	\[I = \lp_1^{e_1} \cdots \lp_r^{e_r} \quad \quad J = \lp_1^{f_1} \cdots \lp_r^{f_r}, \forall 1\leq i \leq r, e_r \leq f_r,\]
	where some $e_i$ can be zero. Choose $a \in \pi_j^{e_j} \setminus \lp_j^{e_j+1}$ for $j = 1,\dots, r$. %TODO: explain
\end{proof}

\begin{prop}
	Any ideal in a Dedekind domain $A$ can be generated by at most two elements of $A$.
\end{prop}
\begin{proof}
	Choose an element $0\neq b \in I$, we have $I \supseteq (b) \neq 0$ and by lemma \ref{twoideals}, $\exists a \in I$, $(a)+ (b) = I$.
\end{proof}
\begin{prop}
	If $I$ is a non-zero ideal of $A$, then there exists an ideal (far from unique) $J$ such that $IJ$ is principal.
\end{prop}
\begin{proof}
	Write $I = \lp_1^{e_1}\cdots \lp_r^{e_r}$ and pick $a \in I$, then $(a) = \lp_1^{f_1}\cdots \lp_r^{f_r}$\footnote{Although it was used before the choice of $a$, the index $r$ is determined by it. Indeed, $(a)$ might have more primes in its decomposition than $I$, but for simplicity, we assume that some $e_j$'s might be zero.} with $e_j\leq f_j$. Let $J = \lp_1^{f_1-e_1} \cdots \lp_r^{f_r-e_r}$. The proposition follows.
\end{proof}
\begin{defn}
	For a Dedekind domain $A$, we define the multiplication monoid of ideals of $A$ as $I_A = \{0 \neq I \lhd A\}$ with operation being multiplication of ideal. We denote $P_A$ to be the submonoid of non-zero principal ideal, it corresponds exactly to $(A-\{0\})/A^{\times}$.\footnote{A corollary of this last proposition is that $I_A/P_A$ is an abelian group (any element has an inverse) it is called the class group of $A$.}
\end{defn}

\begin{thm}
	Let $A$ be a Dedekind domain, $K = \ff(A)$, and $B$ be the integral closure of $A$ in $L$, a finite extension of $K$. Then, $B$ is a Dedekind domain.
\end{thm}
\begin{proof}
	\begin{enumerate}[(i)]
		\item[]
		\item We already showed $B$ is finitely generated as an $A$-module in theorem \ref{intclosefingen}. Consequently, any ideal of $B$ is finitely generated as an $A$-module\footnote{Recall that a ring $R$ is Noetherian if and only if any submodule of a finitely generated $R$-module is finitely generated.} and, a fortiori, as a $B$-module. We conclude that $B$ is Noetherian.
		\item $B$ is integrally closed by definition.
		\item Let $\beta$ be a non-zero prime ideal of $B$. Let $b \in \beta$ be non-zero, it is integral over $A$, so it satisfies 
		\[b^n + a_{n-1}b^{n-1} + \cdots + a_1b+ a_0 = 0,\]
		where $a_i \in A$ and $a_0 \neq 0$. After isolating $a_0$, we see that $a_0$ is $A$-generated by powers of $b$, so that $a_0 \in \beta \cap A =: \lp \lhd A$, it is clear that $\lp$ is a non-zero prime ideal\footnote{It is non-zero because it contains $a_0$ and it is prime because $\beta$ was prime in the bigger ring $B$.}. Consider $B/\beta$ as an $A/\lp$-algebra, the containment map being 
		\[a + \lp \mapsto a + \beta : A/\lp \hookrightarrow B/\beta.\]
		The fact that $B$ is a finitely generated $A$-module implies that $B/\lp$ is a finitely generated $(A/\lp)$-module and hence $B/\beta$ is finitely generated over $(A/\lp)$. Now, we also have that $\lp$ is maximal because $A$ is Dedekind, so $A/\lp$ is a field and we obtain that $B/\beta$ is a finite dimensional vector space. Moreover, since $B/\beta$ is an integral domain\footnote{Because $\beta$ is prime.}, multiplying by any non-zero element yields a full rank linear map. Therefore, we can find an inverse to any non-zero element and we can conclude that $B/\beta$ is a field and that $\beta$ is maximal.
	\end{enumerate}
\end{proof}

Until the end of this section, we will work in the following setting. Let $A$ be a Dedekind domain, $K = \ff(A)$, $K < L$ be a field extension of degree $n$ and $B$ be the integral closure of $A$ in $L$ such that $B$ is locally free over $A$.\footnote{i.e.: for any prime ideal $\lp \lhd A$, $B_{\lp B}$ is free over $A_{\lp}$.}
\begin{defn}
	If $\beta$ is a non-zero prime ideal of $B$, then $\dim_{A/(\beta \cap A)}(B/\beta)$ is called the residue degree of $\beta$ and denoted $f_{\beta}$. If there exists some prime ideal $\lp \lhd A$ such that $\lp B$ decomposes uniquely as $\beta_1^{e_1}\cdots \beta_t^{e_t}$ where $\beta_1 = \beta$, $e_1$ is called the ramification index of $\beta$ in $B/A$, it is denoted $e_{\beta}$.\footnote{%TODO: explain how ramification index only depends on \beta.
	}
\end{defn}
\begin{lem}\label{lem3.33}
	A prime ideal $\beta$ divides $\lp B$ if and only if $\lp = \beta\cap K$.
\end{lem}
\begin{proof}
	($\Rightarrow$) It is clear that $\lp \subseteq \beta \cap K$ because any element in $\lp$ is in a product of ideals including $\beta$ and thus in $\beta$. Also, since $\lp$ is maximal and $\beta \cap K$ is an ideal, we have $\lp = \beta \cap K$.
	
	($\Leftarrow$) If $\lp \subseteq \beta$, then $\lp B \subseteq \beta$, thus $\lp B_{\beta} \neq B_{\beta}$\footnote{Because no element of $\beta$ has an inverse in $B_{\beta}$.}. However, if $\beta$ did not divide $\lp B$, then we would get all of $B_{\beta}$ when localizing at $\beta$.
\end{proof}
\begin{thm}
For any prime ideal $\lp \lhd A$ with $\lp B = \beta_1^{e_1}\cdots \beta_t^{e_t}$, if $f_i$ is the residue degree of $\beta_i$ in $B/A$, then $\sum_{i=1}^t e_if_i = n$.
\end{thm}
\begin{proof}
	On one hand, from the following derivation\footnote{Where the isomorphisms are as vector spaces over $(A/\lp)$.}, we see that $B/\lp B$ is isomorphic to $(A/\lp)^n$:
	\begin{align*}
		B/\lp B &\cong B_{\lp}/\lp B_{\lp} &&\mbox{(by lemma \ref{moddinginlocal})}\\ 
		&\cong A_{\lp}^n/(\lp A_{\lp}^n) &&\mbox{(by local freeness\footnotemark)}\\
		&\cong (A_{\lp}/\lp A_{\lp})^n\\
		&\cong (A/\lp)^n &&\mbox{(by lemma \ref{moddinginlocal})}.
	\end{align*}
	\footnotetext{We know from the proof of proposition \ref{fingenint} that $B$ is both contained and contains free $A$-modules of rank $n$. After localizing at $\lp$, we get that $B_{\lp}$ is between two free $(A_{\lp})$-modules of rank $n$, but by local freeness, it is also finitely generated, so it must be isomorphic to $(A_{\lp})^n$.}
	On the other hand, from the CRT, we know that 
	\[B/\lp B \cong B/\beta_1^{e_1} \times \cdots \times B/\beta_t^{e_t},\]
	as vector spaces over $(A/\lp)$. Therefore, it is enough to show that for all $1\leq i \leq t$, the dimension of $B/\beta_i^{e_i}$ is $e_if_i$. We will show this by induction on $e_i$.
	
	First, observe that for any $1 \leq k \leq e_i -1$, $\beta_i^k/\beta_i^{k+1}$ is a $(B/\beta_i)$-module of dimension one. One can easily check that $(a+ \beta_i)(b + \beta_i^{k+1}) := (ab + \beta_i^{k+1})$ for any $a \in B$ and $b \in \beta_i^k$ is a suitable scalar multiplication. Furthermore, by the fourth isomorphism theorem, any non-trivial strict subspace of $\beta_i^k/\beta_i^{k+1}$ must come from an ideal $\beta_i^{k+1} \subset V \subset \beta_i^{k}$. However, no such ideal can exist because $\beta_i$ is prime, so $\beta_i^k/\beta_i^{k+1}$ must have dimension one.
	
	Our base case (when $e_i = 1$) follows from the definition of $f_i$. Suppose that $B/\beta_i^{e_i-1}$ has dimension $(e_i-1)f_i$ over $A/\lp$. Then, we have the following short exact sequence of modules
	\[0 \rightarrow \beta_i^{e_i-1}/\beta_i^{e_i} \rightarrow B/\beta_i^{e_i} \rightarrow B/\beta_i^{e_i-1} \rightarrow 0.\]
	By a basic result from the study of modules, we get that the dimension of $B/\beta_i^{e_i}$ is $(e_i-1)f_i + f_i = e_i$.
\end{proof}
\begin{thm}
	If $L/K$ is a Galois extension with Galois group $G$, then $G$ acts transitively on the set of prime ideals dividing $\lp B$. In particular, for all $\beta \mid \lp B$, the ramification number $e_{\beta} = e$ depends only on $\lp$ and likewise for $f_{\beta} = f$. Hence, $n = t\cdot f\cdot e$.\footnote{$t$ is the number of distinct prime ideals dividing $\lp$.}
\end{thm}
\begin{proof}
	Suppose $G$ does not act transitively, then there exists $\beta_0 \mid \lp$ and $\beta_1 \mid \lp$ such that $\beta_0 \neq \sigma\beta_1$ for all $\sigma \in G$. Hence, there exists an element $a \in \beta_0$ such that $a \notin \sigma \beta_1$ for all $\sigma \in G$, equivalently, $\sigma^{-1}a \notin \beta_1$. Since $\beta_1$ is prime, we have
	\[\Nm(a) = \prod_{\sigma \in G} \sigma^{-1}a \notin \beta_1.\] However, we also know that $\Nm(a) \in \beta_0 \cap A = \lp\subseteq \beta_1$\footnote{We know that $\Nm(a) \in \beta_0$ because it is a multiple of $a \in \beta_0$. We know that $\Nm(a) \in A$ because $B$ is integral over $A$. We know that $\beta_0 \cap A = \lp$ by lemma \ref{lem3.33}.}, so we have a contradiction.%TODO: prove norm and trace cor 2.20 and 2.21 to obtain this.
\end{proof}
\begin{defn}
	Let $\lp \lhd A$ be prime. If, for some prime ideal $\beta \lhd B$, $\beta^2 \mid \lp B$, then we say that $\lp$ ramifies (or is ramified) in $B/A$.
\end{defn}
Our last goal in this section is to show that there are only finitely many primes that ramify in $B/A$.
\begin{defn}[Discriminant]%TODO: Really understand discriminants.
	If $B \cong A^n$ as an $A$-module, then we define \[\disc(B/A) = \det\bra{\bra{\Tr_{L/K}(e_ie_j)}_{i,j}}\in A,\]
	where $\{e_1, \dots, e_n\}$ is an $A$-basis for $B$. Otherwise, we still have that $B$ is locally free over $A$, so $\disc(B_{\lp B}/A_{\lp}) = u\cdot \pi^{e_{\lp}}\in A_{\lp}$.\footnote{Recall that $A_{\lp}$ is a DVR, so we have this decomposition where $\pi$ is the generator of the unique non-zero prime ideal and $u$ is a unit.} Thus, we can define, in general, 
	\[\disc(B/A) = \prod_{\lp \text{ prime}} \lp^{e_{\lp}} \lhd A.\]
\end{defn}
\begin{rem}
	The general definition is compatible with the definition for PIDs because if $B = Ae_1 \oplus \cdots \oplus Ae_n$, then $B_{\lp} = A_{\lp}e_1 \oplus \cdots \oplus A_{\lp}e_n$. %TODO: explain further.
\end{rem}
\begin{thm}
	A prime ideal $\lp \lhd A$ is ramified in $B/A$ if and only if $\lp \mid \disc(B/A)$.
\end{thm}
\begin{proof}
	We claim that $$\disc(B/A) \equiv \disc(B_{\lp}/A_{\lp}) \Mod{\lp} \equiv \disc((B_{\lp}/\lp)/(A_{\lp}/\lp)) \Mod{\lp}.$$ By the previous remark, we also have $(B_{\lp}/\lp) = (A_{\lp}/\lp)\bar{e}_1 \oplus \cdots \oplus (A_{\lp}/\lp)\bar{e}_n$. \footnote{Argue that they are linearly independent.} \marginnote{If $B$ is an $A$-algebra which is free of rank $N$ as an $A$-module, the discriminant is really an eliment of $A/A^{times ^2}$}
	
	On the other hand, $\lp = \beta_1^{e_1}\times \cdots \times \beta_t^{e_t}$, thus $B_{\lp}/\lp = B_{\lp}/\beta_1^{e_1} \times \cdots \times B_{\lp}/\beta_t^{e_t}$ as $(A_{\lp}/\lp)$-algberas. Moreover, if $B = B_1 \times \cdots \times B_t$ as $k$-algebras, then $\disc(B/k) = \disc(B_1/k) \cdots \disc(B_t/k)$ (matrix that is considered in the discriminant and we can write it as block matrix).
	
	It remains to understand the discriminant of $B_{\lp}/\beta^e$ where $\beta$ is a prime ideal of $B$. Note that this algebra is isomorphic to a field if $e=1$, but it has non-zero nilpotent elements if $e > 1$. The discriminant of a field extension is non-zero (because the trace form is non-degenerate). However, when there are nilpotent elements, the trace form is degenerate, so the discriminant is zero.
\end{proof}
\begin{cor}
	There are finitely many ramified primes in $B/A$.
\end{cor}
\begin{exmps}
	\begin{enumerate}
		\item[]
		\item Let $B = \Z[i]$ and $A = \Z$, their fraction fields are $\Q(i)$ and $\Q$ respectively and they fit in the general set-up of this section with $n = [\Q(i): \Q] = 2$. Let the $A$-basis for $B$ be $\{1,i\}$, we can readily compute
		\[\disc(\Z[i]/\Z) = \det \begin{bmatrix}
		2&0\\0&-2
		\end{bmatrix} = -4.\]
		Let $p \in \Z$ be prime, we will investigate the factorization of $(p)$ in $\Z[i]$. If $p = 2$, then we notice $(2) = (1+i)(1-i) = (1+i)^2$, thus $t =1$ and $e = 2$. Also, because $\Z[i]/(2)$ identifies $1 = -1 = i^2$, we find that it is isomorphic to $\Z/2\Z$, so $f = 1$. Otherwise, we have
		$\Z[i]/(p) = \Z/p\Z[x]/(x^2+1)$ and we consider two cases.
		
		If $p \equiv 1 \Mod{4}$, then $-1$ has a square root $a \in \Z/p\Z$ and $x^2+1 = (x-a)(x+a)$ is reducible, thus \[\Z[i]/(p) \cong \Z/p\Z[x]/(x-a) \times \Z/p\Z[x]/(x+a) \cong \Z/p\Z \times \Z/p\Z,\]
		and $f = 2$. Furthermore, we now see that $p = (p,i-a)(p,i+a)$, so $t =2$ and $e = 1$ which confirms that $(p)$ does not ramify because it does not divides $-4$.
		
		If $p \equiv 3 \Mod{4}$, then $x^2+1$ is irreducible, so $\Z/p\Z[x]/(x^2+1) \cong \F_{p^2}$ and $(p)$ is prime in $\Z[i]$, we obtain $t=2$, $e=1$, $f=2$.
		\item Let $R = \Z[\sqrt[3]{2}] \subset B = O_{\Q(\sqrt[3]{2})}$. We can compute 
		\[\disc(R/\Z) = \det\begin{bmatrix}3&0&0\\0&0&6\\0&6&0\end{bmatrix} = -3^3\cdot 2^2.\]
		We get that $R_{(p)} = B_{(p)}$ for any $p \neq 2,3$. For such a $p$, we have
		\[B/(p) = B_{(p)}/(p) = R_{(p)}/(p) = R/(p) = \Z[x]/(x^3-2)/(p) = \Z/p\Z[x]/(x^3-2)\] We are now in a nicer ring (a UFD), so the strucutre is really nice. $\alpha = \sqrt[3]{2}$
		On the other hand, we have $(2) = (\sqrt[3]{2})^3$ and $(3) = (\sqrt[3]{2}+1)^3$ in $R_{(3)}$ \footnote{detail this}. We have $(5) = (5, \alpha + 2)(5, \alpha^2+3\alpha+4)$ because $(x^3-2) \equiv (x+2)(x^2+3x+4)  = \lp_1\lp_2 \Mod{5}$. We see that $B/\lp_1 = \Z/5\Z$, but $B/\lp_2 = \F_p{5}$. So $f_1 = 1$ and $f_2 = 2$.
		
		In $(7)$, $(x^3-2)$ is irreducible, so $f = 3$. For $(11) = (11,\alpha+4)(11, \alpha^2 + 7\alpha+5)$ and $(13) = (13)$. Interestingly, $(31) = (31,\alpha+11)(31,\alpha+24)(31,\alpha+27)$. Doing this for many primes, we see that 50\% of the time, $f_1 = 1$ and $f_2 = 2$ occurs. $f = 3$ occurs 33\% of the time. $f_1 = f_2 = f_3 = 1$ occurs 16.6\% of the time. Notice that this happens because $\Q(\alpha)$ is not Galois.
		
		Remark: these proportions are predicted by the Chebotarev density theorem. In particular, if $K/\Q$ is a cyclic Galois extension of degree 3 with polynomial $f(x)$, then $f(x) \Mod{p}$ factors either into three linear factors or is irreducible. Interesting questions, are there any patterns satisfied by the function $p \mapsto (f_1, f_2 , \dots, f_t)$.
	\end{enumerate}
\end{exmps}
\begin{rem}
	Explaination of the terminology of ramification. The first appearance of this theory was motivated by the study of Riemann surfaces. Let $p(t,x) \in \C[t,x]$ and $S = \{(t,x) \in \C^2 \mid p(t,x) = 0\}$ is a curve in $\C$ because it is one dimension but topologically it is more of a surface because we are in $\C$. We have the following analogies.
	\begin{table}[]
		\begin{tabular}{ll}
			$\Z$& $\C[t]$ \\
			$\Q$ & $\C(t)$\\
			$K > \Q$, $[K: \Q] < \infty$ & $\C(t)[x]/(x^n+a_{n-1}(t)x^{n-1}+ \cdots + a_1(t)x + a_0(t))$ where $a_i(t) \in \C[t]$, so we indeed have $p(t,x) \in \C[t,x]$.
		\end{tabular}
	\end{table}
	The function $S \rightarrow \C = (t,x) \mapsto t$ is generically $n$-to one except for $t \in R:= \{t \in \C \mid x^n+a_{n-1}(t)x^{n-1} + \cdots a_1(t)x_1 + a_0(t) \text{ has multiple roots} = \{t \mid \delta(t) = 0, \delta = \disc_x(P_t(x))\}$ is called the ramification locus.
\end{rem}
\section{Commutative Algebra and Algebraic Geometry}
\marginnote{The main reference for this section is Ernst Krunz's \textit{Introduction to Commutative Algebra and Algebraic Geometry}, chapter 1-4.}
The main objects studied in algebraic geometry are algebraic varieties, we first introduce two kinds of such objects in the two following sections and then move to a more abstract setting.
\subsection{Affine Varieties}

\begin{defn}
	Let $k$ be a fields, we denote $\bA^n(k)$ to be the \textbf{affine $n$-space} isomorphic to $k^n$.\footnote{In this class, we will always view the affine $n$-space as $k^n$.} If $k$ is algebraically closed, then $\bA^n(k)$ can be identified with the maximal ideals of $k[x_1, \dots, x_n]$.
\end{defn}

\begin{defn}
	A \textbf{variety} $V$ over $k$ is a system of polynomial equations of the form 
	\[\begin{Bmatrix}
		f_1(x_1, \dots, x_n) = 0\\
		\vdots \\
		f_m(x_1, \dots, x_n) = 0
	\end{Bmatrix},\]
	where $f_1, \dots, f_m \in k[x_1, \dots, x_n]$. For a $k$-algebra $L$, we denote $V(L)$ to be the set of solutions of this system in $L^n$.
\end{defn}

\begin{exmps}
	Here are a few brief examples that we might study more in depth in this section. In the context of the above definition:
	\begin{itemize}
		\item We talk about a \textbf{linear} variety if $\deg f_1 = \cdots = \deg f_m = 1$.
		\item We talk about \textbf{hypersurfaces} when $m= 1$. 
		\item We talk about \textbf{quadric hypersurfaces} when $\deg f_1 = 2$ and $m=1$, they are a generalization of conic surfaces.
		\item More generally, if $\deg f_1 = \cdots = \deg f_m = 2$, then $V$ is a \textbf{quadric variety}.
		\item We say that $V$ is a \textbf{cone} when $f_1,  \dots, f_m$ are homogeneous\footnote{We say that a polynomial is homogeneous if all of its monomials have the same degree.}. Such varieties have a nice scaling property, namely, \[(a_1, \dots, a_n) \in V(L) \implies (\lambda a_1, \dots, \lambda a_n) \in V(L), \qquad \forall \lambda \in L.\]
		\item With more generality, one can consider \textbf{quasi-homogeneous} varieties, they satisfy that for some fixed $\{m_i\}_{i \in [n]} \subset \N$ and $m \in \N$ and for all monomials $cx_1^{r_1}\cdots x_n^{r_n}$ of the polynomial $f_j$, $\sum_{i=1}^nr_im_i = m$. We then have \[(a_1, \dots, a_n) \in V(L)\implies (\lambda^{m_1} a_1, \dots, \lambda^{m_n} a_n) \in V(L), \qquad \forall \lambda \in L.\]
		\item If $V_1$ is a system $\{f_1, \dots, f_m\}$ and $V_2$ is a system $\{g_1, \dots, g_l\}$, then we can build another variety\footnote{The notation $\cap$ is justified because for any $L>k$, we have $(V_1\cap V_2)(L) = V_1(L) \cap V_2(L)$.} \[V_1 \cap V_2 = \{f_1, \dots, f_m, g_1, \dots, g_l\}.\] The variety $V_1 \times V_2$ arises from considering the same set but viewing the polynomials in $k[x_1, \dots, x_n, y_1, \dots, y_n]$ where $f_j \in k[x_1,\dots, x_n]$ and $g_j \in k[y_1, \dots, y_n]$.\footnote{The notation $\times$ has the same justification as $\cap$.}
		\item We can look at some common groups with the point of view of varieties. For instance, we can see $GL_n$ as a subset of $\bA^{n^2+1}$ with the variables $x_{i,j}$ for $i,j \in [n]$ and $t$. Observe that $\det((x_{i,j}))$ is a polynomial of degree $n$ in $n^2$ variables and that the solutions in $L^{n^2+1}$ of the equation $t\det(x_{i,j}) - 1 = 0$ are precisely matrices with invertible determinants in $L$, namely $GL_n(L)$, where $L$ is any $k$-algebra. It has a natural group structure arising from the multiplication of matrices. This is what we call an \textbf{algebraic group}.
	\end{itemize}
\end{exmps}

\begin{defn}
	To a variety $V \subseteq \bA^n$ over $k$, we can associate two related ring theoretic invariants. First, the ideal $I(V) \lhd k[x_1, \dots, x_n]$ of polynomials that are identically $0$ on $V$. Second, the coordinate ring of $V$, denoted $\mO_V$, is the quotient $k[x_1, \dots, x_n]/I(V)$. We can think of the latter as polynomially defined functions on $V$.
\end{defn}


The general problem that is studied in this section is to understand the relation between the collection \(\{V(L) \mid L \text{ is a $k$-algebra}\}\) and $I(V)$ or $\mO_V$. We start with a very useful fact about the map $L \mapsto V(L)$. It will use Hilbert's basis theorem, so we prove it for completeness.\footnote{In fact, we prove a more general statement.}
\begin{thm}
	If $R$ is Noetherian, then $R[x]$ is Noetherian.
\end{thm}
\begin{proof}
	We show the contrapositive. Suppose $I \lhd R[x]$ is not finitely generated. Let $f_1(x)$ be a non-zero element of $I$ of minimal degree $d_1$ and $a_1$ be its leading coefficient. Let $f_2$ be the element of smallest degree in $I-(f_1)$, it has degree $d_2 \geq d_1$ and leading coefficient $a_2$. Inductively, let $f_j$ be the element of smallest degree in $I - (f_1, \dots, f_{j-1})$ with degree $d_j$ and leading coefficient $a_j$. Since $I$ is not finitely generated, we will always find non-zero elements, so we get an infinitely many $a_i$'s. We claim that \[(a_1) \subset (a_1,a_2) \subset \cdots \subset (a_1, \dots, a_j) \subset\cdots.\]
	Suppose one inclusion is not strict at some point $j$, then we can write $a_{j+1} = \lambda_1 a_1+ \cdots +\lambda_j a_j$. Therefore, the degree of \[f_{j+1} - \lambda_1x^{d_{j+1}-d_1} f_1 + \cdots + \lambda_jx^{d_{j+1}-d_j}f_j \in I\] is strictly less than $d_{j+1}$, so this polynomial belongs to $(f_1, \dots, f_j)$. However, this implies that $f_{j+1} \in (f_1, \dots,f_j)$ which is a contradiction. We conclude that $R$ is not Noetherian as it has an infinite ascending chain of ideals.
\end{proof}
\begin{cor}[Hilbert's basis theorem]
	For any $n \in \N$, $k[x_1, \dots, x_n]$ is Noetherian.\marginnote{\vspace{-70pt}\begin{rem}\label{remidealvar}
			Hilbert's basis theorem also justifies the fact that varieties can be defined by an ideal. Indeed, if $I \lhd k[x_1, \dots, x_n]$, it is finitely generated by $f_1, \dots, f_m$, then we define $V(I)$ to be the variety defined by $f_1 = \cdots = f_m = 0$, we also say it is the variety corresponding to the ideal $I$. One can verify that $V(I(V)) = V$ is true for any $V$, but $I(V(I)) = I$ is not true for any $I$. %TODO: Verify it is algned with Hilbert
	\end{rem}}
\end{cor}

\begin{prop}
	A variety $V$ determines a functor (the functor of points) from $V: \textbf{Alg}_k \rightsquigarrow \textbf{Sets}$ associating $L \mapsto V(L)$.
\end{prop}
\begin{proof}
	 The action of $V$ on a morphism\footnote{Recall that objects in the $\textbf{Alg}_k$ are $k$-algebras and morphisms are ring homomorphisms that restrict to the identity on $k$.} $f:L \rightarrow M$ is the natural map \[L^n \ni (a_1, \dots, a_n) \mapsto (f(a_1), \dots, f(a_n))\in M^n.\] We first need to verify that if the L.H.S. vanishes on the polynomials defining $V$, then the R.H.S. also does. This is easy to see because properties of $f$ are such that applying a polynomial and then $f$ is the same as applying $f$ and then the polynomial and $f(0) = 0$. The remaining functoriality properties obviously hold.
\end{proof}

\begin{thm}
	A functor $F: \textbf{Alg}_k \rightsquigarrow \textbf{Sets}$ is representable\footnote{We say a functor $F:\textbf{Alg}_k \rightsquigarrow \textbf{Sets}$ is representable if there is a finitely generated algebra $R$ such that $F \cong \Hom_{\textbf{Alg}_k}(R, -)$.} if and only if it arises from some variety $V$ over $k$.
\end{thm}
\begin{proof}
	($\Leftarrow$) Let $V$ be defined by $f_1 = \cdots = f_m = 0$ and let $R = k[x_1, \dots, x_n]/(f_1, \dots, f_m)$. We claim that $V(L) \cong \Hom_{\textbf{Alg}_k}(R, L)$. This is clear because to define a function $\phi$ in the R.H.S., we need to specify where it sends $x_1, \dots, x_n$ in $L$ such that\[f_i(\phi(x_1), \dots, \phi(x_n)) = \phi(f_i(x_1, \dots, x_n)) = \phi(0) = 0, \qquad \forall i \in [m].\] This happens precisely when $(\phi(x_1), \dots, \phi(x_n)) \in V(L)$.
	
	($\Rightarrow$)	Let $R$ be the $k$-algebra that represents $F$, then because $R$ is finitely generated, we can write $R = k[x_1, \dots, x_n]/I$, where $I$ is generated by the relations between the generators of $R$. Since $I$ is finitely generated,\footnote{Because $k[x_1, \dots, x_n]$ is Noetherian.} we can write $I =(f_1, \dots, f_m)$. Then, as in the converse direction, we have $\Hom_{\textbf{Alg}_k}(R, L) \cong V(L)$, where $V$ is the variety defined by $f_1 = \cdots = f_m = 0$.
\end{proof}

\begin{quest}\label{questvar}
	To what extent is a variety $V$ (equivalently, $I(V)$ or $\mO_V$) determined by $V(k)$?
\end{quest}
It is obviously not \textit{completely} determined by $V(k)$, for example: let $V_1 = \{x^2+y^2+1 = 0\}$ and $V_2 = \{x^2+y^2+2 = 0\}$ with $k = \R$. It is clear that $I(V_1) \neq I(V_2)$ but both $V_1(\R)$ and $V_2(\R)$ are empty. One might think that this situation gets better when $k$ is algebraically closed. However, this is still not enough. For instance, if we have $V_1 = \{x = 0\}$ and $V_2 = \{x^2 = 0\}$, then if $L$ is any field, $V_1(L) = V_2(L) = \{0\}$, but these varieties have different defining ideals. We will see that looking at more general algebras will help us greatly towards distinguishing different varieties. Coming back to the previous example, if we let $L = k[\epsilon], \epsilon^2 = 0$\footnote{We can also write\[L = \left\{\begin{bmatrix}a&b\\0&a\end{bmatrix} \mid a,b \in k\right\}.\]}, we see that $V_1(L)= \{(0)\}$ and $V_2(L) = \{\lambda \epsilon \mid \lambda \in k\}$ are different.


Our next main result is Hilbert's Nullstellensatz. We will first prove a proposition that is equivalent to it (sometimes called the field theoretic Nullstellensatz). This proposition needs two simple but very general lemmas.

\begin{lem}\label{rationotfg}
	The field of rational functions $k(x_1, \dots, x_n)$ is not finitely generated as a $k$-algebra.
\end{lem}
\begin{proof}
	Let $\alpha_1 = \frac{p_1}{q_1}, \dots, \alpha_t = \frac{p_t}{q_t}$ be a finite collection of elements of this field. We claim that there is a strict inclusion \[k[\alpha_1, \dots, \alpha_t] \subset k(x_1, \dots, x_n).\] Any element of the L.H.S. can be written as $\frac{p(x)}{q_1^{r_1}\cdots q_t^{r_t}}$, hence if $q\in k[x_1, \dots, x_n]$ is relatively prime to each $q_i$,\footnote{We can always find such $q$ by the CRT.} then $\frac{1}{q} \notin k[\alpha_1, \dots, \alpha_n]$ and the lemma follows.
\end{proof}

\begin{lem}\label{fingensubalg}
	Let $R$ be a Noetherian ring, $T$ be a finitely generated $R$-algebra and $S$ be a subring of $T$ containing $R$. If $T$ is finitely generated as an $S$-module, then $S$ is a finitely generated as an $R$-algebra.
\end{lem}
\begin{proof}
	Let $w_1, \dots, w_r$ be the $S$-module generators of $T$ and assume\footnote{We do not loose generality because any set of $R$-algebra generators of $T$ can be extended to be a spanning $S$-module generators.} that this spanning set also includes a set of $R$-algebra generators of $T$. Then, we can write
	\[w_iw_j = \sum_{k=1}^r a_{i,j}^{(k)} w_k, \qquad\forall i,j,k \in [r], a_{i,j}^{(k)} \in S.\]
	Let $S'$ be the $R$-algebra generated by all the coefficients $a_{i,j}^{(k)}$. We have the inclusions $R \subseteq S' \subseteq S \subseteq T$. Since $R$ is Noetherian, then $S'$ is also Noetherian since it is finitely generated as an $R$-algebra. This implies that $S$, being a submodule of a finitely generated $S'$-module (namely $T$), is finitely generated as an $S'$-module. We conclude that $S$ is finitely generated as an $R$-algebra.\footnote{It is generated by the $R$-algebra generators of $S'$ and the $S'$-module generators of $S$.}
\end{proof}

\begin{prop}\label{lemnull}
	If $L/k$ is an extension of fields and $L$ is finitely generated as a $k$-algebra, then $L$ is algebraic over $k$.
\end{prop}
\begin{proof}
	From general field theory, we know that there exists $x_1, \dots, x_n \in L$ such that $k(x_1, \dots, x_n)$ (we adjoin $x_1, \dots, x_n$ to $k$) is purely transcendental and $L/k(x_1, \dots, x_n)$ is algebraic.\footnote{We say that $x_1, \dots, x_n$ is a transcendental basis of $L$ and $n$ is the transcendence degree of $L$ over $k$.} Our goal is to show that $n = 0$, i.e.: there are no transcendental elements in $L$.\footnote{Why do we know $[L:k(x_1, \dots, x_n)] < \infty$? We can prove this because $L$ is finitely generated as a $k$-algebra. %TOOD: prove this.
	} 

	Since $L$ is finitely generated as a module over $k(x_1,\dots, x_n)$ and as a $k$-algebra, we can use lemma \ref{fingensubalg} to conclude that $k(x_1, \dots, x_n)$ is finitely generated as a $k$-algebra, contradicting lemma \ref{rationotfg} if $n > 0$.
\end{proof}

\begin{rem}
	One might think that the proposition follows from the first paragraph of the previous proof and lemma \ref{rationotfg}, because $k(x_1, \dots, x_n)$ is not finitely generated over $k$ while $L$ is, leading to a contradiction. However, in general, sub-algebras of finitely generated algebras are not necessarily finitely generated hence the need for lemma \ref{fingensubalg}.
	
	For example, let $k$ be a field and $R$ be the $k$-algebra generated by the set $\{x^iy^j \mid i< j\}$  where $x$ and $y$ are commuting formal variables. It is a subalgebra of $k[x,y]$ which is finitely generated over $k$. Suppose that $g_1, \dots, g_t \in R$ and let $\{v_1, \dots, v_m\}$ be the set of degrees of monomials appearing in the generators.\footnote{We view the degree of the monomial $x^iy^j$ as the tuple $(i,j) \in \N^2$.} Now, any degree $(i,j)$ appearing in a monomial in $g_1^{d_1}\cdots g_t^{d_t}$ is a positive linear combinations of $v_1, \dots, v_m$. Since $v_1, \dots, v_m$ are above the diagonal $x = y$ when seen as points of the lattice $\N^2$, this linear combination is also above the diagonal.
	
	In fact, letting $v_1$ be the closest (out of $v_1, \dots, v_m$) to the diagonal, we see that the degree $(i,j)$ is above the line going through the origin and $v_1$. However, we can always find a point below this line and above the diagonal, it represents the degree of a monomial in $R$ that cannot be generated by this finite set. %TODO: Draw a diagram.
\end{rem}


\begin{cor}[Hilbert's Nullstellensatz]
	If $I(V) \neq k[x_1, \dots, x_n]$ and $k$ is algebraically closed, then $V(k)$ is not empty.
\end{cor}
\begin{proof}
	Let $M$ be a maximal ideal containing $I(V)$ and $L = k[x_1, \dots, x_n]/M$. Note that $L$ is a field extension of $k$. Moreover, it is finitely generated as $k$-algebra by $\{x_1 + M, \dots, x_n + M\}$. By proposition \ref{lemnull}, $L$ is contained in the algebraic closure of $k$, but since $k = \bar{k}$, they must be equal. Let $\phi : k[x_1, \dots, x_n]/M \cong k$ and $(a_1, \dots, a_n) = (\phi(x_1),\dots, \phi(x_n)) \in k^n$. Since $\phi$ is a homomorphism, we have $f(a_1, \dots, a_n) = 0$ for all $f \in M$. In particular, this is true for all $f \in I$, thus $(a_1, \dots, a_n)$ belongs to $V(k)$. As desired, we conclude that $V(I)(k) \neq \emptyset$.
\end{proof}
\begin{rem}
	We also note that Hilbert's Nullstellensatz implies proposition \ref{lemnull}.
	\begin{proof}
		If $L/k$ is a field extension that is finitely generated as a $k$-algebra, then $L = k[x_1, \dots, x_n]/M$,\footnote{$M$ is generated by the relations between the generators of $L$.} where $M$ is maximal because $L$ is a field. Let $V$ be the variety corresponding to $M$, the Nullstellensatz implies that $V(\bar{k})$ is non-empty, hence there exists $\phi \in \Hom_{\textbf{Alg}_k}(L,\bar{k})$, hence $L$ is algebraic.\footnote{Any ring homomorphism $\phi:L \rightarrow \bar{k}$ is injective, so $L$ is a subfield of $\bar{k}$ and we conclude any element of $L$ is algebraic over $k$.}
	\end{proof}
\end{rem}

We will now refine Hilbert's Nullstellensatz.

\begin{defn}
	Let $I \lhd R$ be an ideal in a ring. The radical of $I$ is the set \[\sqrt{I}:= \{f \in R \mid \exists m \in \N, f^m \in I\}.\]
	If $I = \sqrt{I}$ we say that $I$ is a radical ideal.
\end{defn}
\begin{fact}
	$R/\sqrt{I}$ has no nilpotent elements\footnote{We often called this a reduced algebra.} and $\sqrt{I}$ is the smallest ideal containing $I$ with this property.
\end{fact}

\begin{quest}
	When is an ideal $I \lhd k[x_1, \dots, x_n]$ of the form $I(V(\bar{k}))$ for a variety $V$?
\end{quest}
We will see that the condition that $I = \sqrt{I}$ is necessary and sufficient.
\begin{thm}
	Let $I \lhd k[x_1, \dots, x_n]$, then $I(V(I)(\bar{k})) = \sqrt{I}$.
\end{thm}
\begin{proof}
	($\supseteq$) It is clear because if $f^d \in I$,then $f^d \equiv 0$ on $V(I)(\bar{k})$, so $f \equiv 0$ as well.
	
	($\subseteq$) Let $f$ be in the L.H.S which is generated by $f_1, \dots, f_m \in k[x_1, \dots, x_n]$ and consider the ideal $J = (f_1, \dots, f_m, tf -1) \lhd k[x_1, \dots, x_n, t]$. Observe that 
	\[V(J)(\bar{k}) = \left\{(x_1, \dots, x_n, t) \in \bar{k}^{n+1} \mid f_i(x_1, \dots, x_n) = 0, \forall i, tf(x_1, \dots, x_n) -1 =0 \right\}\]
	is empty because any points that vanish on $f_1, \dots, f_m$ also vanish on $f$, thus we have $f(x_1, \dots, x_n)= 0$ and the last equation cannot be satisfied. Consequently, Hilbert's Nullstellensatz implies that $J = k[x_1, \dots, x_n, t]$, in particular
	\[1 = \sum_{i=1}^m r_i f_i + s(tf-1), \text{ for some }r_i,s \in k[x_1, \dots, x_n,t].\qquad (*)\]
	Consider now a homomorphism $\phi :k[x_1, \dots, x_n,t] \rightarrow k[x_1, \dots, x_n][f^{-1}]$ with $x_j \mapsto x_j$, $t \mapsto f^{-1}$ and $\phi\mid_k = \text{id}$. Applying $\phi$ to $(*)$, we get
	\[1 = \phi(1) = \sum_{i=1}^m \phi(r_i) f_i, \qquad \phi(r_i) = \frac{g_i(x_1, \dots, x_n)}{f^{d_i}}.\]
	Let $d$ be the maximum of the $d_i$'s and multiply by $f^d$ to obtain \[f^d = \sum_i g(x_1, \dots, x_n) f^{d-d_i}f_i,\] so $f^d \in I$ and $f \in \sqrt{I}$.
\end{proof}
\begin{cor}
	The maps $V \mapsto I(V(\bar{k}))$ and $I \mapsto V(I)$ are mutual inverses\footnote{In remark \ref{remidealvar}, we said that $I(V(I)) = I$ does not always hold. This corollary states that it holds if and only if $I$ is radical.} \[\{\text{varieties over } k\} \leftrightarrow \{\text{radical ideals of } k[x_1, \dots, x_n]\}.\]
\end{cor}

To end this section, we will talk about decomposition of varieties in irreducible components and we will need to take a small detour in topology.\footnote{We assume some basic knowledge about topological spaces.}
\begin{defn}
	The Zariski topology on $\bA^n(\bar{k})$ with respect to $k$ is the topology where the closed sets are the sets of the form $V(\bar{k})$ for $V$ a variety over $k$.
\end{defn}
\begin{prop}
	The Zariski topology makes $\bA^n(\bar{k})$ into a topological space, i.e.:
	\begin{enumerate}
		\item $\emptyset$ and $\bA^n(\bar{k})$ are closed.
		\item If $\{V_{\alpha}\}_{\alpha \in A}$ are closed, then $\bigcap_{\alpha \in A} V_{\alpha}$ is closed.
		\item If $\{V_{\alpha}\}_{\alpha \in A}$ are closed and $A$ is finite, then $\bigcup_{\alpha \in A} V_{\alpha}$ is closed.
	\end{enumerate}
\end{prop}
\begin{proof}
	\begin{enumerate}
		\item The empty set is always the variety defined by $1 = 0$. The whole space is the variety defined by $0=0$.
		\item Let $F_{\alpha} = I(V_{\alpha})$ and $F = \sum_{\alpha \in A} F_{\alpha}$, $F$ is an ideal that is finitely generated by some polynomials $f_1, \dots, f_m$ and it is clear that the variety they define is the intersection of all the $V_{\alpha}$'s.
		\item Let $F_{\alpha} = I(V_{\alpha})$ and $F = \prod_{\alpha \in A} F_{\alpha}$\footnote{While arbitrary sum of ideals is well defined. Products are only well defined when $A$ is finite.  %TODO:Try to say also $I \cap J$, but not infinite intersection)
		}, $F$ is an ideal that is finitely generated by some polynomials $f_1, \dots, f_m$ and it is clear that the variety they define is the union of all the $V_{\alpha}$'s.
	\end{enumerate}
\end{proof}
\begin{defn}
	A topological space $X$ is \textbf{irreducible} if for any decomposition $X = A_1 \cup A_2$ where $A_1$ and $A_2$ are closed, $A_1 = X$ or $A_2 = X$.\footnote{Same for open or closed. %TODO: explain
	}
\end{defn}
\begin{rem}
	Observe that this definition applied to the Zariski topology coincides with the definition of irreducibility of varieties.
\end{rem}
\begin{lem}\label{lemequivirred}
	The following properties of a topological space $X$ are equivalent:
	\begin{enumerate}[(i)]
		\item $X$ is irreducible.
		\item If $U_1$ and $U_2$ are open non-empty subsets of $X$, then $U_1 \cap U_2 \neq \emptyset$.
		\item Any non-empty open subset of $X$ is dense in $X$.
	\end{enumerate}
\end{lem}
\begin{proof}
	(i $\Leftrightarrow$ ii) The contrapositive of this equivalence follows from DeMorgan's laws, namely, let $U_1$ and $U_2$ be subsets of $X$ and $V_1 = X-U_1$ and $V_2 = X-U_2$, then \[\emptyset \neq U_1, U_2 \text{ are open and } U_1 \cap U_2 = \emptyset \Leftrightarrow X \neq V_1, V_2 \text{ are closed and } V_1 \cup V_2 = X.\]
	(ii $\Leftrightarrow$ iii) Again, we show the contrapositive. If $U$ is a non-empty open subset of $X$, then 
	\[\exists \emptyset \neq V \text{ open}, U \cap V = \emptyset \Leftrightarrow \exists x \in X-U, \exists V \text{ open}, U \cap V = \emptyset \Leftrightarrow U \text{ is not dense.}\]
\end{proof}
\begin{cor}
	If $S\subseteq X$ is irreducible, then so is $\overline{S}$ (its closure in the topology).
\end{cor}
\begin{defn}
	An \textbf{irreducible component} of $X$ is a maximal irreducible subset.
\end{defn}
\begin{cor}
	The irreducible components of $X$ are closed.
\end{cor}
\begin{prop}
	Let $X$ be a topological space, then we have
	\begin{enumerate}[(i)]
		\item Any irreducible subset of $X$ is contained in an irreducible component.
		\item $X$ is the union of its irreducible components.
	\end{enumerate}
\end{prop}
\begin{proof}
	\begin{enumerate}[(i)]
		\item Follows from Zorn's lemma. Let $S \subseteq X$ be irreducible and $M$ be the set of all irreducible subsets of $X$ containing $S$. If $\{S_i \in M\}_{i \in I}$ is a chain of inclusions, then let $Y = \cup_{i \in I} S_i$. We claim that $Y$ is also in $M$, implying the chain has an upper bound.
		
		It is obvious that $Y$ contains $S$. To see that $Y$ is irreducible, observe that any two open sets $U_1$ and $U_2$ that intersect $Y$ non-trivially must intersect some element $S_0$ of the chain non-trivially.\footnote{$U_1$ intersects some $S_1$ and $U_2$ intersects som $S_2$, since one of the $S_i$ must be contained in the other (they are in a chain), we can let $S_0$ be the bigger one.} Thus, we have 
		\[\emptyset \neq U_1 \cap U_2 \cap S_0 \subseteq U_1 \cap U_2 \cap Y,\]
		and, by lemma \ref{lemequivirred} part (ii), $Y$ is also irreducible. 
		
		The maximal element of $M$ is an irreducible component of $X$ containing $S$.
		\item Follows from $a)$: the union of the irreducible components of $X$ is the same as the union of the irreducible components containing $x \in X$ (they exist because $x$ is irreducible) which clearly yields the whole space.
	\end{enumerate}
\end{proof}
\begin{defn}
	A topological space $X$ where every descending chain of closed sets stabilizes is said to be \textbf{Noetherian}.
\end{defn}\marginnote{\vspace{-60pt}%TODO: ALIGN
\begin{exmp}
	Consider $\bA^n(k)$ with the Zariski topology, a descending chain of closed sets is a descending chain of varieties which in turn corresponds to an ascending chain of ideals of $k[x_1, \dots, x_n]$. Since $k[x_1, \dots, x_n]$ is Noetherian, these chains must stabilize and we conclude that $\bA^n(k)$ is Noetherian. Moreover if $V$ is a variety, $V(\bar{k})$ with the induced topology is also Noetherian.
\end{exmp}}
\begin{prop}\label{finiteirred}
	A Noetherian topological space has finitely many components. Moreover, no component is contained in the union of the others. 
\end{prop}
\begin{proof}
	Let $M$ be the set of closed subsets of $X$ that are not equal to a finite union of irreducible subsets. Suppose $M \neq \emptyset$ and let $Y$ be a minimal element\footnote{It exists because $X$ is Noetherian.}. It is not irreducible\footnote{Otherwise it would be a finite union of irreducible subsets (namely, just $Y$) and it wouldn't be in $M$.}, hence $Y = Y_1 \cup Y_2$, where $Y_1$ and $Y_2$ are closed. By minimality of $Y$, $Y_1$ and $Y_2$ cannot be in $M$. Thus, each of $Y_1$ and $Y_2$ can be written as a finite union of irreducible components and it follows that $Y$ can also be written as a finite union of irreducible components. This contradicts the fact that $Y \in M$.
	
	In particular, we obtain $X = \cup_{i=1}^n X_i$, where $X_i$ are distinct irreducible components.\footnote{The first part only yields a finite union of irreducible subsets $X'_i$, but in the decomposition of the whole space, we can take the irreducible components that contain the $X'_i$'s and call them $X_i$ and get rid of duplicates.} Suppose $Y$ is an irreducible component of $X$, then $Y = \cup_{i=1}^n Y \cap X_i$ and by irreducibility, we can assume $Y= Y\cap X_1$, namely $Y = X_1$. This shows that all irreducible components are in the $X_i$'s, so there are finitely many.
	
	For the second part of the proposition, assume towards a contradiction that $X_i \subseteq \cup_{i \neq j} X_i$, then intersecting with $X_i$ and using irreducibility of $X_i$ yields $X_i = X_i \cap X_j$ for some $j\neq i$, which means $X_i$ and $X_j$ are not distinct.
\end{proof}

\begin{cor}\footnote{This result can be seen as a ring-theoretic version of proposition \ref{finiteirred}.}
	If $I \lhd k[x_1, \dots, x_n]$, then there are only finitely many minimal prime ideals that contain $I$ and they are called the associated primes of $I$. Moreover, if $I$ is radical, then $I = \lp_1 \cap \cdots \cap \lp_r$, where the $\lp_i$'s are its associated primes.
\end{cor}
\begin{proof}
	%TODO: 
\end{proof}

\subsection{Projective varieties}
It is useful to consider more general classes of varieties obtained by "gluing together" affine varieties. This section is concerned by such objects, which we call projective varieties. They are defined on the projective space.
\begin{defn}
	The $n$-dimensional projective space $\P_n(k)$ is the set of all lines in $k^{n+1}$ that go through the origin. It can be described as $(k^{n+1}-0)/\sim$ where the $\sim$ is the following relation\footnote{We denote the tuples with colons to distinguish these from the usual tuples of affine spaces. This relation says that two elements are equal if one of them is the scaled version of the other. It is clear that all points on the same line going through the origin are equal, hence the first sentence of this definition.}
	\[(x_0: x_1: \cdots : x_n) \sim (y_0: y_1 : \cdots: y_n) \Leftrightarrow \exists \lambda \in k^{\times}, \forall 0\leq i \leq n, \lambda x_i = y_i.\]
	
	We can also see $\P_n$ as the functor $\textbf{Fields}_k \rightarrow \textbf{Sets}$ sending a field $L>k$ to the set \[(L^{n+1}- 0)/\sim.\]
	One often writes $\P^n$ for $\P_n$ even though $\P_n \neq (\P_1)^n$ in general.
\end{defn}

\begin{lem}[Hilbert's Satz 90]
	 Let $k$ be a field. If $\lambda: \gal(\bar{k}/k) \rightarrow \bar{k}^{\times}$ is a continuous crossed homomorphism \footnote{Let $G$ and $H$ be groups with an action $*$ of $G$ on $H$. A map $\phi: G \rightarrow H$ is said to be a \textbf{crossed homomorphism} if it is a homomorphism that satisfies $\phi(g_1g_2) = \phi(g_1)(g_1 *\phi(g_2))$ for any $g_1,g_2 \in G$. Furthermore, we say that it is \textbf{continuous} if ??? %TODO: Complete continuous part.
	 	 $\lambda: \gal(L/k) \rightarrow L^{\times}$}, then there exits $a \in \bar{k}$ such that for any $\sigma \in\gal(\bar{k}/k)$, $\lambda_{\sigma} = \frac{a}{\sigma(a)}$.
\end{lem}
\begin{proof}
	To be seen later in %TODO: put ref.
\end{proof}
\begin{prop}
	Let $k$ be a field, then
	\[\{x \in \P_n(\bar{k}) \mid \forall \sigma \in \gal(\bar{k}/k), \sigma(x) = x\} =: \P_n(\bar{k})^{\gal(\bar{k}/k)} = \P_n(k).\]
\end{prop}
\begin{proof}
	It is clear that $\P_n(k)$ maps injectively into $\P_n(\bar{k})^{\gal(\bar{k}/k)}$ with the inclusion map.\footnote{Any element $(x_0: \dots :x_n) \in \P_n(k)$ will be invariant under $\gal(\bar{k}/k)$ because each coordinate is invariant.} Moreover, for any $(a_0: \dots : a_n) \in \P_n(\bar{k})^{\gal(\bar{k}/k)}$ and any $\sigma \in \gal(\bar{k}/k)$, we know that $(\sigma a_0: \dots : \sigma a_n) = (a_0:\dots : a_n)$, or equivalently, there is $\lambda_{\sigma} \in \bar{k}^{\times}$ such that $(\sigma a_0: \dots : \sigma a_n) = \lambda_{\sigma}(a_0:\dots : a_n)$. We obtain a map $\lambda: \gal(\bar{k}/k) \rightarrow \bar{k}^{\times} = \sigma \mapsto \lambda_{\sigma}$.\footnote{It measures the obstruction to $(a_0: \cdots : a_n)$ being defined over $k$.%TODO: what ?
	}
	
	Observe that $\lambda$ is a crossed homomorphism because if $\sigma, \tau \in \gal(\bar{k}/k)$, then $\lambda_{\sigma\tau} = \lambda_{\sigma}\sigma(\lambda_{\tau})$. %TODO: mention continuity
	Then, by Hilbert's Satz 90, we obtain some $a \in k^{\times}$ such that \[\forall\sigma \in \gal(\bar{k}/k),\sigma(a_0: \dots: a_n)  = \frac{a}{\sigma(a)} (a_0: \dots: a_n),\] so $\sigma(aa_0: \dots: aa_n) = (aa_0: \dots: aa_n)$. We found elements of $aa_j \in \bar{k}$ that are invariant under automorphisms of $\gal(\bar{k}/k)$, this means that they belong to $k$ and we conclude that $(a_0: \cdots: a_n)$ is the image of $(aa_0 : \cdots : aa_n) \in \P_n(k)$, yielding surjectivity of the inclusion map.
\end{proof}

\begin{rem}
	The ring of polynomial functions on $\P_n$ is poor. If $f(x_0, \dots, x_n)$ gives rise to a function on $\P_n$, then it would have to be invariant under scaling, i.e.: $f(\lambda x_0, \dots, \lambda x_n) = f(x_0, \dots, x_n)$, but when $k$ is infinite, this only happens when $f$ is constant.\footnote{Decompose $f$ into its homogeneous components $f_d$ of degree $d$ and fix $x \in k^{n+1}$. Then, for all $\lambda \in k^{\times}$, we have 
	\begin{align*}
	\sum_{d=0}^{\deg(f)} f_d(x) &=\lambda f(x)
	= f(x)
	= \sum_{d=0}^{\deg(f)} \lambda^d f_d(x)	
	\end{align*}
	This is satisfied for infinitely many $\lambda$'s if and only if $f$ is constant.} Although $f(a_0: \dots : a_n)$ is not well-defined when $\deg(f) > 0$, if $f$ is homogeneous of non-zero degree, then $f(a_0 : \cdots :a_n) = 0$ is a well-defined conditions for $(a_0:\cdots :a_n) \in \P_n$. This is because $f(x) = 0$ implies  $f(\lambda x) = \lambda^d f(x) = 0$.
\end{rem}
\begin{defn}
	A \textbf{$k$-projective variety} $V$ in $\P_n$ is the zero locus of a finite system of homogeneous polynomials in $k[x_1,\dots, x_n]$ :
	\[F_1(x_0:\cdots :x_n) = \cdots F_m(x_0:\cdots : x_n) = 0.\]
	We can view $V$ as a functor \[\textbf{Fields}_k \rightsquigarrow \textbf{Sets} = L \mapsto V(L) = \{x \in \P_n(L) \mid \forall i \in [m], F_i(x) = 0\}.\]
\end{defn}
\begin{rem}
	The main justification for working with projective varieties is that the intersection theory is much nicer as we will show.
\end{rem}
\begin{exmp}
	In $\P_2$ (the projective plane), lines corresponds to the solutions of a homogeneous polynomial of degree one ($ax_0+bx_1+cx_2= 0$). Considering the natural map $\bA^3 \setminus\{0\} \rightarrow \P_2$, we see that any lines in $\P_2$ is the image of a plane in $\bA^3$ through the origin. We can conclude that the fact that any two such planes in $\bA^3$ intersect only at a common line is equivalent to the fact that any two lines in $\P_2$ intersect in a common point.
\end{exmp}
\begin{defn}
	We say that a variety in $\P_n$ is \textbf{linear} if it is defined by a collection of linear equations \footnote{Systems of the form $Ax = 0$ for $A \in M_{m\times n}(k)$.}. If the set of solutions in $\bA^{n+1}$ is a vector space of dimension $d+1$, then $d$ is the dimension of the linear variety.
\end{defn}
\begin{defn}
	A \textbf{hypersurface} of degree $d>0$ is a variety defined by a single polynomial equation of degree $d$. The terms linear hypersurface (or hyperplane) and quadric hypersurfaces are used to describe hypersurfaces of degree one and two respectively.
\end{defn}
\begin{prop}
	Let $k$ be algebraically closed, $V$ be a linear variety in $\P_n(k)$ of dimension $d \geq 1$ and $S$ be a hypersurface. Then, $V$ and $S$ have a non-empty intersection.\footnote{More intuitively, this result says that any two spaces of co-dimension one and dimension one respectively will intersect. This is clearly not the case in $\bA_n$, for instance, take two parallel lines in $\bA_2$.}
\end{prop}
\begin{proof}
	We can assume without loss of generality that the dimension of the linear variety is $1$ (otherwise take a line inside it) and that $V$ is given by the equation $x_2 = x_3 = \cdots = x_n = 0$. Indeed, we can always rotate the whole space so that $V$ becomes this linear variety. More generally, the group of symmetries of $\P_n$ is $GL_{n+1}(k)$, so we can perform lots of transformations while keeping the same structure.
	
	Let $S$ be given by $F(x0: \cdots : x_n) = 0$. $S \cap L$ is given by the equations $F(x_0:x_1: 0: \cdots :0) = 0$ which is still homogeneous because all terms with $x_2,\dots x_n$ are killed. This clearly still has zeros (when one of $x_0$ and $x_1$ are zero)\footnote{Notice that $F(\cdot : \cdot : 0: \cdots : 0)$ cannot be constant unless it is zero, otherwise $F$ would have had a non-zero constant coefficient.}, so we are done.
\end{proof}
\begin{prop}
	If $n > 1$, then any two hypersurfaces in $\P_n$ have non trivial intersection over $\bar{k}$.
\end{prop}
\begin{proof}
	Let $V_1$ defined by $F(x_0:\dots: x_n) = 0$ and $V_2$ defined by $G(x_0:\dots : x_n) = 0$ be the two hypersurfaces, we will show that $V_1(\bar{k}) \cap V_2(\bar{k}) \neq \emptyset$.
	
	Assume without loss of generality that the point $(0:\dots:0:1)$ lies neither on $V_1(\bar{k})$ nor $V_2(\bar{k})$\footnote{We can always do that by using the numerous symmetries of $\P_n(\bar{k})$, we just need at least one point in $\bar{k}^{n+1}$ that does not vanish on either of $F$ and $G$. If it does not exist, then $(F,G) = \bar{k}[x_0, \dots, x_n]$, but if two homogeneous polynomials generate $1$, then one of them must be constant, a case that is not allowed for hypersurfaces.}. This implies that $F$ has $x_n^{d_1}$ as a monomial and $G$ has $x_n^{d_2}$, where $d_1$ and $d_2$ are the degrees of $F$ and $G$ respectively. Moreover, we can also assume that $V_1$ and $V_2$ are both irreducible (otherwise we replace them by the irreducible component that contains them) and that neither is contained in the other (otherwise the result is trivial).
	
	Now, we can view $F$ and $G$ as elements of $k(x_0, \dots, x_{n-1})[x_n]$ that are relatively prime in this ring, by our previous assumptions. Thus, we obtain
	\[\exists A,B \in k[x_0, \dots, x_n],\exists N \in k[x_0, \dots, x_{n-1}],\quad \frac{A}{N}F + \frac{B}{N}G = 1\Leftrightarrow AF + BG = N,\]
	where $\frac{A}{N}$ and $\frac{B}{N}$ are in lowest terms. Observe that for any irreducible factor $\phi$ of $N$, if $\phi \mid A$, then $\phi \mid BG$, but as $G$ is irreducible, we get $\phi \mid B$, implying the fractions are not in lowest terms. Similarly, we can show that if $\phi \mid B$, then $\phi\mid A$ which leads to the same contradiction. Hence, $\phi$ divides neither $A$ nor $B$. Moreover, after dividing $A$ by $G$, we can assume $\deg_{x_n}(A) <\deg_{x_n}(G)$.\footnote{The Euclidean algorithm in $k[x_0, \dots, x_{n-1}][x_n]$ yields $A = GQ + A'$ with $\deg_{x_n}(A') < \deg_{x_n}(G)$, so we have
	\[(GQ+A')F+BG = N \text{ or } A'F+(B+FQ) = N.\]}
	
	Let $\varphi$ be an irreducible divisor of $N$, we know that $V((\varphi)) \nsubseteq V(A)$ and $V((\varphi)) \nsubseteq V(B)$, so we can find $(a_0, \dots, a_{n-1}) \in \bar{k}^n$ such that $\varphi(a_0, \dots, a_{n-1})= 0$ and \[A(a_0, \dots, a_{n-1},x_n), B(a_0, \dots, a_{n-1}, x_n) \neq 0.\]
	Therefore, if we partially evaluate both sides of $AF+BG = N$ at $(a_0, \dots, a_{n-1})$, we obtain\footnote{It is important to note that both sides are non-zero because both $F$ and $G$ have a monomial involving only $x_n$ and $(a_0, \dots, a_{n-1})$ was constructed so that the partial evaluation of $A$ and $B$ are non-zero as well.} \[A(a_0, \dots, a_{n-1}, x_n)F(a_0, \dots, a_{n-1},x_n) = -B(a_0, \dots, a_{n-1},x_n)G(a_0, \dots, a_{n-1},x_n),\] and since $\deg_{x_n}(A) < \deg_{x_n}(G)$, there exists a factor $(x_n-a_n)$ of $F(a_0, \dots, a_{n-1},x_n)$ that also divides $G(a_0, \dots, a_{n-1},x_n)$. We conclude that $(a_0, \dots, a_n) \in V_1 \cap V_2$. %TODO: ask why we need phi not dividing A nor B.
\end{proof}

Our main result in this section is the analog of the decomposition of affine varieties into irreducible for their projective counterpart. We will start with a bit more generality.
\begin{defn}
	A ring $R$ is said to be graded if it is isomorphic (as an additive group) to $\oplus_{d\in \N} R_d$ and if for any $d_1, d_2 \in \N$, $R_{d_1}R_{d_2} \subseteq R_{d_1d_2}$. We denote $\pi_d$ to be the projection of $R$ onto its $d$-th coordinate.
\end{defn}
\marginnote{\begin{exmp}
		The ring $R = k[x_0, \dots, x_n]$ has the natural grading
		\[R = \bigoplus_{d\in \N} R_d,\] where $R_d$ is the group of homogeneous polynomials of degree $d$. It is clear that $R_{d_1}R_{d_2} \subseteq R_{d_1d_2}$. Moreover, the projection $\pi_d$ collects all the monomials of degree $d$ of its argument.
	\end{exmp}
}

\begin{defn}
	An ideal in a graded ring is said to be homogeneous if it is generated by homogeneous elements (i.e.: elements of $R_d$ for some $d \in \N$).
\end{defn}
\begin{thm}\label{homogenequiv}
	Let $R= \oplus_{d\in \N} R_d$ be a graded ring and $I \lhd R$, the following are equivalent:\begin{enumerate}
		\item $I$ is homogeneous.
		\item If $f \in I$, then all of its homogeneous components lie in $I$.
		\item $R/I$ is also a graded ring and
		$R/I = \oplus_{d \in \N} (R_d+I)/I$.
	\end{enumerate}
\end{thm}
\begin{proof}
	Write $I = (f_1, \dots, f_t)$ where each $f_i$ is homogeneous in $R_{d_i}$.
	
	(1 $\implies$ 2) If $f \in I$, then for some $\lambda_i \in R$, we can write \[f = \lambda_1f_1 + \cdots +\lambda_tf_t,\]
	which implies that for any $d \in \N$, \[\pi_d(f) = \pi_{d-d_1}(\lambda_1) f_1 + \cdots + \pi_{d-d_t}(\lambda_t)f_t \in I.\]
	
	(2 $\implies$ 1) Write $I = (f_1, \dots, f_t)$ where the $f_i$'s are not a priori homogeneous. If we let $D$ be the highest degree of a monomial of an $f_i$, we obtain $I = (\pi_1 f_1, \dots, \pi_D f_t)$.\footnote{This holds because all homogeneous components of polynomials in $I$ are in $I$ and $I$ is generated by the homogeneous components of its polynomials.}
	
	(2 $\implies$ 3) Clearly, we already have $R/I = \sum_{d \in \N} (R_d+I)/I$. It remains to show that this is in fact a direct sum. If $0 = \sum_{d=0}^m \lambda_d$ where $\lambda_d \in (R_d+I)/I$, then, letting $\widetilde{\lambda}_d$ be a preimage of $\lambda_d$ from the quotient map, this means $\sum_{d=0}^m \widetilde{\lambda}_d \in I$. However, this can only happen if each $\widetilde{\lambda}_d$ is in $I$,\footnote{If $\widetilde{\lambda}_d = r+i \in R_d+I$, then $\sum_{d=0}^m \widetilde{\lambda}_d - i$ is in $I$ but has $r$ as its homogeneous component, so $r \in I$ and $\widetilde{\lambda}_d \in I$.} so for each $d$, $\lambda_d  = 0$. This shows we indeed have a direct sum.
	
	(3 $\implies$ 2) Is similar. %TODO
\end{proof}
\begin{defn}
	A projective variety $V \subseteq \P_n$ naturally gives rise to an affine variety $\widetilde{V} \subseteq \bA^{n+1}$ (by forgetting the quotient).\footnote{%TODO: describe explicitly
	} It is called the \textbf{affine cone} attached to $V$. This can be seen as the union of lines that are in the variety seen in the affine space.%TODO: explain.
\end{defn}
\begin{prop}[Projective Nullstellensatz]
	Let $k$ be algebraically closed, the assignment $V \mapsto I(V(k))$ gives a bijection\footnote{Note that the empty variety is is mapped to $(x_0, \dots, x_n)$ because $0$ is not considered in the projective variety.}
	\[\{\text{projective varieties over $k$}\} \leftrightarrow \{\text{homogeneous radical ideals in } k[x_0, \dots, x_n]\}.\]
	
\end{prop}
\begin{cor}
	A system of homogeneous polynomial equations $F_1 = \cdots F_t = 0$ has no solutions over an algebraically closed $k$ if and only if $\sqrt{(F_1, \dots, F_t)} = (x_0, \dots, x_n)$.
\end{cor}
\begin{rem} We can decompose the projective space as $\P_n = \Sigma_0 \amalg \cdots \amalg \Sigma_n$, where $\Sigma_j = \{(x_0: \cdots : x_n) \mid x_j \neq 0\}$. Observe that we have the bijection $\Sigma_j \cong \bA^n$: \[(x_0:\cdots : x_n)\mapsto (x_0/x_j , \dots,x_{j-1}/x_j,x_{j+1}/x_j,\dots, x_n/x_j),\] and that clearly $\P_n - \Sigma_j = \P_{n-1}$, so $\P_n = \bA^n \amalg \P_{n-1}$.\footnote{Another way to view the projective space is as a compactification of the affine space and this decomposition illustrates this idea.} Moreover, for a projective variety $V$ in $\P_n$, we can write $V = V_0 \cup \cdots \cup V_n$, where $V_j = V \cap \Sigma_j$. To obtain the equations that define $V_j$, replace $x_j$ by $1$ in each $F_i$. This operation does not preserve homogeneity of the $F_i$'s, but still we get the affine variety $V_j$.

Conversely, if $V_0 \subseteq \bA^n$ is an affine variety defined by $F_1 = \cdots = F_t = 0$. We claim that there exists $V \subseteq \P_n$ such that $V_0 = V \cap \Sigma_0$.\footnote{This is called the projective closure of $V$.} Let $D_i$ be the maximum degree of a monomial in one of the $F_i$'s, then define
\[F'_i = \sum_{d=0}^{D_i} x_0^{D_i-d}\pi_d(F_i).\] $F'_i$ is called the homogeneous completion of $F_i$ by $x_0$ and one can verify that the $F'_i$'s define the suitable variety $V$.%TODO: Prove ?
\end{rem}

%In the affine setting, we saw that for a radical ideal $I \lhd k[x_1, \dots, x_n]$, the set of maximal ideals containing $I$ is in correspondence with $V(I)(\bar{k})$. Moreover, we saw that the set of prime ideals containing $I$ are in correspondence with irreducible subvarieties of $V(i)$. The set of all ideals is in bijection with the subvarieties of $V(I)$. The set of minimal prime ideals containing $I$ is in bijection with irreducible components of $V(I)$. All these statements correspond to less obvious ring theoretic statements.

Recall that for affine varieties, their decomposition into a finite union of irreducible components $V = V_1  \cup \cdots \cup V_s$ translated to the ring-theoretic fact that any radical ideal is the intersection of a finite collection of minimal prime ideals containing $I$. We now want to talk about decomposition but for projective varieties and we will work in the opposite direction, namely, from a ring-theoretic argument to the geometric result).

\begin{lem}
	If $\lp$ is a prime ideal of a graded ring $R$, and $\lp^*$ is the ideal generated by the homogeneous elements of $\lp$, then $\lp^*$ is also a prime ideal.\footnote{Geometrically, $\lp$ corresponds to a variety and $\lp^*$ to the affine cone generated by this variety.}
\end{lem}
\begin{proof}
	Suppose there exists $a,b \notin \lp^*$ such that $ab \in \lp^*$ for a $d \in \N$ sufficiently large, write
	\[a = \pi_0(a) + \cdots + \pi_d(a) \text{ and } b = \pi_0(b) + \cdots + \pi_d(b).\]
	Let $i$ and $j$ be the greatest integers such that $\pi_i(a) \notin \lp^*$ and $\pi_j(b) \notin \lp^*$, namely, we have $\pi_{i+t}(a), \pi_{j+t}(b) \in \lp^*$ for all $t > 0$.\footnote{These indices exist because $a, b\notin \lp^*$.}
	
	By construction, $\lp^*$ is a homogeneous ideal, so for each $k$, $\pi_k(ab) \in \lp^*$ (by theorem \ref{homogenequiv}). Then, we have 
	\[\pi_{i+j}(ab) = \pi_i(a)\pi_j(b) + \sum_{t = -i}^j \pi_{i+t}(a)\pi_{j-t}(b),\]
	and since the L.H.S. is in $\lp^*$ and every element of the sum is in $\lp^*$ (by maximality of $i$ and $j$), we infer that $\pi_i(a)\pi_j(b) \in \lp^*$. Therefore, $\pi_i(a)\pi_j(b)$ is also in $\lp$ and by primality either $\pi_i(a) \in \lp$ or $\pi_j(b) \in \lp$, so either $\pi(a)_i \in \lp^*$ or $\pi_j(b) \in \lp^*$ (because they are homogeneous).
\end{proof}

\begin{prop}
	Let $R = \oplus_{j\in \N} R_j$ be a graded ring and $I \lhd R$ be homogeneous. Then, all minimal prime divisors\footnote{In the sense that it contains no other prime divisor of $I$.} of $I$ are homogeneous.
\end{prop}
\begin{proof}[Lem implies prop]
	If $\lp \supseteq I$ is a minimal prime, then, since $I$ is homogeneous, it is generated by some homogeneous elements of $\lp$. Thus,we have $\lp \supseteq \lp^* \supseteq I$ and by the previous lemma $\lp^*$ is prime, so $\lp = \lp^*$ by minimality. We conclude that $\lp$ is also homogeneous.
\end{proof}

\marginnote{\begin{rem}
	The minimality condition is a key condition. Geometrically, consider the usual ring $k[x_0, \dots, x_n]$ and an affine cone $\widetilde{V} \subseteq \bA^n$, we see that every irreducible subset of an affine cone is not necessarily affine cones. %TODO: show that.
\end{rem}}
\begin{cor}
	Every irreducible component of an affine cone is an affine cone and every irreducible component of a projective variety is a projective variety.
\end{cor}

\subsection{Introduction to $\spec$ and Schemes}
The motivation for this section is that the mapping from an ideal $I \lhd k[x_1, \dots, x_n]$ to $V(I)$ does not carry all the information. Recall that if the radical of $I_1$ is equal to the radical to $I_2$, then $V(I_1) = V(I_2)$. More precisely, we are forgetting about some quotients that contain nilpotent elements, but these are important. This section is aimed at positively answering the following:
\begin{quest}
	Is there a geometric object corresponding to general ideals of $k[x_1, \dots, x_n]$?
\end{quest}
We will indeed see that non-radical ideals have a geometric meaning, as informally shown by these simple examples.
\begin{exmps}
	\begin{enumerate}
		\item[]
		\item Consider $I = (x^2) \lhd k[x]$, we know $\sqrt{I} = (x)$ and $V(I) = \{(0)\}$ in every affine space over an extension of $k$. We also have the notion of the coordinate ring $\mO_{V(I)}$ which is $k[x]/(x) = k$. If we "redefine" a new object $\widetilde{V}$ such that its coordinate ring of is $\mO_{\widetilde{V}(I)} = k[x]/(x^2)$. Recall that $f$ vanishes on $V(I)$ if and only if $f = 0 \in \mO_V$, so we analogously define that $f$ vanishes on $\widetilde{V}(I)$ if and only if $f = 0 \in \mO_{\widetilde{V}(I)}$. The former holds if and only $f(0) = 0$ while the latter holds if and only if $f(0) = f'(0)= 0$, we have recovered the lost information.\footnote{Geometrically, we can think of the points of $\widetilde{V}(I)$ as thickened points with tangent vectors associated.}
		
		\item Consider $I = (x^2,xy,y^2) \lhd k[x,y]$. Then, $V(I)$ still only contain the origin in any affine space. However, in $\widetilde{V}(I)$, we have a thickened origin with vectors representing each first order partial derivative.
		\item The "double line": Let $I = (x^2) \lhd k[x,y]$, we have $\mO_{\widetilde{V}(I)} = k[x,y]/(x^2) = k[y] \oplus k[y]x$ with $x^2 = 0$. Also, we can write $f(x,y)$ in $\mO_{\widetilde{V}(I)}$ as $f(0,y) + \frac{\partial}{\partial x}f(0,y)x$. %TODO: develop this example.
	\end{enumerate}
\end{exmps}
The formalization of these examples will use the notion of spectrum of rings.
\begin{defn}
	The spectrum of a ring $R$ is $\spec(R) := \{\lp \lhd R\mid \lp \text{ is prime}\}$.
\end{defn}
\begin{prop}
	Denote $V(I) := \{\lp \in \spec(R) \mid I \subseteq \lp\}$\footnote{When $I$ is principal, say generated by $p$, we also write $V(p)$ for simplicity.}, then the collection of closed sets $\{V(I) \mid I \lhd R\}$ defines a topology on $\spec(R)$, we refer to it as the \textbf{Zariski topology}.
\end{prop}
\begin{proof}
	\begin{enumerate}
		\item We have $\emptyset = V(R)$ and $\spec(R) = V(0)$, so both sets are closed.
		\item If $\{I_{\alpha}\}_{\alpha \in A}$ is a collection of prime ideals, then it is clear that \begin{align*}
		\bigcap_{\alpha \in A} V(I_{\alpha}) &= \{\lp \in \spec(R) \mid \forall \alpha \in A, \lp \supseteq I_{\alpha}\}\\
		&= \{\lp \in \spec(R) \mid \lp \supseteq \sum_{\alpha \in A}I_{\alpha}\} = V\bra{\sum_{\alpha \in A} I_{\alpha}}.
		\end{align*}
		
		\item If $I_1$ and $I_2$ are prime ideals, then we claim that\footnote{The reasoning will work for any finite collection of ideals (or one could use induction), but it is not valid for infinite collections because it would require taking an infinite product.} \[V(I_1) \cup V(I_2) = \{\lp \in \spec(R) \mid \lp \supseteq I_1 \text{ or } \lp \supseteq I_2\} = \{\lp \in \spec(R) \mid \lp \supseteq I_1 \cap I_2\}.\]
		
		The $\subseteq$ inclusion is clear. For $\supseteq$, assume that $\lp \supseteq I_1 \cap I_2$ does not contain $I_1$ nor $I_2$, then there exists $a_1 \in I_1 - \lp$ and $a_2 \in I_2 - \lp$. However, this yields $a_1a_2 \in I_1 \cap I_2 \subseteq \lp$, which contradicts the primeness of $\lp$.
	\end{enumerate}
\end{proof}
%TODO: draw examples of spec(Z), spec(R[x]), spec(C[x])
\begin{exmps}
	\begin{enumerate}
		\item The spectrum of $\Z$ consists of the zero ideal and all the ideals generated by primes $p$. The closed sets are $V(n) = \{(p) \mid p \text{ is a prime factor of } n\}$, if $n\neq 0$ and $V(0) = \spec(\Z)$.
		\item The spectrum of $k[x]$ consists of the zero ideal and the ideals generated by irreducible polynomials. If $k$ is algebraically closed, then $\bA^1(k)$ is in bijection with $\spec(R) - \{(0)\}$ by sending $a$ to $(x-a)$.\footnote{In fact, this map is a homeomorphism with respect to the Zariski topology. Indeed, the inverse image of $V(I)$ is precisely the variety defined by $I$ (hence the notation), while the image of a variety $X$ is $V(I(X))$.} Otherwise, $\spec(k[x])$ contains many more points.
		\item The spectrum of $k[x,y]$ is quite more complicated even when $k$ is algebraically closed, i.e.: the inclusion $\bA^2(k) \hookrightarrow \spec(k[x,y])= (a,b) \mapsto (x-a, y-b)$ is not a bijection. This happens because $f(x,y)$ can be irreducible without being in this kind of ideal. For such an $f$, the closure of $\{(f)\}$ is $\{(f), 0\} \cup \{(x-a,y-b) \mid f(a,b) = 0\}$.
	\end{enumerate}
\end{exmps}
\begin{defn}
	Given $A \subseteq \spec(R)$, the \textbf{vanishing ideal} of $A$ is $I(A) := \cap_{\lp \in A}\lp$.
\end{defn}
\begin{prop}
	If $A \subseteq \spec(R)$, then $V(I(A)) = \overline{A}$ (the closure of $A$).
\end{prop}
\begin{proof}
	Clearly $A \subseteq V(I(A))$ and $V(I(A))$ is closed, thus $\overline{A} \subseteq V(I(A))$. Conversely, if $V(I)$ contains $A$, then for all $\lp \in A$, $\lp \supseteq I$, implying $I \subseteq \cap_{\lp \in A} = I(A)$. We obtain that $V(I(A)) \subseteq V(I)$\footnote{It is true in general that $I \subseteq J$ implies $V(J) \subseteq V(I)$.} and we conclude that $V(I(A))$ is the smallest closed set containing $A$, namely that $V(I(A)) = \overline{A}$.
\end{proof}%TODO: Change bar to overline.
\begin{lem}[Krull]
	Let $S$ be a multiplicative subset of $R$ not containing $0$. If $I$ is an ideal of $R$ that trivially intersects $S$, then there exists a prime $\lp \supseteq I$ such that $\lp \cap S = \emptyset$ as well.\footnote{Note that Krull's lemma implies the fact that every ideal is contained in a maximal ideal, taking $S = \{1\}$.}
\end{lem}
\begin{proof}
	Let $M$ be the collection of ideals $J \supseteq I$ such that $J \cap S = \emptyset$. We know that $M$ is not empty because it contains $I$ and that it satisfies the maximal chain condition, because if $\{J_{\alpha}\}_{\alpha \in A}$ is a chain in $M$, then $\cup_{\alpha\in A} J_{\alpha}$ is clearly in $M$ and is an upper bound. Thus, by Zorn's lemma, $M$ contains a maximal element $\lp$. 
	
	We claim that $\lp$ is prime. Assume towards a contradiction that $a, b \notin \lp$ and $ab \in \lp$, then both $\lp + (a)$ and $\lp +(b)$ are strictly greater than $\lp$, so they must intersect with $S$ (by maximality of $\lp$). Then, we can find $p_1, p_2 \in \lp$ and $r_1, r_2 \in R$ such that $p_1 + r_1a$ and $p_2+r_2b$ are both in $S$, hence, so is their product $p_1p_2+p_2r_1a + p_1r_2b + r_1r_2ab$. However, we see that each term in this sum is in $\lp$, so this contradicts $\lp\cap S = \emptyset$.
\end{proof}
\begin{cor}\label{nilprimes}\footnote{The second part can be seen as a Nullstellensatz and works for more general polynomial rings. The traditional approach in algebraic geometry was to work over $\bar{k}$, so the old Nullstellensatz was enough. In the modern approach, one replaces $V(I) \subseteq \bA^n(k)$ by $\spec(k[x_1, \dots, x_n]/I)$, hence the need for this more general statement.}
	\begin{enumerate}
		\item $\cap_{\lp \in \spec(R)} = \sqrt{0}$, namely the intersection of the prime ideals is precisely the nilpotent elements.
		\item For any ideal $I \lhd R$, $\cap_{\lp \in V(I)}= \sqrt{I}$.
	\end{enumerate}
\end{cor}
\begin{proof}
	\begin{enumerate}
		\item[]
		\item It is clear that $\sqrt{(0)} \subseteq \cap_{\lp \in \spec(R)}$.\footnote{Any ideal contains $0$, thus if $x^m = 0$ for some $m$, then $x^m \in \lp$ which implies $x \in \lp$ by primeness.}
		
		For the other inclusion, suppose that $x$ is not nilpotent, then let $S = \{x^m \mid m \in \N\}$, it does not contain $0$, so we can use Krull's lemma with $I = (0)$ to conclude that there is a prime ideal with $x \notin \lp$.
		\item This is a simple application of the first part to the quotient ring $R/I$.\footnote{By the fourth isomorphism theorem, $V(I)$ in $R$ is in correspondence with $V(0)$ in $R/I$. Moreover, the preimage of nilpotent elements in $R/I$ is precisely $\sqrt{I}$. We conclude with the fact that the preimage of $\cap_{\lp \in V(I)} \overline{\lp}$ is $\cap_{\lp \in V(I)} \lp$.}
	\end{enumerate}
\end{proof}
\begin{thm}
	Any ring homomorphism $\varphi: R\rightarrow S$ induces a continuous map \[\varphi^*: \spec(S) \rightarrow \spec(R) = \lp \mapsto \varphi^{-1}(\lp).\]
\end{thm}
\begin{proof}
	If $\lp \lhd S$ is prime, then it is trivial to show that $\varphi^*(\lp)$ is an ideal of $R$, to see it is prime, note that $R/\varphi^{-1}(\lp) \cong \varphi(R)/\lp \subseteq S/\lp$.\footnote{Since the R.H.S. is an integral domain (by primality), so is the L.H.S. and we conclude that $\varphi^*(\lp)$ is prime.}
	
	The continuity of $\varphi^*$ follows from the following derivation (for any ideal $I \lhd R$):
	\begin{align*}
		(\varphi^*)^{-1}(V(I)) &= (\varphi^*)^{-1}\{\lp \in \spec(R) \mid \lp \supseteq I\}\\
		&= \{\lp \in \spec(S) \mid \varphi^{-1}(\lp) \supseteq I\}\\ 
		&= \{\lp \in \spec(S) \mid \lp \supseteq \varphi(I)\}\\ 
		&= V(\varphi(I)).
	\end{align*} 
	Moreover, it is clear that $\varphi^*\psi^* = (\psi\varphi)^*$ and $\text{id}^* = \text{id}$, so we conclude that the maps $R \mapsto \spec(R)$ and $\varphi \mapsto \varphi^*$ form a contravariant functor $\textbf{Rings} \rightsquigarrow \textbf{Top}$.\footnote{The objects of $\textbf{Top}$ are topological spaces and morphisms are continuous maps.}
\end{proof}
\begin{rem}
	This functor is not injective on objects. For instance, if $R$ is a ring and $I = \sqrt{0}$, then the continuous map $\spec(R/I) \rightarrow \spec(R)$ induced by the quotient map is a homeomorphism. Hence, we can't distinguish between the two spaces and we lost the information about the nilpotent elements. 
\end{rem}
The notion of scheme which we will cover next will add extra structure on $\spec(R)$ in order to regain this information. The basic principle is that an element $f \in R$ can be viewed as a "function" on $\spec(R)$, with $f(\lp)$ being the image of $f$ in $R/\lp$. Note that the codomain of $f$ depends on the argument, so it is not truly a function. What is more, $f$ need not be determined by its values, namely, if $f$ is a nilpotent element of $R$, then $\forall \lp \in \spec(R), f(\lp) = 0$,\footnote{Recall that nilpotent elements are in the intersection of the prime ideals by corollary \ref{nilprimes}.} but we do not want to think of $f$ as $0$ (otherwise, we would not have gained any information).

We start developing the formal concepts.

\begin{defn}
	A sheaf of rings (we will often simply say sheaf) on a topological space $X$ is a contravariant functor $\mO_X: T(X) \rightsquigarrow \textbf{Rings}$\footnote{The category $T(X)$ has all the open sets of $X$ as its objects and the morphisms can be described for any open sets $U,V$ as \[\Hom(U,V) = \begin{cases}i_{U,V} &U \subseteq V\\\emptyset &\text{o/w}\end{cases}.\]} with two additional properties listed below. 
	
	Unpacked, this definition says that for each open $U \subseteq X$, we have a ring $\mO_X(U)$ and for each inclusion of open sets $U \subseteq V$, then we have a ring homomorphisms\footnote{They are called the restriction maps. And we use them with the suffix notation, namely $\mid^V_U(f) = f\mid^V_U$.} $\mid^V_U : \mO_X(V) \rightarrow \mO_X(U)$ satisfying the following properties:
	\begin{enumerate}
		\item For any open set $U$, $\mid^U_U$ is the identity on $\mO_X(U)$.
		\item For any open sets $U \subseteq V \subseteq W$, $\mid^W_U = \mid^V_U \circ \mid^W_V$.
		\item For any open cover $\{U_{\alpha}\}$ of an open set $U$, if $f \in \mO_X(U)$ is such that $\forall \alpha, f\mid^U_{U_{\alpha}} = 0$, then $f = 0$.
		\item For any open cover $\{U_{\alpha}\}$ of an open set $U$ and any collection $\{f_{\alpha} \in \mO_X(U_{\alpha})\}$ such that
		\[\forall \alpha, \beta, \quad f_{\alpha}\mid^{U_{\alpha}}_{U_{\alpha \cap U_{\beta}}} = f_{\beta}\mid^{U_{\beta}}_{U_{\alpha \cap U_{\beta}}},\] there is an element $f \in \mO_X(U)$ with $f_{\alpha} = f\mid^U_{U_{\alpha}}$ for all $\alpha$'s.\footnote{The last two properties are not part of the functoriality of $\mO_X$. Informally, they describe the locality of the sheaf, namely, the first one says that if $f$ is locally zero, then it zero and the second one says that local data can be glued together.}
	\end{enumerate}
\end{defn}
\begin{rem}
	One can infer from the third property that the restriction maps are always injective and we will see later that it follows from the fourth that $\mO_X$ is determined by where it sends an open basis of $X$. Moreover, we can use the third property to see that the glued data $f$ in the fourth property is unique. In fact, one can also define sheaves by requiring that the glued data is unique and dropping the third property.%TODO: determined from basis.
\end{rem}
\begin{defn}
	The sheaves on $X$ form a category where if $\mO_X$ and $\mO'_X$ are sheaves on $X$, then a morphism $\pi: \mO_X \rightarrow \mO'_X$ is a natural transformation between the two functors. Explicitly, it associates to any open set $U$, a homomorphism $\pi_U : \mO_X(U) \rightarrow \mO'_X(U)$ such that for any $U \subseteq V$, the following diagram commutes, where the restriction maps are coming from the appropriate functors.
	\begin{figure}[h]%TODO: align diagarm
		\centering
		\begin{tikzcd}
			\mO_X(V) \arrow[d, "\mid^V_U"'] \arrow[r, "\pi_V"] & \mO'_X(V) \arrow[d, "\mid^V_U"] \\
			\mO_X(U) \arrow[r, "\pi_U"]                        & \mO'_X(U)                      
		\end{tikzcd}
	\end{figure}
\end{defn}

\begin{defn}
	Let $X = \spec(R)$, for any $f \in R$, we define $U_f = \{\lp \in \spec(R) \mid f(\lp) \neq 0\}$, equivalently $\lp \in U_f$ if and only if $f \notin \lp$. We call these sets \textbf{distinguished} open sets.
\end{defn}
\begin{prop}
	The collection of distinguished open sets is a basis for $X$ for the Zariski topology on $\spec(R)$.%TODO: prove this.
\end{prop}

\begin{defn}
	The structure sheaf on $\spec(R)$ is defined by $\mO_X(U_f) = R[f^{-1}]$.\footnote{The intuition for this definition is that for any $g \in R[f^{-1}]$ and $\lp \in U_f$, $g \Mod{\lp}$ is well-defined. Note that $f$ cannot be nilpotent for $R[f^{-1}]$ to be well defined, but if it is, then $U_f$ is empty, so $\mO_X(U_f)$ need not be defined.}
\end{defn}
With the same intuition, we can see that $\mO_X(U_f \cap U_g) = R[f^{-1}, g^{-1}]$. Moreover, by injectivity, we get that $\mO_X(U_f \cup U_g)$ is a subring of $\mO_X(U_f) \times \mO_X(U_g)$. More precisely, since both projections need to restrict to the same thing on $\mO_X(U_f \cap U_g)$, we have
\[\mO_X(U_f \cup U_g) = \{(\alpha_f, \alpha_g) \mid \text{the images of $\alpha_f$ and $\alpha_g$ in $R[f^{-1}, g^{-1}]$ are the same}\}.\]
This is a fiber product and is denoted $R[f^{-1}] \times_{R[f^{-1},g^{-1}]} R[g^{-1}]$.
\begin{fact}
	The restriction maps are well-defined, i.e.: if $U_g \subseteq U_f$, then $R[f^{-1}] \subseteq R[g^{-1}]$.
\end{fact}
\begin{proof}
	Taking the complements, we see that \[U_g \subseteq U_f \Leftrightarrow \{\lp \in \spec(R) \mid g \in \lp\} \supseteq \{\lp \in \spec(R) \mid f \in \lp\}.\] Thus, intersecting the primes in the two sets on the R.H.S., we obtain by corollary \ref{nilprimes}, $\sqrt{(g)} \subseteq \sqrt{(f)}$. In particular, there exists $n>0$ such that $g^n = fh$, or equivalently, $f^{-1} = hg^{-n}$, so $R[f^{-1}] \subseteq R[g^{-1}]$.
\end{proof}

\begin{exmps}
	\begin{enumerate}
		\item The spectrum of a field is a single point $\ast$ and the structure sheaf assigns the whole field to $\{\ast\}$.
		\item Let $k$ be a field and $R = k[\varepsilon]/(\varepsilon^2)$, then $\spec(R) = \{\ast\}$, so it has the same topological space as the first example. However, the structure sheaf is different since it assigns $k[\varepsilon]$ to $\{\ast\}$. We see that the "functions" we obtain have more structure, namely, they can be "evaluated" and "differentiated" once.
		\item Recall that $\spec(\Z) = \{p \mid p \text{ is prime}\} \cup \{\ast\}$. For $f \in \Z$, we have $U_f = \{p \text{ prime} \mid p \nmid f\}$ and $\mO_X(U_f) = \Z[f^{-1}]$.
	\end{enumerate}
\end{exmps}
\begin{defn}
	Let $\mO_X$ be a sheaf of rings on $X$ and $x \in X$. Then, the stalk of $\mO_X$ at $X$ is\footnote{The last two equalities hold when $X = \spec(R)$. If $x$ corresponds to a prime ideal $\lp \lhd R$ (when $X = \spec(R)$), then $\mO_{X,x}$ corresponds to the localization of $R$ at $\lp$.} \[\mO_{X,x} = \varinjlim_{U \subseteq X, x \in U} \mO_X(U) = \bigcup_{U_f \ni x} \mO_X(U_f) = \bigcup_{f \in R, f \notin x} R[f^{-1}].\]
\end{defn}
%TODO: Complete with definitions.
\begin{defn}
	A morphism of affine schemes $\pi:(X,\mO_X) \rightarrow (Y, \mO_Y)$ is a continuous map $\pi: X\rightarrow Y$ along with a morphism of sheaves $\pi^\# : \mO_Y \rightarrow \pi_*\mO_X$, where $\pi_*\mO_X$ is the pushforward of the structure sheaf on $x$, i.e.: $\pi_*\mO_X(U) = \mO_Y(\pi^{-1}(U))$.
\end{defn}
\begin{prop}
	Every morphism $\pi:(\spec(S), \mO_Y) \rightarrow (\spec(R), \mO_X)$ is completely determined by the associated map $\pi^\#: \mO_X(X) \rightarrow \mO_Y(Y) = \pi_*\mO_Y(X) = R \rightarrow S$.
\end{prop}
\begin{cor}
	The functor $\spec: \textbf{Rings} \rightsquigarrow \textbf{Schemes}$ is an anti-equivalence of categories.\footnote{In other words, $\spec$ induces a bijection between the objects of the categories and bijections between the appropriate $\Hom$ sets, i.e.: for any $R,S$,  $\Hom_{\textbf{Schemes}}(\spec(S), \spec(R)) \cong \Hom_{\textbf{Rings}}(R,S)$.}
\end{cor}
The passage from $R$ to $(\spec(R), \mO_X)$ interprets morphisms of rings geometrically. If $S$ is a multiplicative set generated by some $f \in R$ not nilpotent. Then, the inclusion $R \hookrightarrow R[f^{-1}]$ induces the morphism $\spec(R[f^{-1}]) \hookrightarrow \spec(R)$. This map is called an open embedding or open inversion. If $I \lhd R$, then the quotient map $R \rightarrow R/I$ induces $\spec(R/I) \rightarrow \spec(R)$. This map is a closed embedding of $V(I)$ in $\spec(R)$.

Everything here is fairly formal. There is an extensive dictionary between objects/concepts/constructions in ring theory and corresponding geometric notions in scheme theory.

\section{Non-commutative algebras}
\subsection{General examples}
We start with a general construction of a non-commutative ring. Let $R$ be a commutative ring and $M$ be a module over $R$, then we denote $\End_R(M)$ the ring\footnote{Addition is defined point-wise and multiplication is composition of functions, i.e.: for any $\phi, \psi \in \End_R(M)$ and $x \in M$, \begin{align*}	(\phi+\psi)(x) &= \phi(x)+\psi(x)\\(\phi \cdot \psi)(x) &= \phi(\psi(x)).\end{align*}} of endomorphisms of $M$ that preserve the $R$-module structure, it is not commutative in general. A prototypical example of such rings that is thoroughly studied studied in linear algebra is when $R$ is a field and $M \cong R^n$, then endomorphisms of $M$ are precisely the $n\times n$ matrices over $R$, denoted $M_n(R)$.

Our goal for the third part of this course is to arrive at the classification of a particular kind of rings. Unsurprisingly, the class of all rings is not amenable to classification, so we will focus on a soon defined subset of $\textbf{Rings}$. We start our focus on a particular setting, namely, when $R$ is an algebra over a field $k$.\footnote{Notice that we can also view $R$ as a $k$-vector space by forgetting the ring structure and keeping only addition and multiplication by scalars (elements of $k$). When we want to emphasize this structure, we will write $V_R$. }

\begin{exmps}\label{exmpnoncomm}
	Here are some examples of such rings:
	\begin{enumerate}
		\item We have already seen the ring of $n\times n$ matrices over $k$, but it is also a $k$-algebra where the inclusion $k\hookrightarrow M_n(k)$ sends $\lambda$ to the scalar matrix $\lambda I_n$. Also, we remark that, even for general rings $R$, the assignment $R \mapsto M_n(R)$ is a functor $\textbf{Rings}\rightsquigarrow \textbf{Rings}$.
		\item With the underlying field $k$ being the real numbers, we can define the algebra of Hamiltonian quaternions $\bH := \R \oplus \R i \oplus \R j \oplus \R k$ with the relations 
		\begin{gather*}
			i^2=j^2=k^2=-1\\
			ij = -ji = k\\
			ki = -ik = j\\
			jk = -kj = i.
		\end{gather*} Surprisingly, $\bH$ is a division algebra, i.e.: every element has an inverse. \footnote{For $a,b,c,d \in \R$, we have \[(a+bi+cj+dk)^{-1} = \frac{a-bi-cj-dk}{a^2+b^2+c^2+d^2}.\] Warning: in general it is ambiguous to write fractions in a non-commutative division algebra because $\frac{x}{y}$ can be interpreted as $xy^{-1}$ or $y^{-1}x$. Fractions are well-defined precisely when the denominators are in the center, ($\R$ is the center of $\bH$, so the inverse described above is well-defined).}
		\item Let $G$ be a finite group and $k$ a field, the group ring is defined as \[k[G] := \left\{\sum_{g \in G} a_gg \mid \forall g \in G, a_g \in k\right\},\]where addition and multiplication are extended in a natural way from the field addition and group operation respectively. These rings are particularly interesting because $k$-linear representations of $G$ are equivalent to modules over $k[G]$ (in the categorical sense). 
		\item Let $k$ be a field and consider the ring $k[x,\frac{d}{dx}]$ of polynomials in $x$ and $\frac{d}{dx}$ viewed as operators on $k[x]$. Specifically, we want $\frac{d}{dx}P(x) = P'(x)$ for any polynomial in $x$.\footnote{Note that the polynomial $P(x)$ lives in $k[x]$ as a polynomial ring, it does not live in the ring of operators we are considering.} It is non-commutative because we have \[\bra{\frac{d}{dx}x - x\frac{d}{dx}}P(x) = \frac{d}{dx}xP(x) - x\frac{d}{dx}P(x) = xP'(x) + P(x) - xP'(x) = P(x),\] which implies $\frac{d}{dx}x - x\frac{d}{dx} = 1$.
	\end{enumerate}
\end{exmps}
\begin{defn} A module $M$ over a ring $R$ is said to be \textbf{simple} if it has no non-trivial submodule. A module $M$ is \textbf{semisimple} if it is a direct sum of simple submodules.
\end{defn}
\begin{exmps}\label{exmpsemisimple}
	\begin{enumerate}
		\item Let $k$ be a field, $R = M_n(k)$ and $M = k^n$ (viewed as column vectors), then $M$ is simple (as a left $M_n(k)$-module). To see this, let $N \subseteq M$ be a non-zero submodule, then $N$ contains some vector $v_1= (d_1, \dots, d_n)$ where some $d_i$ is non-zero, without loss of generality, we can say $d_1 = 1$ and $d_i = 0$ for $i> 1$.\footnote{First, multiply $v_1$ by a permutation matrix so that the first coordinate is non-zero, then apply projection onto the first coordinate and a rescaling by $d_1^{-1}$.} Then, letting $v_i = M_i v_1 \in N$ where $M_i$ is the permutation matrix corresponding to $(1\ i)$, we see that $\{v_1, \dots, v_n\} \subseteq N$ is the standard basis, so we conclude $N = M$.
		\item Any module over a field is semi-simple. This follows from a basic result in linear algebra, namely, that any vector space has a basis (even infinite dimensional vector spaces).
		\item Let $k$ be a field and $G$ a finite group: \begin{prop}[Mascke]If $\Char(k) \nmid |G|$, then every module over $k[G]$ is semisimple.\end{prop}
		\begin{proof}
			We will show that if $N \subseteq M$, then there exists a complementary submodule $N'$, i.e.: $N' \subseteq M$ such that $N\oplus N' = M$, then the result will follow because you can take a minimal non-zero submodule $N$, write $M = N \oplus N'$ and recurse on $N'$.\footnote{Argue why we can find the minimal.}
			
			Since $N$ and $M$ are $k$-vector spaces, we know that there is a projection $\pi: M\rightarrow N$ (idempotent and surjective). Observe that $\ker \pi$ is the complement of $N$ as a $k$-vector space, but it is not necessarily a $k[G]$-module. Define the following $k[G]$-module homomorphism\footnote{It is clearly a homomorphism because it is the sum of compositions of homomorphisms. Also, notice that $\frac{1}{|G|}$ is only well-defined when $\Char(k) \nmid |G|$, hence the assumption in the statement.} \[\widetilde{\pi} = \frac{1}{|G|}\sum_{g \in G} g\circ \pi \circ g^{-1} \text{, where $g$ is the (left) multiplication map by $g$.}\]
			We claim that $\im(\widetilde{\pi}) = N$. $\subseteq$ is trivial because $\pi$ always projects onto $N$ and $\supseteq$ is clear because $\widetilde{\pi}$ is the identity on $N$. We conclude that $N' = \ker \widetilde{\pi}$ is the complementary $k[G]$-submodule to $N$. %TODO: need to claim $\pi(hv) = h\pi(v)$.
		\end{proof}
	\item Let us construct a counter example to the previous proposition when $\Char(k) \mid |G|$. Let $k = \F_2$ and $C_2 = \{1, \tau\}$ be the cyclic group of order two, then we have 
	\begin{align*}
		\F_2[G] &\cong \F_2[\tau]/(\tau^2-1)\\
		&= \F_2[\tau]/((\tau-1)^2)\\
		&= \F_2[\tau-1]/((\tau-1)^2)\\
		&= \F_2[\epsilon]/(\epsilon^2)
	\end{align*}
	This is not semisimple over itself because $N = \F_2\epsilon$ is a simple submodule, but if $N'$ is its complementary submodule, then  it must contain $1+a\epsilon$ and hence $\epsilon(1+a\epsilon) = \epsilon \in N$ contradicting $N' \cap N = \emptyset$.
	\end{enumerate}
\end{exmps}
\begin{defn}
	A ring $R$ is (left)\footnote{We will see that left semisimplicity is equivalent to right semisimplicity in corollary \ref{rightleftsimple} but until then, we omit the adjective left for brevity and never work with right semisimple rings.} semisimple if it is semisimple as a left module over itself, i.e.: $R$ is a direct sum of minimal left ideals.
\end{defn}
\begin{exmps}
	\begin{enumerate}
		\item A division ring (or skew field) $D$ is a non-commutative $k$-algebra in which every non-zero element has an inverse. Such a $D$ is obviously simple over itself,\footnote{Any submodule of $D$ is an ideal, but $D$ cannot have any non-trivial ideal because $x \in D$ implies $xx^{-1} = 1 \in D$.} hence semisimple.
		\item Let $k$ be a field, $R = M_n(k)$ and denote $V_j$ the set of matrices with non-zero entries only in the $j$-th column. The latter is clearly stable under left multiplication by elements of $R$,\footnote{Not true for right multiplication.} and we have $R = V_1 \oplus \cdots \oplus V_n$. A similar argument to example \ref{exmpsemisimple}.1 yields that each $V_i$ is minimal/simple, thus $R$ is semisimple.
		\item We can extend the previous example and be slightly more abstract. Let $k$ be a field, $V$ a finite dimensional $k$-vector space and $R = \End_k(V)$. Let $\varphi_1, \dots, \varphi_n$ be a basis for $V^*$ and $V_i = \ker(\varphi_i) \subseteq V$, then we can write \[\End_k(V) \cong \Hom(V/V_1, V) \oplus \cdots \oplus \Hom(V/V_n,V),\]
		where $\Hom(V/V_j,V)$ embeds naturally in $\Hom(V,V)$.\footnote{Send a linear map $T: V/V_j \rightarrow V$ to the operator $v \mapsto T(v+V_j)$.} Each summand is clearly stable under left composition by any endomorphism because the kernel of a linear map can only grow after composition.
		
		\item A direct product of semisimple rings is semisimple. 
	\end{enumerate}
\end{exmps}

\subsection{Wedderburn's classification}
We now list several simple lemmas that will lead to Wedderburn's classification.
\begin{lem}
	Let $R = M_n(D)$ where $D$ is a division algebra over a field $k$ and $M = D^n$ viewed as a left $R$-module (we already saw it is simple), then $\End_R(M) \cong \op{D}$.\footnote{Where $\op{D}$ has the ring structure opposite to $D$, namely, $\op{a}\op{b} = \op{(ba)}$ for any $a, b \in D$. Note that it retains the usual algebra structure because $k$ embeds in the center of $D$ which is the same as the center of $\op{D}$. We often omit the $\op{\cdot}$ notation for elements of $\op{D}$.}
\end{lem}
\begin{proof}
	Define \[\phi: \op{D} \rightarrow \End_R(D^n) = d \mapsto (\cdot)d,\] where $(\cdot)d$ is the coordinate-wise multiplication by $d$ on the right. It is clear that \[\phi(d_1+d_2)(v) = \phi(d_1)(v) + \phi(d_2)(v) \text{ and } \phi(d_1d_2)(v) = \phi(d_2)\circ \phi(d_1)(v),\] so $\phi$ is indeed a homomorphism. Moreover, $\phi$ is injective because the kernel, being an ideal of $\op{D}$, must be the zero ideal.\footnote{Recall that $\op{D}$ has no non-trivial ideal and any non-zero $d$ is mapped to a non-zero map, so $\ker \phi$ cannot be the whole ring.} For surjectivity, observe that knowing where a given $f \in \End_R(D^n)$ sends $(1,0,\cdots ,0)^t$ is enough to understand the complete action of $f$ (by the action of $R$). For instance, if $f(e_1) = (d, d_2, \dots, d_n)$ and $P$ is the projection onto the first coordinate, $f(e_1) = f(Pe_1) = Pf(e_1)$, thus for each $i > 1$, $d_i = 0$. Furthermore, for each $M_i$ corresponding to the permutation $(1\ i)$, $f(e_i) = f(M_ie_1) = M_if(e_1) = de_i$. We conclude that $f = \phi(d)$. 
\end{proof}
\begin{lem}\label{lem1wedd}
	Let $R$ be a ring and view $M=R$ as a free left $R$-module of rank one, then $\End_R(M) \cong \op{R}$.
\end{lem}
\begin{proof}
	For any $\varphi \in \End_R(M)$ and $a \in M$, we have $\varphi(a) = \varphi(a\cdot1) = a\cdot \varphi(1)$. Thus, the map $\varphi \mapsto \varphi(1)$ is a bijection from $\End_k(M)$ to $R$ and it is clear that addition is preserved and multiplication is just reversed.
\end{proof}
\begin{lem}\label{endisdivring}
	If $V$ is a simple $R$-module, then $D = \End_R(V)$ is a division ring.
\end{lem}
\begin{proof}
	Let $\varphi \in D$, then since $V$ is simple, $\ker(\varphi)$ is either $V$ or $0$. The former leads to $\varphi = 0$ and the latter leads to $\varphi$ being injective. It is also easy to check $\varphi$ is surjective arguing similarly with $\im(\varphi)$. We conclude that $\varphi$ is a bijection and hence has an inverse.\footnote{Checking the inverse is $R$-linear is an easy exercise.}
\end{proof}
\begin{lem}
	If $V$ is a simple $R$-module, then $\End_R(V^n) \cong M_n(\End_R(V)) = M_n(D)$.
\end{lem}
\begin{proof}
	To see why the first equality holds, first note that any $\phi \in \End_R(V^n)$ can be decomposed into a sum of $R$-module homomorphisms $\phi_i: V^n \rightarrow V$, where $\phi_i = P_i\circ \phi$ and $P_i$ is the projection onto the $i$-th coordinate. Moreover, denote $\phi_{i,j}$ to be the restriction of $\phi_i$ to vectors with only the $j$-th coordinate being non-zero, then $\phi_i = \oplus_{j=1}^n \phi_{i,j}$ and it is clear that the matrix $\phi_{i,j}$ will act on $V^n$ just as $\phi$ does. Checking that $\phi\mapsto (\phi_{i,j})_{i, j \in [n]}$ is an isomorphism is left as an exercise.
	
	The second equality follows from the previous lemma.
\end{proof}
\begin{lem}\label{lem4wedd}
	If $M \cong V_1^{n_1} \oplus \cdots \oplus V_r^{n_r}$, where the $V_i$ are pairwise non-isomorphic simple modules, then $\End_R(M) \cong \oplus_{i=1}^r M_{n_i}(D_i)$ where $D_i = \End_R(V_i)$.
\end{lem}
\begin{proof}
	If we restrict an element $\phi \in \End_R(M)$ to one of the direct summand, then we get a map $\phi_j: V_j^{n_j} \rightarrow M$ and we claim that $\im(\phi_j) \subseteq V_j^{n_j}$. %TODO: Complete.
\end{proof}
\begin{thm}[Wedderburn's classification]\label{wedclass}
	If $R$ is a semisimple ring, then there exists $r, n_1, \dots, n_r \in \N$ and division rings $D_1, \dots, D_r$ such that $R \cong \oplus_{i=1}^r M_{n_i}(D_i)$.
\end{thm}
\begin{proof}
	By semisimplicity, we can write $R = \oplus_{j \in S} I_j$, where the $I_j$'s are minimal left ideals. First, observe that we can assume $S$ is finite because $1 \in R$ can be written uniquely as a finite sum of elements in the $I_j$'s. Namely, for some finite $S' \subseteq S$, $1 = \sum_{j \in S'} a_j$, where $a_j \in I_j$ and this implies $R = \oplus_{j \in S'} I_j$.
	
	We obtain a simpler decomposition $R = I_1^{n_1} \oplus \cdots \oplus I_r^{n_r}$,\footnote{We have regrouped the isomorphic terms toegethers, i.e.: the exponent means a direct product of $I_j$ with itself $n_j$ times as a module (not the $n_j$-th power of the ideal $I_j$).} where each $I_j$ are simple submodules of $R$ and pairwise non-isomorphic. Thus, by lemma \ref{lem1wedd} and \ref{lem4wedd}, we conclude 
	\[\op{R} \cong \End_R(R) \cong M_{n_1}(D_1) \oplus \cdots \oplus M_{n_r}(D_r),\]
	and the theorem follows.\footnote{\begin{rem}\label{remopfunc}
			It remains to show that applying $\op{}$ will not modify the relevant structure of the R.H.S. This can be decomposed into three easy checks, namely that for any $R$-module $A$ and $B$, $n \in \N$ and division ring $D$:\begin{align*}
			\op{(A\oplus B)} &\cong \op{A} \oplus \op{B},\\
			\op{M_n(D)} &\cong M_n(\op{D}),\\
			\op{D} \text{ is a }&\text{division algebra.}
			\end{align*}
		\end{rem}}
\end{proof}%TODO: GO over use of \cong
\begin{rem}
	A refinement of this theorem is that, given a semisimple ring $R$, the objects $r, n_1, \dots, n_r, D_1, \dots, D_r$ are well-defined invariants of $R$. This is a consequence of the Jordan-Holder theorem for $R$-modules. %TODO: complete with (p-83 of Knapp)
\end{rem}
\begin{cor}\label{rightleftsimple}
	$R$ is left semisimple if and only if it is right semisimple.
\end{cor}
\begin{proof}
	Observe that the functor $\op{(-)}: \textbf{Rings} \rightsquigarrow \textbf{Rings}$ maps left semisimple rings to right semisimple rings and vice-versa, but by remark \ref{remopfunc}, it does not change the structure of their decomposition. Thus, $R$ is left semisimple if and only it has a decomposition as in Wedderburn's theorem if and only if it is right semisimple.
\end{proof}
\begin{cor}\label{cordivalgclosed}
	If $R$ is a semisimple finite dimensional algebra over a field $k$, then the decomposition becomes $R \cong M_{n_1}(D_1) \oplus \cdots \oplus M_{n_r}(D_r)$ where the $D_i$'s are finite dimensional over $k$. Moreover, if $k = \bar{k}$, then $R = M_{n_1}(k) \oplus \cdots \oplus M_{n_r}(k)$.\footnote{An important instance of this specific case is the group ring: \[\C[G] = M_{n_1}(\C) \oplus \cdots \oplus M_{n_r}(\C),\] where $n_1, \dots, n_r$ are the dimensions of the irreducible $k$-linear representations of $G$. It follows by dimensionality check that $|G|$ is the sum of the squares of the dimensions of its irreducible representation, a useful result in representation theory.}
\end{cor}
\begin{proof}
	Finite dimensionality of the $D_i$'s is clear and the claim that $D_i = k$ holds because if $\alpha \in D_i$, then $k(\alpha)$ is finite dimensional over $k$ and still commutative because $\alpha$ commutes with all of $k$ and its powers. Therefore, $k(\alpha)$ is a finite dimensional field extension, thus $\alpha$ is algebraic and $\alpha \in k$. We conclude $D_i = k$.
\end{proof}

\subsection{Semisimple Algebras}
\begin{defn}
	A ring $R$ is \textbf{simple} if it has no proper non-trivial two-sided ideals.
\end{defn}
\begin{rem}
	The motivation for this definition is that the kernel of a homomorphism is a two-sided ideal, thus if $R$ is simple then any non-zero $\phi : R\rightarrow S$ is injective. This fact will be used in lots of arguments, often with the additional fact that $R$ and $S$ have the same dimension over some field yielding that $\phi$ is an isomorphism.
\end{rem}
\begin{exmps}\label{exmpsimple}
	\begin{enumerate}
		\item Any field is simple and if $R$ is commutative and simple, then it has no non-trivial ideal so it is a field.
		\item If $D$ is a division ring and $n \in \N$, then $M_n(D)$ is simple.
		\begin{proof}
			Let $I$ be a non-zero two-sided ideal of $M_n(D)$, pick a non-zero $X \in I$ and $s,t \in [n]$ such that $X_{s,t} \neq 0$. We can assume that $X_{s,t} = 1$ because we can rescale by any element of $D$. Let $E_{i,j} \in R$ be the matrix which has a one in its $(i,j)$-th entry and zeros everywhere else. One can verify that $E_{s,s} X E_{t,t} = X_{s,t}E_{s,t} = E_{s,t} \in I$.\footnote{We first recall that \[E_{a,b}E_{c,d} = \delta_{b,c}E_{a,d},\]
			and that we can decompose $X$ as
			\[X = \sum_{k,\ell \in [n]} X_{k,\ell}E_{k,\ell}X_{k,\ell}.\]
			Thus, it follows that
			\begin{align*}
			E_{i,i}XE_{j,j} &= \sum_{k,\ell \in [n]} \delta_{i,k}X_{k,\ell}E_{i,\ell}E_{j,j}\\
			&=\sum_{k,\ell \in [n]} \delta_{i,k}X_{k,\ell}\delta_{\ell, j}E_{i,j} =X_{i,j}E_{i,j}.
			\end{align*}}
			Then, we conclude $E_{i,j} \in I$ for any $i,j \in [n]$ because we can apply any permutation to the columns and rows by multiplying by permutation matrices on the left and right respectively. Since these matrices generate $M_n(D)$, we conclude $I = M_n(D)$.
		\end{proof}
		\item Recall the ring $R = \C[x,\frac{d}{dx}]$ with $x\frac{d}{dx} - \frac{d}{dx}x = -1$ from example \ref{exmpnoncomm}.4. We claim that $R$ is simple.
		\begin{proof}
			Let $I$ be a non-zero two-sided ideal, it contains a non-zero element 
			\[\alpha = P_r(x)\frac{d^r}{dx} + \cdots + P_1(x)\frac{d}{dx} + P_0(x).\]
			One can check that $[\frac{d}{dx}, P(x)] = P'(x)$, $[\frac{d}{dx}, P(x)\frac{d^t}{dx}] = P'(x)\frac{d^t}{dx}$ and $[\frac{d^r}{dx}, x] = r \frac{d^{r-1}}{dx}$. Thus, if we let $m = \max\{\deg P_j \mid 0\leq j \leq r\}$, we can compute 
			\[... m\ times[\frac{d}{dx},[\frac{d}{dx}, \alpha]] = a_t\frac{d^t}{dx} + \cdots + a_1 \frac{d}{dx} + a_0 = \beta,\]
			where $a_i \in \C$ and $0\leq t \leq r$. Now apply the commutator with $[\beta, x]$ $r$ times to obtain $t!a_t \in I$. Thus, $I$ has a unit and $I = R$. %TODO: Complete this proof.
		\end{proof}
		Remarkably, we can also show that $R$ is not semisimple.
		\begin{proof}
			Consider the chain $R \supseteq R\frac{d}{dx} \supset R\frac{d^2}{dx} \supset \cdots$. We claim that these inclusions are all proper, namely, for any $t$, $R\frac{d^t}{dx} \supset R\frac{d^{t+1}}{dx}$. Notice that $\frac{d^t}{dx} \notin R\frac{d^{t+1}}{dx}$ because $\frac{d^t}{dx}x^t = t! \neq 0$ whereas for any $P \in R$, $P(x,\frac{d}{dx})\frac{d^{t+1}}{dx}x^t = 0$.
			
			We have shown that $R$ has an infinite composition series and this implies $R$ is a not a finite direct sum. %TODO: Why ?
		\end{proof}
	\end{enumerate}
\end{exmps}
In order to avoid examples such as $\C[x,\frac{d}{dx}]$ where a simple ring is not semisimple, we can impose some type of finiteness condition on $R$. We explore two different options.
\begin{thm}\label{findimalgsemi}
	If $R$ is a finite dimensional $k$-algebra which is simple, then $R \cong M_n(D)$ for some division $k$-algebra $D$ and hence is semisimple.
\end{thm}
\begin{proof}
	Let $V$ be a minimal non-zero left ideal of $R$.\footnote{Minimal non-zero ideals exist because of finite dimensionality. Indeed, left ideals can be seen as $k$-subspaces of $R$, so you cannot have a strictly decreasing chain os left ideals if $R$ is finite dimensional.} If $r \in R$, then since the right multiplication map $(-)r : V \rightarrow V\cdot r$ is a homomorphism of left $R$-modules, it has a trivial kernel because $V$ is minimal, thus $V\cdot r$ is either $0$ or isomorphic to $V$ as a left $R$-module. Therefore, $\sum_{r \in R} V\cdot r$ is a non-zero two-sided ideal,\footnote{It is clear that it is a left ideal because each $V\cdot r$ is a left ideal. Now, multiplying on the right by any $r' \in R$ yields \[\sum_{r \in Rr'} V\cdot r \subseteq \sum_{r \in R} V\cdot r,\] so it is indeed a right ideal.} and hence must be equal to $R$. 
	
	Now, each of these $V\cdot r$ are minimal ideals, so we have written $R$ as a sum of simple submodules. Furthermore, this sum can be made finite with a similar argument as in the proof of \ref{wedclass} and it is a direct sum because the intersection of any two distinct minimal ideal is a smaller ideal, so it must be trivial. From Wedderburn's classification, we infer that $R \cong M_n(D)$ because if there were more terms in the sum, $R$ would not be simple.
\end{proof}
\begin{cor}
	Every finite dimensional simple $k$- algebra is semisimple and every finite dimensional semisimple $k$-algebra is a direct sum of simple $k$-algebras. %TODO: understand second part.
\end{cor}
\begin{defn}
	A ring $R$ is (left) Artinian if any strictly descending chain of left ideals is finite.\footnote{Note the parallel with Noetherian rings. We will usually refer to left Artinian rings simply as Artinian rings and we will be precise when talking about both kinds at the same time. Just as Noetherian rings have maximal ideals, Artinian rings always contains minimal non-zero ideals.}
\end{defn}
\begin{exmps}
	\begin{enumerate}
		\item Any finite ring is trivially Artinian. (e.g.: $\Z/n\Z$, $M_m(\Z/n\Z)$, etc.)
		\item PID's are Noetherian but usually not Artinian (e.g.: $\Z$ contains the infinite decreasing chain $(2) \supseteq (4) \supseteq (8) \supseteq \cdots $).
		\item A finite dimensional $k$-algebra is both left and right Noetherian and Artinian (e.g.: $\C[G]$, where $G$ is finite). Recall that ideals will be subspaces of that algebra and infinite decreasing or increasing chains cannot exist in finite dimensional vector spaces.
		\item A semisimple ring $R$ is both left and right Noetherian and Artinian.
		\begin{proof}
			We will show that left semisimple implies left Noetherian and Artinian. The argument for the "right" counterpart of this statement is similar and since left semisimple and right semisimple is equivalent, the claim will be proven.
			
			%TODO: Complete.
		\end{proof}
		\item The ring $\C[x,\frac{d}{dx}] $ is neither left nor right Artinian.
		\begin{proof}
			Consider the chain $(x) \supseteq (x^2) \supseteq \cdots$ as left ideals and as right ideals separately.\footnote{More precisely, we can see $(x^k)$ as the left ideal generated by $x^k$ or the right ideal generated by $x^k$.} %TODO: complete.
		\end{proof}
		\item Triangular rings: Let $R$ and $S$ be two rings, an $(R,S)$-bimodule ${}_RM{}_S$ is an abelian group (with operation $+$) equipped with a left $R$-module and a right $S$-module structure such that $r(ms) = (rm)s$ for any $r \in R$, $s \in S$, $m \in M$.\footnote{Example of bimodule: If $V_1$ and $V_2$ are $k$-vector spaces, then $\Hom_k(V_1, V_2)$ is naturally $(\End_k(V_2), \End_k(V_1))$-bimodule with the actions being function composition.} The \textbf{triangular} ring associated to a bimodule ${}_RM_S$ is \[A = \left\{\begin{bmatrix}
		r&m\\0&s\end{bmatrix} \mid r \in R, s \in S, m\in M\right\},\] where addition and multiplication are the usual matrix addition and multiplication.\footnote{We note that order of multiplication is important and having a bimodule structure on $M$ is necessary to make the multiplication well-defined:
		\[\begin{bmatrix}
		r_1&m_1\\0&s_1\end{bmatrix}\begin{bmatrix}
		r_2&m_2\\0&s_2\end{bmatrix} = \begin{bmatrix}
		r_1r_2&r_1m_2+m_1s_2\\0&s_1s_2\end{bmatrix}.\]}
		In the following, we denote
		\[e_1 = \begin{bmatrix}1&0\\0&0\end{bmatrix}, \quad e_2 = \begin{bmatrix}0&1\\0&0\end{bmatrix}, \quad e_4 = \begin{bmatrix}0&0\\0&1\end{bmatrix}.\]
		Observe that the subset $Me_2$ is a two-sided ideal of $A$ because
		\[\begin{bmatrix}
		r_1&m_1\\0&s_1\end{bmatrix}\begin{bmatrix}
		0&m\\0&0\end{bmatrix} = \begin{bmatrix}
		0&r_1m\\0&0\end{bmatrix} \text{ and } \begin{bmatrix}
		0&m\\0&0\end{bmatrix}\begin{bmatrix}
		r_2&m_2\\0&s_2\end{bmatrix} = \begin{bmatrix}
		0&ms_2\\0&0\end{bmatrix}.\]
		Moreover, we have a clear isomorphism $A/Me_2 \cong R \times S$.
		
		\begin{prop}
			If $R$ and $S$ are left Artinian, then $A$ is left Artinian if and only if $M$ is finitely generated over $R$.
		\end{prop}
		
		%TODO: this exercise : Suppose that $R$ and $S$ are left Artinian, when is $A$ left Artinian? when is it right Artinian. (finite generation of $M$ as an $R$-module or $S$-module).
		
		\item In the setting of the last item, let $R = \Q(x)$, $S = \Q$ and $M= \Q(x)$ with the natural bimodule structure, then $A$ is left Noetherian and Artinian but neither right Noetherian nor right Artinian.
		\begin{proof}
			Let $I$ be a left ideal of $A$ we will show that it finitely generated, showing $A$ is left Noetherian. Consider first the case where 
			\[I \subseteq \begin{bmatrix}\Q(x)&\Q(x)\\0&0\end{bmatrix}\]
			and notice that the R.H.S. is also a left ideal and that it has the left $\Q(x)$-module structure of $\Q(x)^2$ where multiplication by $q \in \Q(x)$ is simulated by multiplication by $qe_1$ on the left, i.e.:
			\[\begin{bmatrix}q_1&r_1\\0&0\end{bmatrix}+qe_1\cdot\begin{bmatrix}q_2&r_2\\0&0\end{bmatrix} = \begin{bmatrix}q_1+qq_2&r_1+qr_2\\0&0\end{bmatrix}.\]
			
			Since $\Q(x)$ is a PID, we see that $I$ is generated by at most two elements as a module\footnote{Any submodule $B$ of a free module $A$ over a PID is a free module of rank at most the rank of $A$.} and hence as an ideal. 
			
			In the second case, we know there exists a non-zero $a \in \Q$ such that 
			\[\begin{bmatrix}q_1&q_2\\0&a\end{bmatrix} \in I,\]
			then for any $q \in \Q(x)$, multiplying the element above on the left by $\frac{q}{a}e_2$ yields $qe_2$, so we conclude
			\[\Q e_2 := \left\{\begin{bmatrix}0&q\\0&0\end{bmatrix}\mid q \in \Q \right\} \subset I.\]
			Then, the projection that forgets about the top right coordinate maps into $\Q(x) \times \Q$ and has $\Q(x)e_2$ as its kernel., so we conclude that $A/\Q(x)e_2 \cong \Q(x) \times \Q$. The R.H.S. being Noetherian, we can see that $I/\Q(x)e_2$ is finitely generated in $A/\Q(x)e_2$ and hence so is $I$ in $A$.
			
			We now find an infinite ascending strict inclusion chain of right ideals in $A$ to show it is not right Noetherian. For any $n \in \N$, let
			\[I_n = x^{-n}\Q e_2 = \left\{\begin{bmatrix}0&\frac{q}{x^n}\\0&0\end{bmatrix}\mid q \in \Q \right\},\]
			it is a right ideal of $A$ because for any $q_1,q_2 \in \Q(x)$ and $a \in \Q$,
			\[\begin{bmatrix}0&\frac{q}{x^n}\\0&0\end{bmatrix}\begin{bmatrix}q_1&q_2\\0&a\end{bmatrix} = \begin{bmatrix}0&\frac{aq}{x^n}\\0&0\end{bmatrix} \in I_n.\]
			Moreover, it is clear that $I_1 \subseteq I_2 \subseteq \cdots$, but each inclusion is proper because $\frac{1}{x^{n+1}} e_2 \in I_{n+1} - I_{n}$. Thus, we get the desired chain of ideals.
			%TODO: complete with Artinian.
		\end{proof}
	\end{enumerate}
	
\end{exmps}
\begin{prop}
	Finitely generated modules over a left Artinian ring $R$ satisfies the descending chain conditions for left $R$-modules.
\end{prop}
\begin{proof}
	Exercise. %TODO: Use annihilator
\end{proof}
\begin{thm}[Artin]
	Let $R$ be a simple ring, then the following are equivalent:
	\begin{enumerate}[i.]
		\item $R$ is left Artinian.
		\item $R$ has a minimal non-zero left ideal.
		\item $R$ is semisimple.
		\item $R \cong M_n(D)$, where $D$ is a division ring.
	\end{enumerate}
\end{thm}
\begin{proof}
	(i $\implies$ ii) Follows from the definition of Artinian.
	
	(ii $\implies$ iii)\footnote{We use a generalization of the argument from theorem \ref{findimalgsemi}'s proof.} Let $I$ be a minimal left ideal in $R$ and recall that for any $r$, $I\cdot r$ is either $0$ or a minimal left ideal isomorphic to $I$. Moreover, we have that $\sum_{r \in R} I\cdot r$ is a non-zero two-sided ideal, so is equal to $R$. We can then conclude $R$ is semisimple.
	
	(iii $\implies$ iv) From the classification of semisimple rings, $R$ is isomorphic to a direct sum of matrix rings over division rings. If there were more than one summand, $R$ would not be simple, so iv follows.
	
	(iv $\implies$ i) Observe that $M_n(D)$ is a free left module over $D$ and has rank $n^2$\footnote{The action of $D$ is multiplication by scalar matrices and generators are $\{E_{i,j} \mid i, j \in [n]\}$.}, thus left ideals correspond to submodules and an infinite strict chain of submodules cannot occur (because of finite dimensionality).
\end{proof}
\begin{defn}
	An ideal $I$ in a ring $R$ is said to be \textbf{nilpotent} if there exists $k$ such that $I^k = 0$, i.e.: $a_1\cdots a_k = 0$ for all $a_1, \dots, a_k \in I$.\footnote{Note that, in general, every element in an ideal being nilpotent does not imply the ideal is nilpotent. We call ideals satisfying this property \textbf{nil} ideals.}
\end{defn}
\begin{lem}\label{sumnilp}
	If $I_1$ and $I_2$ are nilpotent left ideals, then $I_1 + I_2$ is also nilpotent. %TODO: maybe remove this.
\end{lem}
\begin{proof}
	Let $r,s \in\N$ be such that $I_1^r = I_2^s = 0$, then consider the general $r+s$-fold product of elements of $I_1+I_2$
	\[(a_1+b_1)\cdots (a_{r+s}b_{r+s}); \forall 1\leq i\leq r+s, a_i \in I_1, b_i \in I_2.\]
	Observe that a any monomial in this product has either more than $r$ $a_i$'s or more than $s$ $b_i$'s, and since $I_1$ and $I_2$ are left modules, we can see this monomial as a product of at least $r$ elements of $I_1$ or $s$ elements of $I_2$.\footnote{For instance, if $r = 3$ and the monomial is $b_1a_1b_2b_3a_4a_5$, then we can collapse it into $a'_1a'_2a'_3$ where $a'_1 = b_1a_2$, $a'_2 = b_2b_3a_4$ and $a'_3 = a_5$. We notice that each $a'_i$ is in $I_1$, so this monomial is zero. This argument is not necessary in commutative rings as we can rearrange the product to have all the $a_i'$ together and we get a zero in the product.} We conclude that all monomials are zero and hence the product is zero and this implies $I_1+I_2$ is nilpotent.
\end{proof}
\begin{lem}
	If $I$ is a nilpotent left ideal in an Artinian ring, then it is contained in a nilpotent two-sided ideal.
\end{lem}
\begin{proof}
	Let $J = \sum_{r \in R} I\cdot r = IR$, note that $J$ is still nilpotent\footnote{Indeed, if $I^k= 0$, then we can prove $J^k = 0$ by using the same collapsing trick as in lemma \ref{sumnil}, namely, we have 
	\[i_1r_1\cdots i_kr_k = i_1i'_2\cdots i'_k = 0.\]} and is a two-sided ideal that contains $I$.
\end{proof}
\begin{lem}\label{nilisnilp}
	If $R$ is an Artinian ring and $I$ is a nil left ideal, then $I$ is nilpotent.
\end{lem}
\begin{proof}
	Consider the infinite chain of ideals 
	$I \supseteq I^2 \supseteq \cdots$. Since $R$ is Artinian, the chain must become constant at some $k \in \N$. We will prove that $J := I^k = 0$. Suppose it is not and let $K_0$ be the minimal ideal such that $JK_0 \neq 0$,\footnote{It exists because $R$ is Artinian.} then since $J^2 = I^{2k} = I^k = J$ is non-zero, we obtain $K_0 \subseteq J$.
	
	Now, fix $a \in K_0$ such that $Ja \neq 0$, we have $Ja \subseteq K_0$ and $J(Ja)= J^2a = Ja \neq 0$, thus by minimality of $K_0$, we must have $Ja = K_0$. This implies there exists $x \in J$ such that $xa = a$ and hence $x^na = a\neq 0$ for any $n \in \N$. This contradicts the fact that $x$ is nilpotent.
\end{proof}

\begin{thm}
	If $R$ is an Artinian ring, then there is a unique maximal nilpotent two-sided ideal of $R$.
\end{thm}
\begin{proof}
	Let $J$ be the sum of all nilpotent left ideals in $R$, then $J$ is clearly a left ideal and we claim that it is two-sided. Indeed, for any $a \in I$, $a$ can be written as a finite sum, so there are ideals $I_1, \dots, I_n$ such that $a \in \sum_{i=1}^n I_i = K$. Since $K$ is a nilpotent left ideal, we can infer that $a$ is nilpotent and that it is contained in a two-sided nilpotent ideal $K^{+}$. We infer that $aR \subseteq K^{+} \subseteq J$,\footnote{The first inclusion holds because $K^{+}$ is two-sided and the second holds because $K^{+}$ is a nilpotent left ideal, so it is a summand in $J$.} so we conclude that $J$ is a two-sided ideal.
	
	Furthermore, observe that any element in $J$ is nilpotent because it is contained in a finite sum of nilpotent left ideals. Thus, by lemma \ref{nilisnilp}, we have that $J$ is nilpotent. Uniqueness and maximality follows trivially because any other nilpotent two-sided ideal is a nilpotent left ideal and hence contained in $J$.
\end{proof}
\begin{defn}
	This unique maximal nilpotent two-sided ideal is called the Artin-Wedderburn radical.
\end{defn}

\begin{lem}[Brauer]
	If $I$ is a minimal left ideal, then either $I^2 = 0$ or $\exists e \in R$ such that $e^2 = e$ and $I = Re$.
\end{lem}
\begin{proof}
	Recall that for any $a \in R$, $I\cdot a$ is either $0$ or isomorphic to $I$ by the minimality of $I$. If $I^2 \neq 0$, then $\exists a \in I$ such that $Ia \neq 0$, then $Ia = I$ and right multiplication by $a$ is an isomorphism from $I$ to itself.\footnote{$Ia$ is a subset of $I$ isomorphic to $I$, so it must be $I$.} Hence, there exists $e \in I$ such that $ea = a$ and we infer that $(e^2-e)a = 0$ and since $e^2-e \in I$ and $a$ is a bijection (its kernel is trivial), we must have $e^2 -e = 0$. In other words, $e$ is idempotent and furthermore, it is obvious that $Re \subseteq I$ and then $I = Re$ follows from minimality of $I$.
\end{proof}

\begin{thm}\label{zeroawrad}
	If $R$ is Artinian and its Artin-Wedderburn radical is zero, then $R$ is semisimple.
\end{thm}
\begin{proof}
	Let $I_1$ be a minimal left ideal of $R$. If $I_1^2 = 0$, then $I_1$ is contained in the Artin-Wedderburn radical (use the lemmas), so $I_1 = 0$ which is a contradiction. Thus, $I_1^2 \neq 0$ and by Brauer's lemma, there exists an idempotent $e \in I$ that generates $I$, namely $I = Re$. Observe that $R = Re + R(1-e)$. Moreover, if $x \in Re$, then $xe = x$ and if in addition $x \in R(1-e)$, then $xe = y(1-e)e = 0$.  We conclude that $Re \cap R(1-e) = \{0\}$ and that $R$ is a direct sum.
	
	We found that $R = I_1 \oplus I_1'$ where $I_1'$ is a left ideal. If $I_1'$ is minimal, we are done. Otherwise, we can apply the same argument on $I_1'$ to get a decomposition $I_2 \oplus I_2'$ and the recursion is guaranteed to terminate because $R$ is Artinian. The end result is a decomposition of $R$ into minimal left ideals and we conclude $R$ is semisimple.
\end{proof}
\begin{cor}
	If $R$ is an Artinian ring and $J$ is its A-W radical, then $R/J$ is semisimple.\footnote{Prove this.}
\end{cor}
For completeness, we will state Wedderburn's main theorem that refines the last result, but we will not prove it. The results needed to prove it are covered in chapter II.5 of Knapp.
\begin{thm}
	If $R$ is a finite dimensional $k$-algebra and $J$ its Artin-Wedderburn radical, then there is a semisimple $k$-algebra $S \subseteq R$ such that $R \cong S \oplus J$ as $k$-vector spaces.\footnote{
		\begin{rem}
			We have proved already that there is an exact sequence
			\[0 \rightarrow J \rightarrow R \rightarrow S \rightarrow 0,\]
			where $S$ is semisimple and this finer theorem states that there is a splitting $S \rightarrow R$.
	\end{rem}}
\end{thm}
\begin{exmps}
	\begin{enumerate}
		\item Let $R$ be the ring of upper triangular $2\times 2$ matrices with entries in a division ring $D$. A simple computation shows that if the diagonal entries of an element are non-zero, then any power of this element will be non-zero. Thus, we can see the Artin-Wedderburn radical is \[I = \left\{\begin{bmatrix}0&b\\0&0\end{bmatrix}\mid b\in D \right\}.\]It is also easy to show that we have a decomposition $R/I = D\oplus D$.
		
		\item Let $R$ be the ring of upper triangular $(n+1)\times (n+1)$ matrices with entries in a division ring $D$. Similarly to above, we can infer that the Artin-Wedderburn radical $J$ is the set of matrices with a zero diagonal. One can further observe that $J^n = 0$, but $J^k \neq 0$ for $1\leq k < n$.
		
		\item Let $A$ be the triangular ring associated to ${}_RM_S$, this gives a generalization of the first example. The A-W radical is the set of matrices of the form $\begin{bmatrix}0&m\\0&0\end{bmatrix}$ and we have the decomposition $A/J = R \oplus S$.
	\end{enumerate}
\end{exmps}
\begin{rem}
	The converse of theorem \ref{zeroawrad} is also true, namely, if an Artinian ring $R$ is semisimple, then its Artin-Wedderburn radical is zero. To see this, recall Wedderburn's classification that states if $R$ is semisimple, then \[R \cong M_{n_1}(D_1) \oplus \cdots \oplus M_{n_t}(D_t).\] Moreover, we know from example \ref{exmpsimple}.2 that $M_{n}(D)$ is simple for any $n\in \N$ and division algebra $D$, so any two sided ideal must contain a full summand of $R$,\footnote{To obtain this, one has to argue that the projection of a two-sided ideal on any of the summand is also a two-sided ideal of the summand.} and hence it cannot be nilpotent. We conclude that the Artin-Wedderburn radical is zero.
\end{rem}

\begin{fact} %TODO: What is that ?
	The following are equivalent.
	\begin{enumerate}
		\item $B$ is semisimple as a module over itself $B = \oplus Ib_i$.
		\item Every left $B$-module is isomorphic to a sum of modules isomorphic to $I$.
		
		If $M$ is a left $B$-module and $M'$ is a submodule, then there exists $M''$ submodule such that $M = M' \oplus M''$.
	\end{enumerate}
\end{fact}

Our final goal for this course is to delve more deeply into the structure of simple $k$-algebras.

\subsection{Central simple algebras}
In this section, we let $k$ be a field and $A$ and $B$ be finite dimensional $k$-algebras. Recall the usual definition $A \otimes_k B$ for $k$-vector spaces where elements are of the form\footnote{Note that this representation is not unique.}\[\sum_{i} a_i \otimes b_i, \text{ with } a_i \in A, b_i \in B.\] We also have $\dim_k(A\otimes_k B) = \dim_k(A)\dim_k(B)$.\footnote{There is a natural basis for the tensor product of two vector spaces. That is, if $\{a_i \mid i \in [n]\}$ and $\{b_i \mid i \in [m]\}$ are $k$-bases for $A$ and $B$ respectively, then $\{a_i\otimes b_j \mid i\in [n], j\in[m]\}$ is a basis for $A \otimes_k B$.} Moreover, $A\otimes_k B$ inherits the structure of $k$-algebra by setting $(a_1 \otimes b_1)(a_2 \otimes b_2)= (a_1a_2 \otimes b_1b_2).$\footnote{We leave the cumbersome task of verifying this is a well-defined operation to the reader. Alternatively, we refer the reader to the Higher Algebra I lecture notes written by Prof. Eyal Goren for a more general and thorough study of tensor products.}

\begin{rem}
	$A$ is naturally a sub-algebra of $A \otimes_k B$ by sending $a$ to $a\otimes 1$ and similarly for $B$. This is clear when consider the definition of $A \otimes_k B$ as the pushout of the following diagram in the category of $k$-algebras.
	\begin{figure}[h]
		\centering
		\begin{tikzcd}
			k \arrow[d] \arrow[r] & A \\
			B                     &  
		\end{tikzcd}
	\end{figure}
\end{rem}
\begin{fact}\label{factstensor}
	For any $A, B, C \in \textbf{Alg}_k$, we have
	\begin{enumerate}
		\item $A \otimes_k (B \oplus C) \cong A\otimes_k B \oplus A \otimes_k C$ (distributivity),
		\item $A \otimes_k (B \otimes_k C) \cong (A \otimes_k B) \otimes_k C$ (associativity), and
		\item $A \otimes_k B \cong B \otimes_k A$ (commutativity).
	\end{enumerate}
\end{fact}

\begin{exmps}
	\begin{enumerate}
		\item If $L$ is any $k$-algebra and $n \in \N$, then $M_n(k) \otimes_k L \cong M_n(L)$. 
		
		\begin{proof}
			As $k$-vector spaces, we have 
			\[M_n(k) \cong kE_{1,1} \oplus \cdots \oplus kE_{n,n},\]
			then if we tensor by $L$ and apply the isomorphisms\footnote{Explain why they are isomorphisms.} $kE_{i,j} \otimes_k L \rightarrow LE_{i,j} = kE_{i,j} \otimes \ell \mapsto k\ell E_{i,j}$, we get
			\[M_n(k) \otimes_k L \cong (kE_{1,1} \otimes_k L) \oplus \cdots \oplus (kE_{n,n}\otimes_k L) \cong LE_{1,1} \oplus \cdots \oplus LE_{n,n} \cong M_n(L).\]
			
		\end{proof}
		\item A special case of the above yields $M_{n_1}(k) \otimes_k M_{n_2}(k) \cong M_{n_1}(M_{n_2}(k)) \cong M_{n_1n_2}(k)$.
		\item Let $\bH$ be the Hamiltonian quaternions then $\bH \otimes_{\R} \C \cong M_2(\C)$. %TODO: find elementary proof (not using the double centralizer).
	\end{enumerate}
\end{exmps}

\begin{prop}\label{finprodext}
	If $F$ and $L$ are finite field extensions of $k$ with $F/k$ separable, then $F \otimes_k L$ is a finite product of field extensions of $k$.\footnote{Separability is crucial in this statement as is witnessed by the following. Let $k = \F_p(t)$ and $F = L k[x]/(x^p-t) = \F_p[t^{1/p}]$. What is $F \otimes_k F$? By our argument beside, it is isomorphic to \[F[x]/(x^p-t) = F[x]/(x-t^{1/p})^p,\] and this cannot be a product of fields because it has nilpotent elements, namely $(x-t^{1/p})$.}
\end{prop}
\begin{proof}
	The primitive element theorem implies that $F = k[\alpha] = k[x]/(p(x))$, where $p(x)$ is the minimal polynomial of $\alpha \in k$. Taking the tensor, we obtain
	\[F \otimes_k L \cong k[x]/(p(x)) \otimes_k L \cong (k[x] \otimes_k L)/(p(x) \otimes 1) \cong L[x]/(p(x)).\]
	Note that there is no reason that $p(x)$ stays irreducible in $L[x]$, but it is still separable, so $p(x)$ factors into distinct irreducible factors $p(x) = p_1(x) \cdots p_n(x)$, thus we can use CRT to get the finite product of field extensions.
\end{proof}

\begin{quest}
	If $A$ and $B$ are simple/semisimple algebras over $k$, what can we say about $A \otimes_k B$?
\end{quest}

To answer this question, we will need to understand the structure of two-sided ideals of $A \otimes_k B$. Moreover, we will assume from now on that all algebras are finite dimensional.

\begin{defn}
	A $k$-algebra $A$ is \textbf{central} if the center of $A$ is exactly $k$.
\end{defn}
\begin{exmps}
	\begin{enumerate}
		\item $\bH$ is central simple over $\R$. Even though it contains copies of $\C$ they do not commute with each other.
		\item $M_n(k)$ is central simple over $k$. The only matrices in the center are the scalar matrices ($\text{diag}(\lambda, \dots, \lambda)$ for $\lambda \in k$).
		\item If $L$ is a field extension of $k$, then $L$ is central over $k$ if and only if $L = k$ because the center of $L$ is $L$ itself.
	\end{enumerate}
\end{exmps}
\begin{prop}
	Suppose that $A$ and $B$ $k$-algebras with $B$ central simple over $k$. Then, the two-sided ideals of $A \otimes_k B$ are precisely the ones of the form $I \otimes_k B$ where $I$ is a two-sided ideal of $A$.
\end{prop}
\begin{proof}
	Clearly, $I \otimes_k B$ is a two-sided ideal if $I$ is a two-sided ideal of $A$. Conversely, let $J$ be a two-sided ideal of $A \otimes_k B$ and $I = \{a \in A \mid a \otimes 1 \in J\}$, it is easy to check that $I$ is a two-sided ideal of $A$ and $I \otimes_k B \subseteq J$,\footnote{The first part is true because \[ar \in A \Leftrightarrow ar \otimes 1 \in J \Leftrightarrow (a\otimes 1)(r \otimes 1) \in J,\]and similarly for right multiplication. The second part is true because if $a \otimes 1 \in J$, then $a \otimes b \in J$ for any $b \in B$.} we claim this is in fact an equality.
	
	Let $x_1, \dots, x_n$ be a $k$-basis for $A$ chosen so that $x_1, \dots, x_m$ is a $k$-basis for $I$ and $x_{m+1}, \dots, x_n$ is the completion of the basis. Then, we can write
	\[A \otimes_k B = \{\sum_{i=1}^n x_i \otimes b_i \mid b_1, \dots, b_n \in B\}.\]
	We need to show if $\sum_{i=1}^n x_i \otimes b_i \in J$, then $b_i = 0$ for all $i > m$. Equivalently, we need to show if $\sum_{i = m+1}^n x_i \otimes b_i \in J$, then $b_i = 0$.\footnote{We already know $\sum_{i=1}^m x_i \otimes b_i \in J$, so we can subtract by it.} Let $t$ be minimal for the property that there exists a subset $\{y_1, \dots, y_t\} \subseteq \{x_{m+1}, \dots, x_n\}$ with $0 \neq \sum y_i \otimes b_i \in J$. By minimality, all the $b_i$'s are non-zero. Consider the set $\{b_1 \in B \mid \exists \sum y_i \otimes b_i \in J\}$. It is clearly a two-sided ideal which is non-zero by definition of $t$, but since $B$ is simple, it must be all of $B$, in particular it contains one. Therefore, there exists $\beta = y_1 \otimes 1 + \sum y_i \otimes b_i \in J$. For any $b \in B$, since $J$ is a two-sided ideal, we have \[J \ni (1 \otimes b)\beta - \beta(1 \otimes b) = y_2 \otimes bb_2-b_2b + \cdots + y_t \otimes bb_t - b_tb.\]
	The minimality of $t$ implies that $bb_j - b_jb = 0$ for $j = 2,...,t$. In other words, $b_2,..., b_t$ are in the center in $B$, namely in $k$. Finally, we set $y_i' = y_ib_i$ to find 
	\[\sum_{i=1}^t y_i' \otimes 1 \in J,\]
	thus $\sum y_i' \in I$ and this contradicts our construction of the $y_i$'s.\footnote{They were a subset of $\{x_{m+1}, \dots, x_n\}$ of which, none are in $I$.}
\end{proof}
\begin{cor}\label{tensorsimple}
	If $A$ is simple over $k$ and $B$ is central simple over $k$, then $A \otimes_k B$ is also simple over $k$.\footnote{If $A$ is simple, then the only two-sided ideals of $A \otimes_k B$ are $0 \otimes_k B = 0$ and $A \otimes_k B$, so there is no non-trivial two-sided ideal.}
\end{cor}
\begin{prop}
	If $k$ has characteristic $0$, $A$ is semisimple over $k$ and $F$ is a finite separable extension of $k$, then $A \otimes_k F$ is also semisimple over $k$.
\end{prop}
\begin{proof}
	By Wedderburn's classification, we can write $A$ as direct sum of simple $k$-algebras, so we will first prove the case where $A$ is simple. Let $Z$ be the center of $A$, it is clear that $A$ is a central simple algebra over $Z$ and we have\footnote{In facts \ref{factstensor}, we mentioned that the tensor product was associative when done over the same algebra, but the version of associativity used here is slightly more complex and uses the fact that $Z$ is a $(Z,k)$-bimodule.}
	\[A \otimes_k F = (A \otimes_Z Z) \otimes_k F= A \otimes_Z (Z \otimes_k F) = A \otimes_Z (K_1 \oplus \cdots \oplus K_s),\]
	where $K_i$'s are finite extensions of $Z$ that arise from proposition \ref{finprodext}.\footnote{To use this result, we only need to argue that $Z$ is a finite field extension because any field extensions over a finite field of characteristic $0$ is separable. Since $A$ is simple and finite dimensional, we can write it as $M_n(D)$ for a finite dimensional division $k$-algebra $D$. Then, it is easy to see the center is the center of $D$ and hence is a finite field extension of $k$.} The tensor product also distributes so we obtain $A \otimes_Z K_1 \oplus \cdots \oplus A \otimes_Z K_s$. Since each $A \otimes_Z K_i$ is simple because $A$ is central simple over $Z$ and $K_j$ is simple over $Z$, we conclude that $A \otimes_k F$ is semisimple.
\end{proof}
\begin{defn}
	If $B$ is a subalgebra of $A$, then the centralizer of $B$ in $A$, denoted $\mZ_A(B)$, is the set of all elements of $A$ that commute with all of $B$. For instance, $\mZ(A) := \mZ_A(A)$ is the center of $A$. The centralizer is in fact a subalgebra.
\end{defn}
\begin{lem}\label{centeroftensor}
	Let $A$ and $B$ be $k$-algebras with $B$ central, then 
	\begin{enumerate}[i.]
		\item $\mZ_{A \otimes_k B}(1 \otimes_k B) = A \otimes_k 1$, and
		\item $\mZ(A \otimes_k B) =\mZ(A) \otimes_k 1$.
	\end{enumerate}
\end{lem}
\begin{proof}
	\begin{enumerate}[i.]
		\item ($\supseteq$) is trivial.\footnote{For any $a \in A$ and $b,b' \in B$, \[(a\otimes b)(1 \otimes b') = (1 \otimes b')(a\otimes b)\] if and only if $bb' = b'b$, so $a \otimes b$ is in the center of $1 \otimes_k B$ if and only if $b$ is in the center of $B$, namely, $b \in k$. This implies that \[a\otimes b = ab\otimes 1 \in A \otimes_k 1.\]}
		
		($\subseteq$) Let $a_1, \dots, a_n$ be a basis for $A/k$, and recall that every element of $A \otimes_k B$ can be written uniquely in the form $a_1 \otimes b_1 + \cdots + a_n \otimes b_n$ for some $b_1, \dots, b_n \in B$. In other words, after having chosen a basis for $A$, $A \otimes_k B$ can be identified by $B^n$.\footnote{Addition and multiplication behave just like in $B^n$:
		\begin{align*}
			\sum_i a_i \otimes b_i + \sum_i a_i \otimes b'_i &= \sum_i a_i \otimes (b_i + b'_i),\text{and}\\
			b\bra{\sum_i a_i \otimes b_i} &= (1\otimes b)\bra{\sum_i a_i \otimes b_i}\\
			&= \sum_i a_i \otimes bb_i.
		\end{align*}} Let $\alpha \in \mZ_{A\otimes_k B}(1 \otimes_k B)$, then $(1 \otimes b)\alpha = \alpha(1\otimes b)$ and we can write 
		\[a_1 \otimes bb_1 + \cdots + a_n \otimes bb_n = a_1 \otimes b_1b + \cdots + a_n \otimes b_nb,\]
		where $\alpha = a_1 \otimes b_1 + \cdots + a_n \otimes b_n$. By uniqueness, we get $b_ib = b_ib$ for any $b$, so each $b_i$ is in the center of $B$, namely, in $k$. Then, we have 
		\[\alpha = a_1b_1 \otimes 1 + \cdots + a_nb_n \otimes 1 = \bra{\sum_i a_ib_i}\otimes 1 \in A \otimes_k 1.\]
		
		\item Part i implies that the center is contained in $A \otimes_k 1$ and it is obvious that $(a' \otimes 1)\alpha = \alpha(a' \otimes 1)$ for all $\alpha$'s if and only if $a \in \mZ(A)$ (it is even enough to take $\alpha \in A \otimes_k 1$).
	\end{enumerate}
\end{proof}

\begin{cor}\label{tensorcentsimple}
	If $A$ and $B$ are central simple algebras, then $A \otimes_k B$ is central simple.\footnote{Simplicity comes from corollary \ref{tensorsimple} and according to the previous theorem, the center of $A \otimes_k B$ will be $1 \otimes 1 = k$.}
\end{cor}
\begin{exmp}\label{AtensAop}
	If $A$ is central simple over $k$, then certainly $\op{A}$ is also central simple\footnote{The center and two-sided ideals of the opposite alebra are the exactly the same.} and in fact $A \otimes_k \op{A} \cong M_n(k)$. More precisely, $A \otimes_k \op{A}$ is canonically identified with $\End_k(V_A)$ by sending $a \otimes a'$ to the linear map $v \mapsto ava'$.\footnote{By tedious verifications, one can show that this is a $k$-algebra homomorphism.} This map is injective because its kernel is a two-sided ideal and $A \otimes_k \op{A}$ is simple, surjectivity follows from counting dimensions ($\dim_k(A)^2 = \dim_k(A)^2$).
\end{exmp}
\begin{cor}
	If $A$ is central simple over $k$ and $L$ is a field extension of $k$, then $A\otimes_k L$ is central simple over $L$.
\end{cor}
\begin{proof}
	We already know it is simple by corollary \ref{tensorsimple}. Moreover, lemma \ref{centeroftensor} yields \[\mZ(A \otimes_k L) = 1 \otimes_k \mZ(L) = 1 \otimes_k L = L.\]
\end{proof}
\begin{prop}
	If $A/k$ is central simple, then $\dim_k(A)$ is a square.
\end{prop}
\begin{proof}
	Let $\bar{k}$ be the algebraic closure of $k$, then the previous corollary says that $A \otimes_k \bar{k}$ is a central simple algebra over $\bar{k}$ and from Wedderburn's classification, we know $A \otimes_k \bar{k}= M_n(D)$ where $D$ is a finite dimensional division $\bar{k}$-algebra. In addition, note that such a $D$ must be equal to $\bar{k}$\footnote{Since $D$ is a finite extension, it is algebraic and hence cannot be bigger than $\bar{k}$.} an therefore $\dim_{\bar{k}}(A \otimes_k \bar{k}) = \dim_{\bar{k}}(M_n(k)) = n^2$. But, we also have that this is equal to $\dim_k(A)$ (why?) %TODO: complete.
\end{proof}
\begin{cor}
	If $D$ is a finite dimensional central division algebra over $k$, then $\dim_k(D) = n^2$ for some $n$.\footnote{This is a trivial application of the proposition using the fact that division algebras are simple. Note that a particular application of this corollary is the fact that there are no central division algebras of dimension three over $\R$. In fact, we will later see Frobenius' theorem that shows there are only two central division algebras over $\R$ up to isomorphism.}
\end{cor}
\begin{thm}[Skolem-Noether]
	Let $A$ be a central simple algebra over $k$ and $B$ be a simple subalgebra of $A$. If $f,g: B \rightarrow A$ are $k$-algebra homomorphisms, then there exists $x \in A^{\times}$ such that $f(b) = xg(b)x^{-1}$ for all $b \in B$.
\end{thm}
\begin{proof}
	Consider first the case where $A = M_n(k)$. Notice that $f:B\rightarrow A$ endows $V_f := k^n$ with the structure of a left $B$-module, namely, for $v \in k^n$ and $b \in B$, $b \ast v = f(b) \cdot v \in k^n$ (with matrix vector multiplication). Because $B$ is simple, there is a unique simple left $B$-module $V_B$ up to isomorphism. \footnote{Why is this true? Is this an app of (Jordan-Holder)? $V_B$ is minimal left ideal unique up to isomorphism.} In particular, $V_f \cong V_B^t$ for some $t \in \N$ (as left modules), and likewise, $V_g$ is also isomorphic to $V_B^t$ as left $B$-modules.\footnote{They are of the same dimension, so they are isomorphic.} Thus, there exists an isomorphism of left $B$-modules $\phi: V_f \rightarrow V_g$, and it must satisfy $\phi(f(b)\cdot v) = g(b) \cdot \phi(v)$. Equivalently, for all $v \in V_g$, $f(b) = \phi^{-1}g(b)\phi$.\footnote{To conclude that $\phi \in A = M_n(k)$, we have to show $\phi$ is a $k$-linear map from $V_f = k^n$ to $V_g = k^n$. To see this, first note that $\phi(v+w) = \phi(v)+\phi(w)$ follows from the properties of a $B$-module homomorphism. Second, the $k$-algebra homomorphism properties imply that for any $\lambda \in k$,  \[f(\lambda) = g(\lambda)= \text{diag}(k,\cdots, k).\]
	Therefore, $\phi$ is an element of $M_n(k)$.}
	
	For the more general case, we consider the maps
	\[f\otimes 1, g\otimes 1: B\otimes_k \op{A} \rightarrow A \otimes_k \op{A} = M_n(k).\]
	Since $\op{A}$ is CSA and $B$ is simple, $B\otimes_k \op{A}$ is simple by corollary \ref{tensorsimple}. Then, by applying our first case, we find an element $X \in \bra{A\otimes_k \op{A}}^{\times}$ such that $f \otimes 1 = X(g \otimes 1)X^{-1}$. More explicitly, for all $b \otimes a' \in B \otimes \op{A}$, we have 
	\[f(b) \otimes a' = X(g(b) \otimes a') X^{-1}.\]
	
	Setting $b = 1$, we observe that $(1\otimes a')X = X(1 \otimes a')$ for all $a' \in \op{A}$, thus $X \in \mZ_{B \otimes_k \op{A}}(1 \otimes_k \op{A})$. We can conclude from lemma \ref{centeroftensor} that $X \in B \otimes 1$. Thus, our more general equation becomes 
	\[f(b) \otimes a' = (x\otimes 1)(g(b) \otimes a')(x^{-1} \otimes 1) = (xg(b)x^{-1} \otimes a'),\]
	and we conclude $f(b) = xg(b)x^{-1}$.
\end{proof}
\begin{cor}
	If $A$ is a CSA over $k$, then every non-zero $k$-algebra homomorphism $\phi: A \rightarrow A$ is inner, i.e.: there exists $x \in A^{\times}$ such that $\phi(a) = xax^{-1}$.\footnote{Apply the Skolem-Noether theorem with $B= A$, $f = \phi$ and $g = 1$.}
\end{cor}
\begin{exmp}
	As a further corollary, we obtain that any element of $\aut(M_n(k))$ is realized by a conjugation of an invertible matrix. In other words, any automorphism of $M_n(k)$ is a change of basis.
\end{exmp}
\marginnote{\begin{rem}
	Recall that any simple $k$-algebra $A$ will be a CSA over its center $\mZ(A)$, thus, in the corollary above, we can drop the centrality assumption and instead assume $\phi$ is a $\mZ(A)$-algebra automorphism.
\end{rem}}
\begin{lem}
	Let $B \subseteq A$ and $B' \subseteq A'$ where $A$ and $A'$ are CSA over $k$ and $B$ and $B'$ are simple over $k$ and let $C = \mZ_A(B)$ and $C' = \mZ_{A'}(B')$. Then, \[\mZ_{A \otimes_k A'}(B \otimes_k B') = C \otimes_k C'.\]
\end{lem}
\begin{proof}
	Clearly $C \otimes_k C'$ commutes with $B \otimes_k B'$, therefore $\supseteq$ is trivial. Moreover, obvious properties of centers yield \[\mZ_{A \otimes_k A'}(B \otimes_k B') \subseteq_k \mZ_{A \otimes_k A'}(B\otimes_k 1) \cap \mZ_{A \otimes_k A'}(1 \otimes_k B').\]
	
	It is easy to check that the R.H.S. is equal to $C \otimes_k A' \cap A \otimes_k C'$,\footnote{For any $a\otimes a' \in A \otimes_k A'$, $(ab\otimes a') = (ba\otimes a')$ if and only if $a \in \mZ_A(B)$ and similarly for the symmetric case.} and furthermore the latter is equal to $C \otimes C'$ because if $c \otimes a' = a \otimes c'$ is in the intersection, then it commutes with $B \otimes_k 1$ and $1 \otimes_k B$, so we infer $a \in C$ and $a' \in C'$.
\end{proof}
\begin{lem}
	Let $B$ be a simple $k$-algebra and let $A = \End_k(V_B)$, where $V_B$ is $B$ viewed as a $k$-vector space.\footnote{There are two $k$-algebra homomorphism $\ell: B \rightarrow \End_k(V_B) = b \mapsto b(-)$ and $r: \op{B} \rightarrow \End_k(V_B) = (-)b$.} Then, $\mZ_A(\ell(B)) = r(\op{B})$.
\end{lem}
\begin{proof}
	We can think of $\mZ_{\End_k(V_B)}(\ell(B))$ as endomorphisms of $V_B$ that commute with the left action of $B$, namely, the left $B$-module endomorphisms of $V_B$. Let $\phi$ be an element of $\End_B(V_B)$, then $\phi(b) = b\cdot \phi(1)$, hence $\phi$ is the image of $\phi(1)$ under $r$. We get that $r: \op{B} \rightarrow \End_B(V_B)$ is surjective, it is injective because $\op{B}$ is simple.\footnote{Checking that this map preserves the $k$-algebra structure is left as a simple exercise.}
\end{proof}
\begin{thm}[Double centralizer]\label{doublecent}
	Let $A$ be a CSA over $k$, $B$ a simple subalgebra and $C = \mZ_A(B)$, then \begin{enumerate}
		\item $C$ is simple.
		\item $\dim_k(B) \dim_k(C) = \dim_k(A)$.
		\item $B = \mZ_A(C)$.
	\end{enumerate}
\end{thm}
\begin{proof}[Proof]
	Consider the $k$-algebra $\End_k(V_B)$, it is also central simple,\footnote{Since $B$ is finite dimensional, we can pick a basis for $B$ over $k$ and identify $\End_k(V_B)$ with $M_{\dim_k(B)}(k)$, we have already seen why the latter is a CSA over $k$.} hence $A \otimes_k \End_k(V_B)$ is a central simple $k$-algebra. Consider the maps $f,g: B \rightarrow A \otimes_k \End_k(V_B)$ defined by $f(b) = b \otimes 1$ and $g(b) = 1 \otimes \ell(b)$. By the Skolem-Noether theorem, these two maps can be conjugated into each other by an invertible element of $A \otimes_k \End_k(V_B)$, namely, there exists $x \in (A \otimes_k \End_k(V_B))^{\times}$ such that $f = xgx^{-1}$. In particular, $f(B) = xg(B)x^{-1} \Leftrightarrow B \otimes 1 = x(1 \otimes \ell(B))x^{-1}$. Then, it is obvious that \[\mZ_{A \otimes_k \End_k(V_B)}(B \otimes_k 1) = x\mZ_{A \otimes_k \End_k(V_B)}(1 \otimes_k \ell(B))x^{-1}.\]	
	Using the previous lemmas, we can compute that the L.H.S. is $\mZ_A(B) \otimes_k \End_k(V_B)$ and the R.H.S. is a conjugate of $A \otimes_k r(\op{B})$, we infer that\footnote{We use the fact that conjugation by $x$ is a $k$-algebra homomorphism.}
	\[C \otimes_k \End_k(V_B) \cong A \otimes_k r(B^{op}), \quad \text{as $k$-algebras.}\]
	
	We are now ready to prove each part of the statement using this isomorphism.
	\begin{enumerate}
		\item Because the R.H.S. is simple, the L.H.S. also is and it is clear that $C$ must also be simple as any non-trivial two-sided ideal of $C$ would give rise to a non-trivial two-sided ideal of $C \otimes_k \End_k(V_B)$.
		\item By computing the dimensions of both sides, we get 
		\[\dim_k(C) \cdot \dim_k(B)^2 = \dim_k(A) \cdot \dim_k(B).\]The result then follows by dividing by $\dim_k(B)$.
		\item Clearly $B$ centralizes $C$, so $B\subseteq \mZ_A(C)$ and part two of the theorem applied to $C$ implies that $\dim_k(C) \dim_k(\mZ_A(C)) = \dim_k(A)$ and, so $\dim_k(B) = \dim_k(\mZ_A(C))$ and we conclude $B$ must equal $\mZ_A(C)$.
	\end{enumerate}
\end{proof}
\begin{exmps}
	\begin{enumerate}
		\item[]
		\item Let $A = M_n(D)$ where $D$ is a central division algebra over $k$ and $B = M_n(k)$. Then we claim $\mZ_A(B)= D\cdot I_n$ (the scalar diagonal matrices). Clearly $DI_n \subseteq \mZ_A(B)$ and the double-centralizer theorem implies $DI_n= \mZ_A(B)$ by dimensionality:  $\dim_k(D) \dim_k(M_n(k)) = \dim_k(M_n(D))$.
		\item Let $A = M_n(k)$ and $B$ a field extension of $k$ of degree $n$. We can view $B$ as a subalgebra of $M_n(k)$ by choosing a basis $e_1,\dots, e_n$ of $B$ over $k$.\footnote{There is a natural map $B \mapsto \End_k(B) = M_n(k)$ that sends $b \in B$ to the left multiplication by $b$.}. This time, $\mZ_A(B)$ clearly contains $B$ because $B$ is a field and commutes. The double centralizer yields $B = \mZ_A(B)$ by dimensionality again.
		\item %TODO: put new exercise from the end.
	\end{enumerate}
\end{exmps}
There are two important complementary settings where theorem \ref{doublecent} is used.
\begin{cor}
	In the same setting as the theorem, if $B$ is central simple over $k$, then $B \otimes_k C= A$.
\end{cor}
\begin{proof}
	Since $B$ is central simple and $C$ is simple, $B \otimes_k \mZ_A(B)$ is simple and there is a natural homomorphism $B \otimes_k C \rightarrow A$ sending $b \otimes c$ to $bc$.\footnote{Note that this is well-defined precisely because $C$ commutes with $B$. It is a module homomorphism by $k$-bilinearity and algebra homomorphism by commutation of $B$ and $C$.} Because the L.H.S. is simple, the map is injective and by dimension counting (recall part ii of the double centralizer), it must be an isomorphism.
\end{proof}
\begin{cor}\label{cormaxabel}
	In the same setting as the theorem, if $B$ is a maximal abelian subfield\footnote{We mean a subalgebra that is a field.} of $A$, then $C = B$ and $\dim_k(B)^2 = \dim_k(A)$.\footnote{This is one way to see that the dimension of a CSA is a square, but it requires the existence of a maximal abelian subfield. Finite dimensionality guarantees that, but it is not true in the general case.}
\end{cor}
\begin{rem}
	Note that the last corollary does not necessarily apply here. For instance if $A$ is non-commutative, we cannot have $B\otimes_k B = A$ because the L.H.S. is commutative.
\end{rem}
\begin{proof}
	It is obvious that $C$ contains $B$ because $B$ is commutative. Moreover, if $C$ contains an element not in $B$, adjoining it to $B$ yields a bigger field contradicting the maximality of $B$, thus we conclude $B = C$. The result for the dimension is a trivial application of the double centralizer.\footnote{After noting that $B$ is simple because it is a field.}
\end{proof}
\begin{exmp}
	Let $A = M_n(k)$, it contains many subfields of degree $n$ and we claim that if $E$ is a field of degree $n$ over $k$, then $E$ is isomorphic to a subfield of $A$. To see this, fix a basis for $E$, and recall that $\End_k(E) \cong M_n(k)$, so the embedding\footnote{It sends $e$ to the left multiplication by $e$.} $\ell: E \rightarrow \End_k(E)$ extends to an embedding into $M_n(k)$.
\end{exmp}
\begin{cor}
	If $D$ is a division algebra over $k$, then any maximal subfield $F$ of $D$ satisfies $[F:k]^2 = [D:k]$.
\end{cor}
\begin{cor}\label{cormaxsubfield}
	Let $A$ be a CSA over $k$ and $K$ be a subfield, then the following are equivalent:
	\begin{enumerate}
		\item $K = \mZ_A(K)$.
		\item $[K:k]^2 = \dim_k(A)$.
		\item $K$ is a maximal commutative subalgebra of $A$.
	\end{enumerate}
\end{cor}
\begin{proof}
	Easy application of the earlier results.
\end{proof}

\subsection{Classification of Central Simple Algebras}
As a first goal, we would like to classify all the central division algebras over a given field $k$ (up to isomorphism). We will denote $DA(k)$ to be the set of central division $k$-algebras modulo the isomorphism relation and denote $CSA(k)$ to be the set of central simple $k$-algebras modulo the isomorphism relation. Note that the former is contained in latter and by Wedderburn's classification we can write \[CSA(k) = \{M_n(D) \mid D \in DA(k), n\geq 1\}.\]
We first consider the case when $k$ is finite.

A result in field theory states that finite fields are completely determined, up to isomorphism, by their cardinality. More precisely, if $n \in \N$, $p$ is prime and $q = p^n$, then there exists a unique field $\F_q$ of size $q$. In particular finite field extensions of finite fields are isomorphic if and only if they have the same dimension over the base field. To classify finite division ring, we need a general group theoretic lemma.
\begin{lem}
	If $G$ is a finite group and $H$ is a proper subgroup, then $G \neq \cup_{x \in G} xHx^{-1}$.
\end{lem}
\begin{proof}
	Suppose this were true, then we could also range the union over representatives of $G/H$ and preserve equality.\footnote{This is because if $xH = yH$, then $xHx^{-1} = yHy^{-1}$.} This union involves $|G|/|H|$ subsets of size $|H|$, but each subset is not disjoint (they all contain the identity), hence the size of the union is strictly less than $|G|$.
\end{proof}
\begin{thm}[Wedderburn]\label{thmwedd}
	Any finite division ring is commutative, i.e.: it is a field.
\end{thm}
\begin{proof}
	Let $D$ be a finite division ring and $k = \mZ(D)$ its center. Since $k$ is a commutative division ring, it is a field and by finiteness, it is isomorphic to $\F_q$ for $q$ a power of a prime. Let $K$ be a maximal commutative $k$-subalgebra of $D$,\footnote{The existence of $K$ is guaranteed by the finiteness of $D$.} it is a field extension of $k$. By corollary \ref{cormaxabel}, we have $\dim_k(K)^2 = \dim_k(D)$ and all other maximal commutative subalgebras must be isomorphic to $K$\footnote{The equation for dimension yields that all such subalgebras have the same dimension as $K$ and we saw that this imply they are isomorphic when over finite fields.} and hence conjugate (by Skolem-Noether). Therefore, since every $\alpha \in D$ generates a commutative algebra $k[\alpha]$ over $k$, we infer that $k[\alpha]$ is contained in a maximal commutative algebra of the form $xKx^{-1}$ for some $x \in D^{\times}$. We conclude that  
	\[D = \bigcup_{x \in D^{\times}} xKx^{-1} \text{ and furhtermore } D^{\times} = \bigcup_{x \in D^{\times}} xK^{\times}x^{-1}.\]
	This contradicts the previous lemma if $K^{\times}$ is a proper subset of $D^{\times}$, so we conclude $D = K$ and hence $D$ is a field.
\end{proof}
\begin{cor}
	If $k$ is a finite field and $A$ is a finite dimensional CSA over $k$, then $A \cong M_n(k)$ for some $n$.\footnote{Might also be true if $A$ is not finite dimensional.}
\end{cor}
Next, we study the case when $k = \R$.
\begin{thm}[Frobenius]\label{frobenius}
	If $D$ is a division algebra over $\R$, then $D$ is either isomorphic to $\R$, $\C$ or $\bH$.
\end{thm}
\begin{proof}
	If $D$ is commutative, then it follows from the fundamental theorem of algebra that $D$ is either $\R$ or $\C$.\footnote{Recall that we are only considering finite dimensional algebras, and along with commutativity, it implies $D$ is an algebraic extension of $\R$.}
	
	Otherwise, the center of $D$ has to be equal to $\R$ since there are no finite dimensional division algebra over $\C$.\footnote{Recall the argument in corollary \ref{cordivalgclosed}.} Let $E$ be a maximal commutative subalgebra of $D$, then $E$ is a non-trivial field extension of $\R$, so $E = \C$ and by corollary \ref{cormaxabel}, we have $\dim_{\R}(D) = [E:\R]^2 = 4$.
	
	Observe that $D$ is a two dimensional $\C$ vector space under left multiplication and for any $w \in D-\C$, $\{1, w\}$ forms a $\C$-basis for $D$. We proceed to choose a particular $w$ that will help us understand the multiplication in $D$. Consider the maps $f,g: \C \rightarrow D$ where $f = \text{id}$\footnote{More precisely, $f$ is the isomorphism $\C \cong E \subseteq D$.} and $g = z \mapsto f(\bar{z})$. By Skolem-Noether, $\exists w \in D^{\times}$ such that $wzw^{-1} = \bar{z}$ for any $z \in \C$ and we infer that $w$ does not commute with $\C$, hence it is in $D-\C$ and is independent of $1$. Thus, we got a basis and\footnote{The equalities are as $\C$ vector spaces.}
	\[D = \C 1 + \C w = \{z_1 +z_2w \mid z_1, z_2 \in \C\}.\]
	Notice that since $wz = \bar{z}w$ for any $z$, we have
	\[w^2z = w\bar{z}w = zw^2,\] therefore $w^2$ commutes with $\C$. We infer that $w^2$ commutes with all of $D$,\footnote{It obviously commutes with $w$.} namely, $w^2 \in Z(D) = \R$. Furthermore, we claim that $w^2 < 0$. Suppose otherwise, then there would exists $r \in \R$ such that $r^2 = w^2$, or equivalently, $(w-r)(w+r) = 0$. This would imply $D$ has zero-divisors because $D-\C \ni w \neq r,-r$ and contradict the fact that $D$ is an integral domain.
	
	Finally, replacing $w$ by $\lambda w$ with $\lambda \in \R$ will not change its properties (because $\R$ commutes with $D$), so we can assume $w^2 = -1$. We conclude that the multiplication in $D$ is now completely determined: for any $z_1,z_2,y_1,y_2 \in \C$,
	\[(z_1 + z_2w)(y_1+y_2w) = z_1y_1 + z_1y_2w + z_2\bar{y}_1w + z_2\bar{y}_2w^2 = z_1y_1 - z_2\bar{y}_2 + (z_1y_2 + z_2\bar{y}_1)w.\]
	Hence, there can only be one non-commutative division algebra over $\R$.\footnote{Indeed, if $D$ and $D'$ are two non-commutative division $\R$-algebras, then we find the distinguished elements $w \in D$ and $w' \in D'$ and what we have show is that the map sending $1_D$ to $1_{D'}$ and $w$ to $w'$ (extend it with $\C$-linearity) is an $\R$-algebra isomorphism.}
\end{proof}

Although the two previous settings ($k$ finite and $k= \R$) resulted in a simple classification, this is not always the case and we now turn to the more general question.

Recall from corollary \ref{tensorcentsimple} that $CSA(k)$ is closed under the tensor product over $k$, and this operation is associative and commutative. Moreover, since tensoring with $k$ does not change the structure, i.e.: $A \otimes_k k \cong A$, we can see $CSA(k)$ as a monoid under the operation $\otimes_k$.

\begin{rem}
	The set $T(k) = \{k, M_2(k), M_3(k), \dots, \}$ is submonoid of $(CSA(k), \otimes_k)$ and is isomorphic to $(\N^*, \cdot)$. We use it in the followin definition.
\end{rem}

\begin{defn}[Brauer group]
	We say that two elements $A_1$ and $A_2$ of $CSA(k)$ are Brauer equivalent if $\exists n_1, n_2 \geq 1$ such that $M_{n_1}(A_1) \cong M_{n_2}(A_2)$, or equivalently $A_1 \otimes M_{n_1}(k) \cong A_2 \otimes M_{n_2}(k)$. The Brauer group of $k$, denoted $\Br(k)$ is the set of Brauer equivalence classes in $CSA(k)$, i.e.: $\Br(k) \cong CSA(k)/T(k)$.
\end{defn}
\begin{rem}
	Wedderburn's classification implies that the following composite is a bijection\footnote{Surjectivity is obvious because every class contains at least one CSA which can be written as $M_n(D)$ for some central division algebra and it follows that this also class contains $D$. The uniqueness of the decomposition implies that if $D$ and $D'$ are central division $k$-algebras such that $M_{n}(D) = M_{m}(D')$ for some $n,m \in \N$, then $D = D'$ and $n = m$, so the map is injective.}:
	\[DA(k) \hookrightarrow CSA(k) \rightarrow \Br(k) = D \mapsto D \mapsto [D] = \{M_n(D) \mid n \geq 1\}.\]
	In other words, every Brauer equivalence class has a unique central division algebra.
\end{rem}

\begin{exmps}
	\begin{enumerate}
		\item If $k$ is finite, then theorem \ref{thmwedd} shows $\Br(k) = \{k\}$.
		\item Frobenius' theorem implies $\Br(\R) = \{\R, \bH\}$. Moreover, we can infer from the group structure that $[\bH]\otimes [\bH]  =[\R]$ and this implies (by comparing dimensions) that $\bH \otimes_{\R} \bH \cong M_4(\R)$.
		\item If $k$ is algebraically closed, $\Br(k) =\{k\}$.\footnote{Recall corollary \ref{cordivalgclosed}.}
		\item Compared to the previous examples $\Br(\Q)$ is way more complex, it is infinite and in fact not even finitely generated.
	\end{enumerate}
\end{exmps}
The next proposition justifies the name "Brauer group".
\begin{prop}
	The operation $\otimes_k$ makes $\Br(k)$ into an abelian group.
\end{prop}
\begin{proof}
	We just need to argue that any element $[A] \in \Br(k)$ has an inverse.\footnote{Because $\Br(k)$ is already the quotient of a commutative monoid, hence a commutative monoid.}
	
	We claim that $[\op{A}]$ is that inverse. Recall from example \ref{AtensAop} that $A \otimes_k \op{A} \cong \End_k(A) \cong M_n(k)$ and this clearly implies $[A]\otimes [\op{A}] = [k]$.
\end{proof}

\begin{prop}
	The assignment $k \mapsto \Br(k)$ is a functor $\textbf{Fields} \rightsquigarrow \textbf{AbGrps}$.
\end{prop}
\begin{proof}
	 More precisely, if $k \rightarrow K$ is a morphism of fields, then we get a map $\Br(k) \rightarrow \Br(K)$ sending $[A]$ to $[A \otimes_k K]$. %TODO: complete.
\end{proof}

\begin{defn}
	The relative Brauer group of an extension $K/k$ is the kernel of the map defined above. We denote it $\Br(K/k)$.
\end{defn}
\begin{prop}\label{brauerfiniteext} The Brauer group of $k$ is the union of the relative Brauer groups of all finite extensions $K>k$, i.e.:
	$\Br(k) = \bigcup_{[K:k] < \infty} \Br(K/k)$.
\end{prop}
\begin{proof}
	Given $X$ in $\Br(k)$, let $D$ be the unique central division algebra in $X$. We want to find a finite extension $K/k$ such that $D \in \Br(K/k)$, that is $D \otimes_k K \cong M_n(K)$. If $K$ is any maximal subfield of $D$, we know that $n^2 = \dim_kK^2 = \dim_kD$.\footnote{Since $D$ is a division algebra, any commutative subalgebra is a subfield. Hence $K$ is a maximal commutative subalgebra and we can use corollary \ref{cormaxsubfield}.} We claim that for such a $K$, $D \otimes_k K \cong M_n(K)$.
	
	View $D$ as a $K$-vector space $V_D$ via right multiplication, it has dimension $n$, and observe that $D$ acts on $V_D$ $K$-linearly by left multiplication. Furthermore, since both actions clearly commute and right and left multiplication by $k$ coincide, we obtain a $k$-algebra homomorphism\footnote{Checking the properties is left as an exercise.} $D \otimes_k K \rightarrow \End_K(V_D) = M_n(K)$  that sends $d \otimes \lambda$ to $v \mapsto dv\lambda$. It is enough then to observe that the dimensions of both sides is $n^3$ because the L.H.S. is simple, hence the map must be an isomorphism.
\end{proof}

\begin{defn}
	A field $K/k$ is said to split a CSA $A$ if $A \otimes_k K \cong M_n(K)$.
\end{defn}
\begin{exmp}
	Let $k = \Q$ and $D = \Q(i,j,k)$.\footnote{The relations between $i,j,k$ are as in $\bH$ but we restrict the coefficients to lie in $\Q$.} Every maximal subfield is quadratic and it is easy to construct them because for any $x \in D-\Q$, $\Q(x)$ is a field of degree two over $k$. For instance, it is easy to see that $\Q(i) \cong \Q(j) \cong \Q(k)$ because $i$, $j$ and $k$ all satisfy the same minimal polynomial, namely $x^2+1$.
	
	A less obvious fact is that $\Q(i+j) \cong \Q(\sqrt{-2})$. To see this, we note that\footnote{Recall example \ref{normandtracequaternion} to compute the norms and traces.} \[\Nm(i+j) = \Nm(\sqrt{-2}) = -2 \text{ and } \Tr(i+j) = \Tr(\sqrt{-2}) = 0.\] 
			
	%TODO:Find more general form.
	We can infer that no field of the form $\Q(\sqrt{d})$ for $d > 0$ is in $\Q(i,j,k)$. Moreover $\R$ cannot arise because it is infinite dimensional, thus $D \otimes_{\Q} \R \cong \bH$ is not isomorphic to $M_2(\R)$.\footnote{But we already knew that because $\bH$ is a division algebra and $M_2(\R)$ is not.}
	
	The proof of proposition \ref{brauerfiniteext} implies that all these fields are examples of splitting fields for $D$.	
\end{exmp}




The next (and last) theorem of this class is motivated by the study of $k$-linear representations. An important result in representation theory is the decomposition of the group ring for a finite group $G$. In the well-studied case $k = \C$, we have
\[\C[G] \cong \oplus_{i=1}^t M_{d_i}(\C),\]
where $t$ is the number of distinct irreducible representations of $G$ and $d_1, \dots, d_t$ are their dimensions. We can read off a lot of information from this.
\begin{enumerate}
	\item By comparing the dimensions, we get $|G| = d_1^2 + \cdots + d_t^2$.
	\item The number of conjugacy classes of $G$ is $t$.
	\begin{proof}
		We will take the center on both sides. On the R.H.S. we obtain a copy of $\C$ for each summand and hence the center is $\C^t$. On the L.H.S., we note that the center is precisely the elements which have the same coefficients in front of elements of the same conjugacy class. Indeed, if $a \in \mZ(\C[G])$, we can decompose its sum in conjugacy classes and write
		\[a = \sum_{X \in \conjc(G)} \sum_{g \in X} a_g\cdot g.\]
		Then, since for any $h \in G$, $hah^{-1} = a$, we have
		\[a = \sum_{X \in \conjc(G)} \sum_{g \in X} a_g\cdot hgh^{-1},\]
		and we infer\footnote{Also using the fact that for any $g_1,g_2 \in G$, there exists $h \in G$ such that $hg_1h^{-1} = g_2$ if and only if $g_1$ and $g_2$ are in the same conjugacy class.} that the coefficients of all elements of a single conjugacy class must be the same. It is obvious that the dimension of $\mZ(\C[G])$ ($t$) is the number of conjugacy classes.
	\end{proof}
\end{enumerate}

In order to attain more generality, we can ask what happens if $\C$ is replaced by a field $k$ with $\text{char}(k) \nmid |G|$.\footnote{We saw in Mascke's theorem that this restriction on $k$ ensures that $k[G]$ is semisimple, namely $k[G] = A_1 \oplus \cdots \oplus A_s$ where the $A_i$'s are simple.} Since the study of $\C$-linear representations is so well understood, we use it to understand $\Q$-linear representations in the following examples.

\begin{exmps}
	\begin{enumerate}
		\item[]
		\item Let $p$ be a prime and $G = \Z/p\Z$. Since $G$ is abelian, it has $|G| = p$ conjugacy classes and we obtain the decomposition	\[\C[G] \cong \C \oplus \stackrel{p}{\cdots} \oplus \C.\]
		Another way to obtain this decomposition is to view $\Z/p\Z$ as the set $\{1, \sigma, \dots, \sigma^{p-1}\}$ and then we clearly have\footnote{The polynomial $x^p-1$ splits into irreducible linear factors and then we can apply the Chinese remainder theorem.}
		\[\C[G] \cong \C[x]/(x^p-1) \cong \oplus_{n=1}^{p} \C[x]/\bra{x-\exp\bra{\frac{2\pi i n}{p}}}.\]
		However, $x^p-1$ does not split in $\Q$ and instead we get the decomposition
		\[\Q[G] \cong \Q[x]/(x^p-1) \cong \Q[x]/(x-1)(x^{p-1}+ \cdots + x + 1) \cong \Q \oplus \Q(\zeta),\] where $\zeta = \exp\bra{\frac{2\pi i}{p}}$.\footnote{Note that both summands are field and in particular they are simple, thus, we found the decomposition on $\Q[G]$.}
		\item Let $Q$ be the quaternion group, we know from computing the character table\footnote{Here is the character table for $Q$. We only use the dimension of the representations (namely the first column) to obtain the decomposition.
				\begin{tabular}{lrrrrr}
					& $1$ & $-1$ & $i_{[2]}$ & $j_{[2]}$ & $k_{[2]}$\\ \hline
					$\chi_1$ & $1$& $1$& $1$& $1$&$1$\\
					$\chi_2$ & $1$& $1$& $1$& $-1$&$-1$\\
					$\chi_3$ & $1$& $1$& $-1$& $1$&$-1$\\
					$\chi_4$ & $1$& $1$& $-1$& $-1$&$1$\\
					$\chi_5$ & $2$& $-2$& $0$& $0$&$0$\\ \hline
				\end{tabular}} of $G$ that \[\C[G] = \C \oplus \C \oplus \C \oplus \C \oplus M_2(\C).\]
		By inspecting the two-dimensional representation, we find that $Q$ is realized as a subgroup of $GL_2(\C)$ where all the traces are rational, thus one might wonder if we can realize $Q$ as a subgroup of $GL_2(\Q)$. The answer is negative.
		
		In decomposing $\Q[Q]$, we trivially get the first four terms\footnote{The one-dimensional $\C$-representations are also $\Q$-representations.}, but the last term is (for now) unknown: \[\Q[Q] \cong \Q \oplus \Q \oplus \Q \oplus \Q \oplus D.\]
		We know that $\dim_{\Q}(D) = 4$ and $D$ must be central because if its center were bigger than $\Q$, then the dimension over that would be at least four (it has to be a square and it cannot be one because $\Q[G]$ is not commutative)%TODO: find the actual reason.
		and hence the dimension over $\Q$ would be too big. We can see that $D = \Q(i,j,k) \subseteq \bH$ by defining a map $\phi: \Q[Q] \rightarrow D$ that sends $a[x]$ to $ax$ for any $x \in Q$.
		
		We conclude with the following proposition.
 	\end{enumerate}
\end{exmps}
\begin{prop}
	The two-dimensional irreducible representation of $Q$ can be realized over $K/\Q$ if and only if $K$ splits $\Q(i,j,k)$.\footnote{It is clear thata $K[Q] = \Q[Q] \otimes_{\Q} K$. Using the decomposition above we see that the decomposition of $K[Q]$ will be composed of matrix rings if and only if $D \otimes_{\Q} K$.}
\end{prop}


This proposition in turns motivates the following question which we answer right away.
\begin{quest}
	Given a CSA $A$ over a field $k$, how can we understand the collection of all splitting fields for $A$?
\end{quest}

\begin{thm}
	Let $X \in \Br(k)$, then $X$ belongs to $\Br(K/k)$ if and only if there exists a CSA $A \in X$ such that $K$ is contained in $A$ and $[K:k]^2 = \dim_k(A)$.
\end{thm}
\begin{proof}
	($\Leftarrow$) If $\exists A$ with these properties, then we can repeat the argument in proposition \ref{brauerfiniteext} to conclude $K$ splits $A$.
	
	($\Rightarrow$) Suppose that $K$ splits $A$, namely $A \otimes_k K = M_n(K)$. We can apply the opposite functor and obtain $\op{A} \otimes_k K \cong M_n(K)$.\footnote{This holds because matrix algebras over a field are isomorphic to their opposites (by taking transposes for instance) and the opposite of $K$ is $K$.} There is a natural embedding of $\op{A} \otimes_k 1$ in the L.H.S. and composing with the injective map described above,\footnote{It is injective by simplicity of $\op{A} \otimes_k K$ and non-triviality.} we get an embedding $\op{A} \otimes_k 1 \rightarrow M_n(K) = B$, where $B$ is a CSA over $k$.
	
	Let $C = \mZ_B(\op{A} \otimes_k 1)$, the double centralizer theorem implies that $C$ is simple because $B$ is CSA and $\op{A}$ is simple.\footnote{We dropped the tensor with $1$ because it is unnecessary.} It also states $\dim_k(C)\dim_k(\op{A}) = \dim_k(B) = ([K:k]n)^2$, and since $\dim_k(\op{A}) = n^2$, we infer $\dim_k(C) = [K:k]^2$. It remains to show $K$ sits inside $C$ and $C \in [A]$.
	
	The first part is true because
	\[1 \otimes_k K \subseteq \mZ_{\op{A}\otimes_k K}(\op{A}\otimes_k 1) \subseteq \mZ_B(\op{A}) = A.\]
	The second part is true because\footnote{In general if $A \subseteq B$ are both CSA over $k$, then $C = \mZ_B(A)$ commutes with $A$ and $A \otimes_k C \rightarrow B$. The L.H.S. is simple over $k$ by the double centralizer, so the map is injective and hence an isomorphism by dimension counting (we can use the double centralizer for that as well).} \[C \otimes_k \op{A} \cong B= M_{[K:k]n}(K),\] thus $C$ is Brauer equivalent to the inverse of $\op{A}$ which is $A$.
\end{proof}
\begin{cor}
	If $K$ splits a central division algebra $D$ and $\dim_k(D) = d^2$, then $d \mid [K:k]$.
\end{cor}


\newpage
\section{Review Questions}
\begin{exer}
	Consider the ring $\Z[\alpha]$ where $\alpha = \frac{1+\sqrt{-23}}{2}$ and its ideal $I = (2,\alpha)$. Show that $I$ and $I^2$ is not principal by $I^3$ is. This ring is the integral closure of $\Z$ in $\Q(\sqrt{-23})$.
\end{exer}
\begin{proof}
	Note that $\alpha$ is a root of $x^2-x+6$. To show that $I$ is not principal, we need to show that $2$ and $\alpha$ have no common divisor but one and that one is not in $I$. If we write $2 = uv$, then taking norms on both sides yields $4 = \Nm(u)\Nm(v)$, so $\Nm(u) = \Nm(v) = 2$ because the norms cannot be one.
	
	However, we observe that $\Nm(x+y\alpha) = x^2+xy+6y^2 = (x+\frac{1}{2}y)^2 + 23(\frac{y}{2})^2$, we conclude that the equation above have no solutions. Thus, the only divisors of $2$ are $\pm 1$ and $\pm 2$. Since $\pm 2$ does not divide $\alpha$, the only common divisor is one.
	
	Finally, we claim that $I \neq (1)$, namely, $2R + \alpha R \neq R$. An element of the L.H.S. is of the form $2u + \alpha v$ and its norm is
	\[(2u+\alpha v)(2\bar{u}+\bar{\alpha}\bar{v}) = 4u\bar{u} + 2\bar{\alpha}u\bar{v} +  2\alpha\bar{u}v + 6v\bar{v}.\]
	This belongs to $2\Z$, hence $1 \notin I$.
	
	For the second part, we write $I^2 = (4, 2\alpha, \alpha^2) = (4,2\alpha, \alpha -6)$. Since we can write $2\alpha = 2(\alpha-6) + 3\cdot 4$, we obtain $I^2 = (4, \alpha-6) = (4,\alpha-2)$. We claim that $R/I^2 \cong \Z/4\Z$ because $a + b\alpha \mapsto a+2b$ is surjective and has $I^2$ as its kernel. We conclude that $I \neq (1)$.
	
	Furthermore, we find the divisors of $4$. If $4 = uv$, then taking norms yields $\Nm(u) \Nm(v) = 16$, but no elment has norm two (why?), so we get $\Nm(u) = \Nm(v) = 4$. Using the calculations above, we must find a solution of $4 = (\frac{a}{2})^2 + 23(\frac{b}{2})^2$ or equivalently of $16 = a^2 + 23b^2$. Hence the only divisors of $4$ are $\pm 1$, $\pm 2$ and $\pm 4$. But the only divisors of $\alpha-2$ are $\pm 1$ and hence $I^2$ cannot be principal.
	
	When we calculate $I^3$, we get \[(2,\alpha)(4,\alpha-2) = (8,2\alpha-4,4\alpha, \alpha^2-2\alpha) = (8, 2\alpha- 4, 4\alpha, \alpha+6).\]
	We can remove both terms in the middle, so $I^3 = (8,\alpha-2)$ and we notice that $\Nm(-2+\alpha) = 8$, thus $-2+\alpha$ divides $8$ and hence $I^3 = (\alpha-2)$ is principal.
\end{proof}

\begin{exer}
	Prime ideals of Dedekind domain are locally principle (because any localization is a DVR). We would like to understand how to find the generator of the localization of an ideal.
\end{exer}
\begin{fact}
	If $I$ is a prime ideal in a Dedekind $R$. Let $R = \Z[\alpha]$ as above and $\lp = (2,\alpha)$ is a prime ideal because $R/\lp = \Z/2\Z$. What is a generator for $\lp R_{\lp}$. If we try to divide $2$ by $\alpha$, we get 
	\[\frac{2}{\alpha} = \frac{2\bar{\alpha}}{\alpha\bar{\alpha}} = \frac{2\bar{\alpha}}{6} = \frac{\bar{\alpha}}{3},\]
	thus $2 = \alpha \frac{\bar{\alpha}}{3}$ implying $\alpha$ divides $2$, thus $(\alpha) = \lp R_{\lp}$.
	
	Note that this will not work if we tried dividing $\alpha$ by $2$. In fact, $R/(\alpha) = \Z/6\Z$ and $R/(2) = \Z/2\Z \times \Z/2\Z$ because we can write $R = \Z[x]/(x^2-x+6)$, so $R/(2) = \Z/2\Z[x]/(x(x-1)) \cong \Z/2\Z \times \Z/2\Z$.
\end{fact}
\begin{exer}
	If $R = \C[x]$, what is the spectrum of $R$?.
\end{exer}
\begin{proof}
	As a set: it is the set of prime ideals of $\C[x]$ which are precisely $(x-a)$ for $a \in \C$ and $(0)$. So, we can naturally identify $\spec(R)$ with $\C \cup \{\ast\}$, where $\ast$ represents the generic point $(0)$.
	
	As a topological space: Recall that the closed sets are $V(I)= \{\lp \in \spec(R) \mid \lp \supset I\}$ and that any $I$ is generated by finitely many polynomials which will have finitely many common roots. If $I = (f_1, \dots, f_n)$ with common roots $a_1, \dots, a_m$, then $V(I) = \{a_1, \dots, a_m\}$. If $I = 0$, then $V(0) = \C \cup \{\ast\}$. Hence, the open sets are co-finite sets containing $\{\ast\}$ together with $\emptyset$.
	
	What is the sheaf $\mO$ on $\spec(R)$. Let $U_S = \spec(\C[x]) - \{a_1, \dots, a_m\}$, then it is natural set $\mO(U_S) = \C[x][\frac{1}{x-a_1}, \dots, \frac{1}{x-a_m}]= \C[x][\frac{1}{f}]$ where $f = (x-a_1)\cdots(x-a_n)$.
	
	What is the stalk? of $\mO$ at $a \in \spec(R)$? We have $\mO_a = \varinjlim_{U \subseteq \spec(R), a \in U} \mO(U) = \{\frac{q(x)}{p(x)} \mid q(x),p(x) \in \C[x] \text{ and } p(a) \neq 0\}$. For the generic point, $\mO_{\ast}$ is the inverse limit of every open sets which are non-empty which will yield the fraction field.
\end{proof}

\begin{exer}
In the following setting:
\begin{figure}
	\begin{tikzcd}
		\mO_L \arrow[r, hook]           & L           \\
		\mO_K \arrow[u] \arrow[r, hook] & K \arrow[u]
	\end{tikzcd}
\end{figure}
If $L/K$ is Galois, with $G = \gal(L/K)$ and $\lp \lhd \mO_K \subset K$, and $\lp \mO_L = \lp_1^{e_1}\cdots \lp_r^{e_r}$. Show that $G$ acts transitively on the set of primes in the decomposition.
\end{exer}
\begin{proof}
	Assume not, then after reordering, we can assume that $\lp_1 \neq \sigma \lp_2$ for all $\sigma \in G$. Therefore $\{\sigma \lp_2\}_{\sigma \in G}$ is a finite collection of distinct primes not containing $\lp$. Using the CRT, we can find $a \in \mO_L$ such that $a \in \lp_1$ and $a \equiv 1 \Mod{\sigma \lp_2}$ for all $\sigma \in G$.
\end{proof}
\begin{exer}
	Let $V= M_{m\times n}(k)$, then $M_m(k)$ acts naturally on $V$ by left multiplication. Then, $\mZ_{\End_k(V)}(M_m(k) = M_n(k)$
\end{exer}
\begin{proof}
	$M_n(k)$ also acts on $V$ by right multiplication but less naturally, i.e.: it sends $A$ to the map $(M \mapsto MA^{t})$. This clearly commutes with $M_m(k)$ and then we just have to check dimensionality (using double centralizer).
\end{proof}
\begin{exer}
	Suppose that $V$ is a finite dimensional vector space of dimension $N$ and $A \subseteq \End_k(V)$ and $A \cong M_n(k)$. Then, $n \mid N$ and $B = \mZ_{\End_k(V)}(A) \cong M_{N/n}(k)$.
\end{exer}
\begin{proof}
	The double centralizer implies $B$ is simple and $B$ is central over $k$. We claim there is a natural map $A\otimes B \rightarrow \End_k(V) = a\otimes b \mapsto a\cdot b$. Since $A$ and $B$ are CSA over $k$, then $A \otimes_kB$ is also a CSA and then by dimension count and simplicity, we find it is an isomorphism. We conclude with the Brauer equivalence and the fact that $\End_k(V) = M_{(mn)^2}(k)$.
\end{proof}
\end{document}